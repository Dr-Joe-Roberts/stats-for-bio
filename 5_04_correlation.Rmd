# Correlation

## Introduction

Correlations are statistical measures that quantify an **association** between two variables in a sample. An association is any relationship between the variables that makes them dependent in some way: knowing the value of one variable gives you information about the possible values of the second variable. The terms association and correlation are often used interchangeably, but strictly speaking correlation has a narrower definition. A correlation quantifies, via a **correlation coefficient**, the degree to which an association tends to a certain pattern.

```{r, echo=FALSE}
set.seed(25081978)
x <- data.frame(x = rnorm(50))
cor_eg <- bind_rows(
  mutate(x, y = +x * 0.6, 
         labs = "cor = +1.0"),
  mutate(x, y = +x * 0.35 + rnorm(n(), sd = 0.48), 
         labs = "cor = +0.6"),
  mutate(x, y = rnorm(n(), sd = .6), 
         labs = "cor = +0.0"),
  mutate(x, y = -x * 0.35 + rnorm(n(), sd = 0.48), 
         labs = "cor = -0.6"),
  mutate(x, y = -x * 0.6, 
         labs = "cor = -1.0")
) 
cor_eg <- mutate(cor_eg, 
                 labs = factor(labs, levels = rev(unique(cor_eg$labs))))
```

There are a variety of methods for quantifying correlation, but these all share common properties:

1.  If there is no relationship between the variables then the correlation coefficient will be zero. The closer to 0 the value of the weaker the relationship. A perfect correlation will be either -1 or +1, depending on the direction. This is illustrated below...

```{r cor-strength-eg, echo=FALSE, out.width='100%', fig.asp=0.3, fig.align='center', warning=FALSE}
ggplot(cor_eg, aes(x = x, y = y)) + 
  geom_point() + facet_wrap(~labs, ncol = 5) + 
  scale_x_continuous(limits = c(-1,1) * 3.1) + 
  scale_y_continuous(limits = c(-1,1) * 1.8)
```

2.  The value of a correlation coefficient indicates the direction and strength of the association, but says nothing about the steepness of the relationship. A correlation coefficient is just a number, so it can not tell us exactly how one variable depends on the other. 

3.  A correlation coefficient doesn't tell us whether an apparent association is likley to be real or not. It is possible to construct a statistical test to evaluate whether a correlation is different from zero. Like any statistical test, this requires certain assumptions about the variables to be met.

There are several different measures of correlation between two variables. We will consider the two most widely used methods^[People sometimes just refer to 'the correlation coefficient' without stating which one they are using. When this happens, they probably used the most common method: Pearson’s product-moment correlation.]: 

1.  The first is called **Pearson’s product-moment correlation** ($r$), though for convenience it is often called Pearson’s correlation. Pearson’s correlation is considered to be a parametric method, because the associated significance test make strong assumptions about the variables distribution (but see our comment below).

2.  The second is **Spearman’s rank correlation** ($\rho$), which as the name suggests is a method based on the rank order of observations. Spearman’s rank correlation is considered to be a non-parametric method, because the associated significance test relies on fairly loose assumptions. 

We'll look at each of these in turn, but first, we need to clear up a source of potential confusion...

## Correlation and regression

Correlation and regression are both concerned with associations between variables, but they are different techniques, and each is appropriate under different circumstances. This is a frequent source of confusion. Which technique is required for a particular analysis depends on the way the data were collected and the purpose of the analysis. There are two broad questions to consider:

#### Where do the data come from? {-}

Think about how the data have been collected. If the data are from an experimental study where one of the variables has been manipulated, then choosing the best analysis is easy. We should use a regression analysis, in which the independent variable is the experimentally manipulated  variable  and the dependent variable is the measured outcome. The fitted line from a regression analysis describes how the outcome variable *depends on* the manipulated variable---it describes the causal relationship between them. 

It is generally inappropriate to use correlation to analyse data from an experimental setting. A correlation analysis examines association but does not imply the dependence of one variable on another. Since there is no distinction of dependent or independent variables, it doesn’t matter which way round we do a correlation (The phrase 'which way round' doesn't even make sense in the context of a correlation). 

If the data are from an observational study, either method may be appropriate. Time to ask another question...

#### What is the goal of the analysis? {-}

Think about what question is being addressed. A correlation coefficient only quantifies the strength and direction of an association between two variables. It will be close to zero if there is no association between the variables; a strong association is implied if the coefficeints are near -1 or +1. A correlation coefficient tells us nothing about the form of a relationship. Nor does it allow us to make predictions about the value of one variable from the value of a second variable. A regression does allow this because it involves fitting a line through the data---i.e. there's a model for the relationship. 

This means that if the goal of an analysis is to understand the form of a relationship between two variables, or to use a fitted model to make predictions, we have to use regression. If we just want to know whether two variables are associated or not, the direction of the association, and whether the association is strong or weak, then a correlation analysis is sufficient. It is better to use a correlation analysis when the extra information produced by a regression is not needed, because the former will be simpler and potentially more robust (if we use Spearman's correlation).

## Pearson’s product-moment correlation coefficient

Pearson’s correlation, being a parametric technique, makes some reasonably strong assumptions:

-   The data are on an interval or ratio scale.

-   The relationship between the variables is linear.

-   Both variables are normally distributed in the population.

The requirements are fairly simple and shouldn’t need any further explanation. It is worth making one comment though. Strictly speaking, only the linearity assumption needs to be met for Pearson’s correlation coefficient ($r$) to be a valid measure of association. As long as the relationship between two variables is linear, $r$ produces a sensible measure. However, if the first two assumptions are not met, it is not possible to construct a valid significance test via the standard 'parametric' apparoach. In this course we will only consider the Pearson’s correlation coefficient in situations where it is appropriate to rely on this approach to calculating *p*-values. This means the first two assumptions need to be met.

```{r, echo=FALSE}
bracken <- read.csv("./data_csv/BRACKEN.CSV")
```

We'll work through an example to learn about Pearson’s correlation. 

### Pearson’s product-moment correlation coefficient in R

Bracken fern (*Pteridium aquilinum*) is a common plant in many upland areas. One concern is whether there is any association between bracken and heather (*Calluna vulgaris*) in these areas. To determine whether the two species are associated, an investigator sampled 22 plots at random and estimated the density of bracken and heather in each plot. The data are the mean *Calluna* standing crop (g m^-2^) and the number of bracken fronds per m. 

The data are in the file BRACKEN.CSV. Read these data into a data frame, calling it `bracken`:
```{r, eval=FALSE}
bracken <- read.csv("BRACKEN.CSV")
```
```{r}
glimpse(bracken)
```
There are only two variables in this data set: `Calluna` and `Bracken`. The first thing we should do is summarise the distribution of each variable:

```{r bracken-calluna-dist, fig.show = 'hold', fig.width=4, fig.asp=1, out.width='40%'}
ggplot(bracken, aes(x = Calluna)) + geom_dotplot(binwidth = 100)
ggplot(bracken, aes(x = Bracken)) + geom_dotplot(binwidth = 2)
```

It looks like we're dealing with numeric variables (ratio scale), each of which could be normally distributed. What we really want to asess is the association. A scatter plot is obviously the best way to visualise this:

```{r bracken-calluna-scatter, fig.width=3.5, fig.asp=1, fig.align='center'}
ggplot(bracken, aes(x = Calluna, y = Bracken)) +
  geom_point()
```

It seems clear that the two plants are negatively associated, but we should confirm this with a statistical test. We'll base this on Pearson's correlation.

**How do we know to use a correlation analysis with these data?** We didn't set out with a directional relationship in mind of the form "X causes Y". There may be an association between the two species, but it is not obvious which should be the dependent and independent variables. Neither variable is controlled by the investigator, and we're not interested in using one variable to predict the values of the other. Taken together, these observations indicate that correlation analysis is the appropriate method to use to evaluate the significance of the association.

**Why are we using Pearson's correlation?** A test based on Pearson's correlation will be more powerful than one using Spearman’s correlation. We need to be confident that all the assumptions are met though. The scatter plot indicates that the relationship between the variables is linear, so Pearson's correlation is a valid measure of association. Is it appropriate to carry out a significance test though? The data of the right type---both variables are measured on a ratio scale---and the two dot plots above suggests the normality assumption is reasonable.

Let's proceed with the analysis... Carrying out a correlation analysis in R is straightforward. We use the `cor.test` function to do this:
```{r}
cor.test(~ Calluna + Bracken, method = "pearson", data = bracken)
```
We use `method = "pearson"` to control which kind of correlation coefficient was calculated. There are three options, and although the default method is Pearson's correlation, it is a good idea to be explicit. We use R's formula system to determine which pair of variables are analysed. However, instead of placing a variable on the left hand side and a variable on the right hand side (e.g. `Calluna ~ Bracken`), both two variables appear to the right of the `~` separated by a `+` symbol. This convention makes good sense if you think about where we use correlation: a correlation analysis examines association, but it does not imply the existence of dependent and independent variables. To emphasise the fact that neither variable has a special status, the `cor.test` function expects both variables to appear to the right of the `~`, with nothing on the left. 

The output from the `cor.test` is very similar to that produced by the `t.test` function. We won't step through most of this output, as its meaning should be clear. The `t = -5.2706, df = 20, p-value = 3.701e-05` line is the one we care about. Here are the key points:

-   The first part says that the test statistic associated with a Pearson's correlation coefficient is type of *t*-statistic. We're not going to spend time worrying about where this came from, other than to note that it is intepreted in exactly the same way as any other *t*-statistic. 

-   Next we see the degrees of freedom for the test. Can you see where this comes from? It is $n-2$, where $n$ is the sample size. Together, the degrees of freedom and the *t*-statistic determine the *p*-value...

-   The *t*-statistic and associated *p*-value are generated under the null hypothesis of zero correlation ($r = 0$). Since *p* < 0.05, we conclude that there is a statistically significant correlation between bracken and heather. 

What is the actual correlation between bracken and heather densities? That's given at the bottom of the test output: $-0.76$. As expected from the scatter plot, there is quite a strong negative association between bracken and heather densities.

### Reporting the result

When using Pearson's method we report the value of the correlation coefficient, the sample size, and the *p*-value^[People occasionally report the value of the correlation coefficient, the *t*-statistic, the degrees of freedom, and the *p*-value. We won't do this.]. Here's how to report the results of this analysis:

> There is a negative correlation between bracken and heather among the study plots (r=-0.76, n=22, p < 0.001).

Notice that we did not say that bracken is having a negative *effect* on the heather, or _vice versa_.

## Spearman’s rank correlation

The assumptions of Pearson’s correlation are not too restrictive, but if the data do not match them a non-parametric method such as Spearman’s rank correlation ($\rho$) is the best approach. The advantages of using Spearman’s rank correlation are: 1) the two variables do not need to be normally distributed, and 2) ordinal data can be used. This means Spearman’s rank correlation can be used with data having skewed (or other odd) distributions, or with data originally collected on a rank/ordinal scale. This latter feature makes it very useful for many studies in, for example, behaviour and psychology, where the original data may have been collected on such a scale.

The key assumptions of Spearman’s rank correlation are: 

-   both variables are measured on ordinal, interval or ratio scales

-   there is a monotonic relationship between the two variables

A monotonic relationship occurs when, in general, the variables increase in value together, or when the values of one variable increase, the other variable tends to decrease. What this means in practise is that we should not use Spearman’s rank correlation if a scatter plot of the data forms a clear 'hill' or 'valley' shape.

Spearman’s rank correlation is somewhat less powerful (roughly 91% in some evaluations) than Pearson’s method when the data are suitable for the latter. Otherwise it may even be more powerful. We'll work through an example to learn about Spearman’s correlation...

### Spearman’s rank correlation coefficient in R

```{r, echo=FALSE}
grouse <- read.csv("./data_csv/GROUSE.CSV")
```

Some bird species, at a particular point in the spring, form ‘leks’---gatherings of birds, with males each defending a small area of ground, displaying, and each mating with the such females as he is successful in attracting. In general, in leks, a few birds secure many matings and most birds secure rather few. In a study of lekking in black grouse, a biologist is interested in whether birds that secure many matings in one season also do better the next year. Using a population with many colour-ringed birds he is able to get data for a reasonable number of males from two leks in successive years.

The data are in the file GROUSE.CSV. Read these data into a data frame, calling it `grouse`.
```{r, eval=FALSE}
grouse <- read.csv("GROUSE.CSV")
```
```{r}
glimpse(grouse)
```
Each row of the data is the number of matings for a male in the two successive leks: `Year1` (year 1) and `Year2` (year 2). The first thing we should do is summarise the distribution of each variable:
```{r grouse-years-dist, fig.show = 'hold', fig.width=4, fig.asp=1, out.width='40%'}
ggplot(grouse, aes(x = Year1)) + geom_dotplot(binwidth = 2)
ggplot(grouse, aes(x = Year2)) + geom_dotplot(binwidth = 2)
```

Notice that the data are integer-valued (i.e. they are counts). These distributions seems to tie in with the biological observation that the distribution of matings is right-skewed: in both years there are only a few males that have high mating success, with most males securing only a handful of matings. Next we need to visualise the association:

```{r grouse-years-scatter, echo=FALSE, fig.width=3.5, fig.asp=1, fig.align='center'}
ggplot(grouse, aes(x = Year1, y = Year2)) +
  geom_point(alpha = 0.5)
```

The data are integers, which means there is a risk of over-plotting (points will lie on top of one another). We made the points semi-transparent `alpha = 0.5` to pick this up where it occurs. It seems clear that mating success is positively associated, but we should confirm this with a statistical test. We'll base this on Pearson's correlation.

**How do we know to use a correlation analysis with these data?** Although there seems to be an association between the counts, it is not obvious that success in one year 'causes' success in another year and neither variable is controlled by the investigator. We're also not interested in using the success measure in year 1 to predict success in year 2. This indicates that correlation analysis is the appropriate method to evaluate the significance of the association.

**Why are we using Spearman's correlation?** The relationship appears roughly linear, so in that regard Pearson's correlation might be apppropriate. However, the distribution of each count variable is right-skewed, which means the normality assumption is probably suspect in this instance. We're left with no choice but to use Spearman's correlation.

Carrying out a correlation analysis using Spearman’s rank correlation in R is simple. Again, we use the `cor.test` function to do this:
```{r}
cor.test(~ Year1 + Year2, method = "spearman", data = grouse)
```
The only other thing we had to change, compared to the Pearson's correlation example, was to set `method = "spearman"` to specify the use of Spearman’s rank correlation. Notice the warning message: 'Cannot compute exact p-value with ties'. This is generally not something we need to worry about for this particular test.

The output is very similar to that produced by `cor.test` when using Pearson's correlation. Once again, the `S = 592.12, p-value = 0.01112` line is the one we care about. The main difference is that instead of a *t*-statistic, we end up working with a different kind of test statistic ('*S*'). We aren't going to explain where this comes from because it's quite technical. Next we see the *p*-value. This is generated under the null hypothesis of zero correlation ($\rho = 0$). Since *p* < 0.05, we conclude that there is a statistically significant correlation mating success in successive years. Wait, where are the degrees of freedom? Simple---there aren't any for a Spearman's correlation test.

What is the correlation between mating success? That's given at the bottom of the test output again: $+0.55$. This says that there is a moderate, positive association between mating success in successive years, which is what we expect from the scatter plot.

### Reporting the result

When using the Spearman method is is fine to report just the value of the correlation coefficient, the sample size, and the *p*-value (there is no need to report the test statistic). Here's how to report the results of this analysis:

> There is a positive association between the number of matings achieved by a particular male in one lek and the number the same male achieves in a subsequent lek (Spearman's rho=0.55, n=20, p < 0.05).





