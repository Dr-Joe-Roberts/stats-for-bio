# One sample *t*-tests

## When do we use one-sample *t*-test?

The one-sample *t*-test is perhaps the simplest statistical tests we can carry out. It is appropriate in situations where we are studying a sample of numeric variable from a single population. We use the one-sample *t*-test to compare the mean of the variable to an expected value, i.e. the test uses information in the sample evaluate whether the population mean is likely to be different from an expected value. This latter value might be something predicted from theory or some other prespecified value we are interested in. Here are a couple of examples:

-   We have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If we have data on the actual time spent by 25 animals observed foraging in the patch, we could test whether the mean foraging time is significantly different from the prediction using a one-sample *t*-test.

-   We are monitoring sea pollution and have a series of water samples from a beach. We wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach as a whole can be regarded as greater, or less than the legislated limit. A one-sample *t*-test will enable us to test whether the mean value for the beach as a whole exceeds this limit^[The question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue].

## How does the one-sample *t*-test work?

The key assumption of one-sample *t*-test that *the variable is normally distributed in the population*. Imagine that we have taken a sample of a variable (imaginatively called 'X') from a population we're investigating. Here's an example of what these data might look like, assuming a sample size of 50 is used:

```{r one-t-eg-samps, echo = FALSE, out.width='55%', fig.asp=1, fig.align='center', fig.cap='Example of data used in a one-sample t-test'}
set.seed(27081975)
nsamp <- 50
plt_data <- data.frame(X = rnorm(nsamp, mean = 11))
line_data <- plt_data %>% summarise(Mean = mean(X))
ggplot(plt_data, aes(x = X)) +
  geom_dotplot(alpha = 0.6, binwidth = 0.3) +
  geom_vline(aes(xintercept = Mean), line_data, colour = "red") +
  geom_vline(xintercept = 10, colour = "blue") +
  theme_grey(base_size = 22)
```

The distribution of the sample look roughly bell-shaped, so it seems plausible that it was drawn from a normal distribution^[It's actually quite hard to determine whether a sample was drawn from a normal distribution when the sample size is small.]. The red line shows the mean of the sample, and the blue line shows the expected value (which is 10, so this could correspond to the foraging example mentioned above). The observed sample mean is about one unit larger than the expected value. The question is, how do we decide whether the population mean is really different from the expected value?

We tackle this question by first setting up an appropriate null hypothesis---the null hypothesis in this instance is that the population mean is *equal to the expected value*. We then work out what the sampling distribution of the sample mean looks like under this null hypothesis (this is the 'null distribution'), and use this to assess how likley the observed result is. This chain of reasoning is quite similar to that developed in the bootstrapping example considered in the [Statistical significance and *p*-values] chapter. There are some differences in the details (e.g. we examined the frequency of a categorical variable earlier, whereas here, we're considering the mean of a numeric example), but these aren't so important. 

The really important change is that now, because we're now prepared to make the normality assumption, the whole process of carrying out the statisical test is much simpler. No resampling of data is involved. The key consequence of making the normality assumption is that the null distribution will have a known mathematical form. We can use this knowledge to construct the test of statistical significance. But instead of using the whole sample (as we did for the bootstrapping), we only need three pieces of information from the sample: the sample size, the sample variance, and the sample means.

OK, how does a one-sample *t*-test it actually work? It is carried out as follows:

**Step 1.** Estimate the standard error of *the sample mean*. This estimate gives us an idea of how much sampling variation we can expect to observe. It turns out that as long as the variability in the population is constant, this doesn't depend on the true value of the mean. So the standard error of the observed sample mean is also the standard error of a mean under any particular null hypothesis.

Don;t worry too much if that is confusing (yes, frequentist statiscs is odd). Ultimately, this step boild down to applying a simple formula involving the sample size and the standard deviation of the sample ...

$$\text{standard error of the mean, s.e.} = \frac{s}{\sqrt{n}}$$

...where $s$ stands for the sample variance and $n$ stands for the sample size. This is the formula we introduced in the [Parametric statistics] chapter. Notice that this quantity gets smaller as the sample sizes grows or the sample variances shrinks. 

**Step 2.** Calculate the mean. That's simple enough. This estimate is our 'best guess' of the unknown population mean. However, its role in the two-sample *t*-test is to allow us to construct a test statistic in the next step.

**Step 3.** Calculate a test statistic from the sample mean and standard error. We calculate the test-statistic by dividing the sample mean (from step 2) by its estimated standard error (from step 1):^[Notice that the magnitude of the test statistic is increased when either the sample means grows, or its standard error shrinks.]

$$ \frac{\text{sample means}}{\text{standard error of the mean}}$$

Why is this useful? If our normality assumption is appropriate, then if we could repeatedly sampled from the population, working through steps 1-3 each time, the collection of *t*-statistics that results form this process would follow a *t*-distribution. This is guaranteed by the normality assumption. So this test statistic is in fact a type of *t*-statistic. That knowledge leads to the final step...

**Step 4.** Compare the*t*-statistic to the theoretical predictions of the *t*-distribution in order to assess the statistical significance of the difference between observed and expected value. That is, we calculate the probability that we would have observed a difference between means as large as, or larger than, the observed difference, if the null hypothesis were true. That's the *p*-value for the test.

We could step through the actual calculations involved in these steps in detail, but there isn't a whole lot to be gained by doing this. We're going to let R to handle the heavy lifting (it's just one line of R code). Before we see how to do this we should review the assumptions of the one-sample *t*-test first.

### Assumptions of the one-sample *t*-test

There are a number of assumptions that need to be met in order for a one-sample *t*-test to be valid. Some of these are more important than others. We'll start with the most important and work down the list in order of importance:

1. **Independence.** People tend to forget about this one, probably because they can't do much about it if there is a problem. We're going to discuss the important idea of independence later in the book when we think about experimental design. For now, we'll briefly tell you why it matter: if the data are not independent, then the *p*-values generated by the one-sample *t*-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale, i.e. it should be a numeric variable. It generally doesn't make much sense to apply a one-sample *t*-test to a variable that isn't measured on one of these scales. 

3. **Normality.** The one-sample *t*-test will always produce reliable *p*-values if the variable is normally distributed in the population. However, this assumptions is less important than many people think. The *t*-test is fairly robust to mild departures from normality when the sample size is small, and when the sample size is large the normality assumption matters even less^[It's hard to define what constitutes a 'large' sample, but 100s of observations would often be safe.]. We don't have the time to properly explain why the normality assumption is not too important for large samples, but we will at least state the reason: it is a consequence of something called the ['central limit theorem'](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem).

How do we go about evaluating these assumptions? The first two are really aspects of experimental design, i.e. we can only evaluate them by thinking carefully about how the data were gathered and what was measured. What about the 3^rd^ assumption? This is best evaluated by plotting the distribution of the sample, using something like a histogram or a dot plot. If the sample size is small, and the sample looks approximately normal when we visualise its distribution, then it is probably fine to use the *t*-test. Just remember, if we have a large sample we don't need to worry much about moderate departures from normality. Ask someone with experience of data analysis if you run into this situation and are not sure whether there is a problem or not.

## Carrying out a one-sample *t*-test in R

```{block, type='advanced-box'}
**A bit more about degrees of freedom**

Degrees of freedom (abreviated d.f. or df) are closely related to the idea of sample size. In a nutshell, the greater the degrees of freedom associated with a test, the more likely it is to detect an effect (however defined) if it's present, i.e. we say the test has more 'statistical power'. To calculate the degrees of freedom, we start with the sample size, and then we reduce this number by one for every mean we calculate to construct the test. That's a bit of an over simplication, but it's enough to be getting on with.

Calculating degrees of freedom for a one-sample *t*-test is quite simple. The degrees of freedom are just n-1, where n is the number of observations in the sample. We lose one degree of freedom becase we have to calculate the sample mean to construct the test.
```

### Checking the assumptions

### Summarising the result

[[REWRITE THIS SECTION]]

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A *t* statistic can come out negative or positive, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of *t* that determines the *p*-value, when presenting the results, just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The *t* statistic should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, d.f. and *p* are always best as lower case.

3.  **How should I present _p_?** 

[[FINISH]]

```{block, type='warning-box'}
**p = 0.0000? It’s impossible! p = 1e-16? What's that?** 

Some computer packages (e.g. Minitab) will sometimes give a probability of p=0.000. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, we should never say this. When a computer package says p=0.000 it just means that the probability was 'very small'.

R uses a different convention for presenting small probabilities. `p-value < 2.2e-16`
```
