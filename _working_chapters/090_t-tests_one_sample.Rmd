# One sample *t*-tests

[[PREAMBLE]]

## The t-distribution

A statistician called W.G. Gosset showed that when we take samples from a normally distributed variable and calculate their means, the sampling distribution of these means has a particular form---it follows a Student's t-distribution. The same is true if you take two samples from a normal distribution, calculate their means, and then subtract these from one another. The sampling distribution of the differences among means also follows a Student's t-distribution.

(Why is it called Student's t? W.G. Gosset was a statistician employed by the Guinness Brewery, who published his statistical work under the pseudonym of 'Student'. He didn't use his real name because Guinness would have claimed ownership of his work.)

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's t-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a t-distribution.

We are not going to delve any deeper into the t-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the standard error of the difference. Different estimates of the standard error are available.

(This generates a test statistic)

4. Compare the test statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the observed difference.

Thsi procedure is called a two sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### So what exactly is a one-sample t-test?

The one-sample t-test allows us to compare the mean from a sample with an expected value. More precisely, it allows us to use the sample evaluate whether or not the unknown population mean is likely to be different from an expected value. This value might be something predicted from theory or some other prespecified value you are interested in. Here are some examples:

-   You have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If you have data on the actual time spent by 15 animals observed foraging in the patch, then you could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test.

-   You are monitoring sea pollution and have a series of water samples from along a beach. You wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach as a whole can be regarded as greater, or less than the legislated limit. You have a sample, which has variation, and a single value; a one-sample t-test will enable you to test whether the mean value for the beach as a whole, exceeds the limit (of course the question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue).

-   You are in charge of packaging seed samples from a horticultural firm, and your sales literature says that the packs will contain an average of 40 seeds. You select 20 packets off the production line at random and count the seeds in each. You could use a one-sample t-test to test whether this claim was true (or perhaps more importantly your competitors could!)

You can see that a paired-sample t-test, testing a set of observed differences against an expected value of zero (what we’d expect if there was no systematic difference across all the pairs), is simply another thing you can do with a one-sample test. It happens to be an extremely useful thing to be able to do, because it can allow you to design some very powerful experiments, and in practice you will probably end up using using one-sample tests for testing differences among pairs more often than any other use.

### Assumptions of the one-sample *t*-test

There are a number of assumptions that need to be met in order to use a one-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by the *t*-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It generally doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales. 

The one-sample *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the distribution of the sample. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

#### Assumptions: The example

[[WRITE THIS]]

### Carrying out a one-sample *t*-test in R

```{block, type='advanced-box'}
**A bit more about degrees of freedom**

[[UPDATE THIS]]
```

### Summarising the result of a one-sample *t*-test

[[REWRITE THIS SECTION]]

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistic should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = [[FIX ME]]). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.

```{block, type='warning-box'}
**p = 0.0000? It’s impossible!** 

Some computer packages (e.g. Minitab) will sometimes give a probability of p=0.000. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, you should never say this. If you actually know something is impossible for good biological, or other, reasons then you do not need to test it using statistics. When a computer package such as Minitab says p=0.000 it just means that the probability was 'very small'.
```
