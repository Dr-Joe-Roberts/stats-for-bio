# Two-sample *t*-test

[[PREAMBLE]]

## Using a *t*-test to compare two means

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
tmod.equlv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = TRUE )
tmod.diffv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = FALSE)
```

Think back to the permutation test we carried out in chapter [[LINK ME]]. In order to construct the test, we "pretended" that the two samples were both drawn from one population. We used the combined purple morph + green morph sample to mimic the process of drawing new samples from this hypothetical population, each time assigning arbitary labels to generate artificial observations. 

The two-sample *t*-test can be viewed as a "parametric" version of this procedure. It starts by making an assumption about the mathematical form of the population distribution. The key assumption is that the two samples are **normally distributed** in the population.

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's *t*-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a *t*-distribution.

We are not going to delve any deeper into the *t*-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the *standard error of the difference*.

(This generates the *t*-test statistic)

4. Compare the test statistic to the theoretical predictions of the *t*-distribution to assess the statistical significance of the observed difference.

This procedure is called a two-sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### Assumptions of the *t*-test

There are a number of assumptions that need to be met in order to use a two-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by a *t*-test will not be reliable. Even mild non-independence can be a serious problem. This is why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It really doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales.

The *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. The 4^th^ assumption only matters if you plan to use the equal variance version of the two-sample *t*-test (the original Student's *t*-test). This version of the *t*-test is potentially a little more powerful than Welch's version, but not by much, and it is only correct if the population variances really are identical. Since we can never verify this, it is safer to just use the unequal variance version. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the sample distribution of each group. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

If you learned about the two-sample *t*-test at some point in the past you may have been told that variances of the samples need be the same. What about the **equal variance** assumption? This isn't really correct. The original version of Student's two-sample *t*-test was derived by assuming that the *population variance* of each group was identical, so it is the population variances, not the sample variances, that matter. This isn't the critical point though. What matters from a practical perspective is that R uses the "Welch" version of the *t*-test by default (Welch was another statistician, in case you're wondering). Welch's version of the two-sample *t*-test does not make the equal variance assumption, so as long as you stick with this version of the *t*-test, the equal variance assumption isn't one you need to worry about.

### Carrying out a two-sample *t*-test in R

You should work through the example in this section. If you haven't already done so, you will need to download the MORPH_WEIGHTS.CSV file from MOLE and place it in your working directory (this is the location you just set). Next, read the data in MORPH_WEIGHTS.CSV into an R data frame, giving it the name `morph.weights`:

```{r, eval = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

We looked at the sample distributions of the green and purple morphs in chapter [[LINK ME]]. It looks like they may have different variances, but as we have just seen, this isn't something we need to worry about. The sample size of each group is 25. This is fairly small (though not bad), so we should keep an eye on the normality assumption. The two dot plots we produced earlier suggest that there is nothing too 'non-normal' about their distributions, so it should be fine to go ahead and use a *t*-test.

It is very straightforward to carry out a two-sample *t*-test in R. We'll work with our plant morph example to demonstrate how to do it. The function we need to use is called `t.test` (no suprises there). Remember, we read the data into a data frame called `morph.weights`. This has two columns: `weight` contains the dry weight biomass of each plant, and `pmorph` is an index variable that indicates which group (plant morph) an observation belongs to. Here is the R code to carry out a two-sample *t*-test 
```{r, eval = FALSE}
t.test(weight ~ pmorph,  morph.weights)
```
We have surpressed the output for now as we want to focus on how to use `t.test` function. We have to assign two arguments (remember function arguments?--these control what a function does):

1. The first argument is a **formula**. We know this because it includes a 'tilde' symbol: `~`. The variable name on the left of the `~` should be the variable that contains the actual data (i.e. the numbers we want to compare). The variable on the right should be the indicator variable that says which group each observation belongs to. These are `weight` and `pmorph`, respectively.

2. The second argument is the name of the data frame that contains the two variables listed in the formula.

That's it. Let's take a look at the output:
```{r}
t.test(weight ~ pmorph,  morph.weights)
```

The first line reminds us what kind of *t*-test we have used. This says: `Welch two-sample *t*-test`, so we know that we have used the version of the two-sample *t*-test that accounts for unequal variance in the samples.

The next line just reminds us about the data. This says: `data: weight by pmorph`, which is R-speak for 'we compared the means of the `weight` variable, where the groups are defined by the values of the `pmorph` variable'.

The third line of text is the most important. This says: `t = 2.9381, df = 39.523, p-value = 0.005487`. The first part of this, `t = 2.9381`, is the test statistic (i.e. the value of the t statistic). The second part, `df = 39.523`, summarise the 'degrees of freedom'. This is essentially a measure of how much power our statistical test has (see the box below). The third part, `p-value = 0.005487`, is the all-important p-value. This says that there is a statistically significant difference in the mean dry weight biomass of the two morphs, because *p*<0.05.

The fourth line of text (`alternative hypothesis: true difference in means is not equal to 0`) just reminds us what the alternative to the null hypothesis is. We will discuss the meaning of this at the end of this chapter.

The next two lines show us the 95% confidence interval for the difference between the means. We don't really need this information, but you can think of this interval as a summary of the likely values of the true difference (a confidence interval is more complicated than that in reality).

The last few lines just summarise the sample means of each group. This is only useful if you did not bother to summarise these already (which you should always do!). 

<div class="advanced-box">
#### A bit more about degrees of freedom (again)
<div class="box-text">
In the original version of the *t*-test (which assumes equal variances) the degrees of freedom of the test are give by (n~a~-1) + (n~b~-1)  where n~a~ is the number of measurements from sample a and n~b~ the number of measurements from sample b. The plant morph data has measurements for 25 male and 25 females, so if we had used the original version of the test we would have (25-1) + (25-1) = 48 df. However, the R default version of the *t*-test reduces the numbers of degrees of freedom using a formula which takes into account the difference in variance in the two samples--the greater the difference in variances the smaller the number of degrees of freedom. This usually results in degrees of freedom that are not whole numbers.

In either case, a test with high degrees of freedom is more powerful than one with low degrees of freedom. 
</div>
</div>

### Summarising the result of a *t*-test

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistics should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = `r round(tmod.diffv$p.value, 6)`). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.
