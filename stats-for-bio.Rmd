--- 
title: "APS 240: Data Analysis and Statistics with R"
author: "Dylan Z. Childs"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    includes:
      in_header: extras.css
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: dzchilds/stats-for-bio
description: "Course book for Data Analysis and Statistics with R (APS 240) in the Department of Animal and Plant Sciences, University of Sheffield "
---
```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```

# Course information and overview

This is the online course book for the __Data Analysis and Statistics with R__ ([APS 240](https://www.shef.ac.uk/aps/currentug/level2/aps240)) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained---it contains all the material you are expected to learn this year.

[Dylan Childs](https://www.shef.ac.uk/aps/staff-and-students/acadstaff/childs) is the course co-coordinator. Please [email him](mailto:d.childs@sheffield.ac.uk?Subject=APS%20133%20general%20query) if you have have any general queries about the course. [Andrew Beckerman](https://www.shef.ac.uk/aps/staff-and-students/acadstaff/beckerman) is the second course instructor. The Teaching Assistants ('TAs') this year are Ross Booton, Matthew Hethcoat, Bethan Hindle, Tamora James, Felix Lim, and Simon Mills.

## Why do a data analysis course? {#why-data-analysis}

To do science yourself, or to understand the science other people do, you need some understanding of the principles of experimental design, data collection, data presentation and data analysis. That doesn’t mean becoming a maths wizard, or a computer genius. It means knowing how to take sensible decisions about designing studies and collecting data, and then being able to interpret those data correctly. Sometimes the methods required are extremely simple, sometimes more complex. You aren’t expected to get to grips with all of them, but what we hope to do in the course is to give you a good practical grasp of the core techniques that are widely used in biology and environmental sciences. You should then be equipped to use these techniques intelligently and, equally importantly, know when they are not appropriate, and when you need to seek help to find the correct way to design or analyse your study.

You should, with some work on your part, acquire a set of skills which you will use at various stages throughout the remainder of your course, in practicals, field courses and in your project work. These same skills will almost certainly also be useful after your degree, whether doing biology, or something completely different. We live in a world that is increasingly flooded with data, and people who know how to make sense of this are in high demand. The R statistical programming environment increasingly underpins much of this endeavour, in both academic and commercial settings. Learning the basic principles of data analysis, using R, can only improve your employment prospects and opportunities for further study following your undergraduate training. 

## Course overview {#overview}

### Aims {#aims}

This course has two main, and equal, aims. The first is to provide a basic training in the use of statistical methods and computers (R!) to analyse biological data. The second is to introduce some of the principles of experimental design, sampling, data interpretation, graphical presentation and scientific writing relevant to the biological and environmental sciences.

### Objectives {#objectives}

By the end of the course you should be familiar with the principles and use of a range of basic statistical techniques, be able to use the R programming language to carry out appropriate analyses of different types of biological data, and make sensible interpretation of the results. You should be able to relate the ways in which data are collected (by different designs of sampling or experiment) to the types of statistical methods that can be used to analyse those data. In combination with the skills you developed in [APS 135](https://www.shef.ac.uk/aps/currentug/level1/aps135), you should be able to decide on appropriate ways of investigate data graphically, be able to produce good quality scientific figures, and incorporate these, along with statistical results, into a formal report.

### Assumed background {#assumed-background}

You are assumed to be familiar with the use of personal computers on the University network, and with the use of R for data input, manipulation and plotting introduced in [APS 135](https://www.shef.ac.uk/aps/currentug/level1/aps135). If you are unsure about these basic methods, then you will need to revise the material covered in the Level 1 IT practicals. The key skills you need are covered in the [Programming prerequisites] chapter.

```{block, type='do-something'}
**Environmental Sciences Students** 

Don't panic if you are one of the Environmental Sciences students joining us from Geography. We'll provide extra help at the beginning of the course to help you learn the fundamentals of R.
```

### Methods {#methods}

The course runs over semester 1 (weeks 1-12). The first 10 weeks consists of a 2-hour IT practical each week, along with some additional preparatory practical work and/or extra reading. The remaining 2 weeks are devoted to an assessed data analysis project.

Students come to APS 240 with very different experiences and competancies. For this reason, the course is designed as a ‘self-teaching’ module, allowing you flexibility in the rate at which you work through the material. Each week starts with some preparatory reading and practical from a few chapters of the course book. We will tell you what you need to work through each week. You will then complete this in your own time. 

The book chapters generally come in one of two forms:

-   **Practical walk-through chapters.** These are designed to introduce the practical aspects of different analysis techniques. These generally focus on the 'when' and 'why' we use a particular technique, as well how to actually do it in R. You are free (encouraged even) to work through these in groups.

-   **Extra reading chapters.** These focus on ideas and concepts, rather then using R. They provide background information or more detailed discussion relating to the topics you are covering. These are an integral part of the course so please don't skip them. 

Nobody is expecting you to understand everything the first time around. That point is so important it's worth saying again. You are not expected to understand everything in the course book the first time you read! Just do your best to understand it, taking careful notes of anything you're struggling with. The TAs and staff will happily answer any questions you have during the timetabled sessions. That's what they're there for! There should also be some spare time to go back over chapters during the timetabled sessions.

The timetabled practicals take place in the APS IT rooms, either Perak IT labs or B56. In each of these sessions, you will work through a number of small exercises to help you consolidate what you're been working on. TAs and staff will be able to help if you get stuck, but it is up to complete the exercises if you do not manage to finish them during a timetabled session. 

You are welcome to use your own computer to complete your work. Keep in mind that the university computers are the only 'officially supported' platform though. If you run into a problem using your own computer, the TAs and staff will try to help resolve these. Unfortunately, if these prove to be intractable, you will have to use the university computing facilities. It just isn't fair on other students for teaching staff to spend valuable contact time trying to solve installation / setup problems.

#### Non-assessed material

Although every topic is important, in the sense that it contains material that will help you analyse the data you collect in field courses, practicals and projects, we want to avoid creating too much of an assessment burden in this course. To this end, the material in some of the self-study practicals and extra readings will not be formally assessed. The syllabus tells you what you need to learn for assessment purposes, so if you're the kind of person whoi just wants to focus on the assessment, use that as your guide. Feel free ask a mamber of staff for clarification if you're not sure.

### What is required of you? {#what-is-required-of-you}

A willingness to learn and to take responsibility for your learning! Data analysis is not the easiest subject in the world, but neither is is the most difficult, and what you learn in this course will form the basis for much of what you do in field course, practical and project work that follows. 

The minimum requirement for the course is that you:

-   attend your designated practical session each week (please ensure that you arrive on time and sign the register, or you will be recorded as absent)

-   complete the preparatory practical work and extra reading, taking careful notes of anything that you don't understand

-   be proactive and ask the TAs and staff questions about anything you're struggling with

-   complete the exercises for each week before the next practical class, and check through your answers to questions using MOLE

How you work through the book is fairly flexible, but in each practical session you should aim to complete most, if not all, of the exercises for that practical. If you don’t, then you should try to finish off the work in your own time before the next practical. Remember, the self-study work lays the foundatation forthe work covered in the timetabled practicals.

If you have problems with any of the work, then staff will help you during the practical sessions, even if it is not the topic designated for that session. So if you need to catch up, or get help with something you didn’t understand there will always be opportunities to do so.

A word of advice: Don’t let the flexibility of the course tempt you into letting a backlog of work build up. This will compromise your ability to do the assessed work when it is set.

One last point: **the university expects the total study time nominally associated with a 10 credit module to be about 100 hours**. There is an expectation that you will spend significant time outside the timetabled practical classes working on this module. You should aim to spend about 5 hours each week working through the chapters, completing the practical exercises, and finally, working on the assessed project. This leaves you about 40 hours to revise for the formal exam.

### Assessment {#assessment}

Assessment of the course will have two components. The first is a short data analysis project in weeks 11-12 of the course, the second is an open book exam in the winter exam period. Full details will be given as the course progresses.

## How to use the teaching material {#how-to-use-the-teaching-material}

### The online course book {#printed-material}

All of the teaching material will be made available through a single online course book (this website). The book is organised such that it forms a complete, stand-alone introduction to data analysis. You should bookmark this now if you haven't already done so. There are a couple of good reasons for delivering the course material this way: 

-   **Practicality**: Many of the exercises that you will complete require you to build a previous example in the course material. Copying the relevant R code from the course website and pasting it into your script is much more efficient, and less error-prone, than copying by eye from a printed page. A website also allows us to cross-reference topics and link to outside sources of reading.

-   **Permanence**: Experience suggests that many of you will want to refer to the material in this course after you graduate. However, bits of paper are easy to lose, and because the R landscape is always changing, some elements of the course may be less relevant in a few years time. By putting everything on a website, we can ensure that you will always be able to access a familiar, but up-to-date data analysis course.

### Printed material: what you get and what you do with it {#printed-material}

There is a small amount of printed material in this course:

-   **Cheat sheets**: We will supply you with copies of the `dplyr` and `ggplot2` [cheat sheets](https://www.rstudio.com/resources/cheatsheets/) produced by the people who build RStudio . It may help you to refer to these when you need use either the `dplyr` or `ggplot2` packages in a practical.

-   **Assessment information**: Although much of the assessment will be done on the computer, any information relating to the assessments will be produced in printed form on exciting pink paper, so you can’t miss it!

### How to make best use of the teaching material {#how-to-make-best-use-of-it}

DO:

-   When working through an exercise, follow the instructions carefully, but also **think about what you are doing!** Work at your own pace; you are not being assessed on whether you can do an exercise in a particular time.

-   Ask teaching staff for help in the practicals if there are things that you don’t follow, or when things don’t seem to come out the way they should---that’s what we’re there for!

-   Collaborate! If you are not sure you understand something feel free to discuss it with a friend — more often than not this is exactly how scientists resolve and clarify problems in their experimental design and analysis.

-   Be prepared to experiment to solve any problems that you encounter. You can't break your R or RStudio by generating errors (well, one can, but it's quite hard to do). When you run into a problem, go back to the line of code that generated the first error and try making a change.

-   Complete each week’s work before the next week’s session. You may be able to complete some sessions quite quickly, others may take more time and require more work on your own outside the timetabled periods.

DON’T:

-   Just copy what someone else tells you to do without understanding why you are doing it. You need to understand it for yourself (and you’ll be on your own in the exam).

-   Skip practicals or preparatory work and get behind schedule — there is too much material to assimilate all at once when you get to the assessments. Like all skills data analysis is something you have practice.

### Conventions used in the course material {#conventions}

The teaching material, as far as possible, uses a standard set of conventions to differentiate between various sorts of information, action and instruction:

#### Text, instructions, and explanations

Normal text, instructions, explanations etc. are written in the same type as this document, we will tend to use bold for emphasis and italics to highlight specific technical terms when they are first introduced (italics will also crop up with Latin names from time to time, but this is unlikely to produce too much confusion!)

When we want to say something REALLY IMPORTANT---e.g., we are summarising the key learning outcomes or giving you a set of instructions---we place the text inside a grey filled box, like this one: 

```{block, type='do-something'}
**Brown boxes**

Here is some important text telling you to do something or remember something important. Sometimes it just contains a warning that the next bit will be hard...
```

Don't ignore these. At various points in the text you will also come across different coloured boxes that contain supplementary information:

```{block, type='advanced-box'}
**Blue boxes**

These contain __supplementary information__ or an __advanced topic__. They aim to offer a not-too-technical discussion of how or why something works the way it does. These are things that it may be useful to know, or at least know about. These are not generally meant to be read as part of the main text.
```

```{block, type='warning-box'}
**Red boxes**

These contain a __warning__ or flag a common __gotcha__ that may trip you up. These boxes aim to highlight potential pitfalls and show you how to avoid them. These are not generally meant to be read as part of the main text. You will avoid mistakes now and in the future if pay close attention to these.
```

We use block quotations to indicate an example of how a particular statistical result should be presented when you write it in a report: e.g.

> The mean lengths of male and female locusts differed significantly (t=4.04, df=15, p=0.001), with males being significantly larger.


#### R code, files and RStudio

`This typeface` is used to distinguish R code within a sentence of text: e.g. "We use the `summary` function to obtain information about an object produced by the `lm` function."

A sequence of selections from an RStudio menu is indicated as follows: e.g. **File ▶ New File ▶ R Script**

File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV.

### Feedback {#feedback}

There are a number of ways in which you can obtain feedback on how well you understand the course material

#### Self-assessment questions:

At various points in the course material there are questions for you to answer. When you reach one of these, you should be in a position to answer the question —- so make a note of the answer! When you’ve completed the session, you can then check your answers using the ‘self-test’ for that particular session on MOLE. You will see if you have the correct answer and in some cases you will also get some additional explanation as to why that answer is right (or wrong!).

#### Each other:

Discussing what you are doing with someone as you go along, or working through a problem with someone else, can help clarify your understanding. Please bear in mind, however, that you learn little or nothing by simply copying information from someone else, and when it comes to the assessed project, it must be your own work.

#### Staff:

In the practicals you will have opportunities to ask questions and discuss what you are doing with staff and teaching assistants. They are not just there to help you with the practical. You should use them to help you work through any problems you have with the course material, both "conceptual" and "practical". There will also be an opportunity to have topics you raise discussed in later practicals.

### Help sessions {#help}

We will run an open 'help' session every Wednesday from 12-2.00pm, in the B56 IT Room in APS. An instructor will be on hand during this period to answer specific questions about the course material. This room holds about 40 students, so please only attend if you require one-to-one assistance, i.e, don't just use this session to complete unfinished practicals (unless you are stuck).

### Overall… {#overall}

We hope that the material is clear and easy to use, and that you find the course useful, or even enjoy it!

In a text of this size, which is continually being improved and updated, errors do creep in; if you find something you think is wrong please tell us. If it’s not wrong we will be happy to explain why, and if it is then you will save yourself and others a lot of confusion. Similarly, if you have any comments or suggestions for improving the teaching materials please let us know.

## Health and safety using display screen equipment {#health-and-safety}

Although using a computer may not seem like a particularly risky activity you should be aware that you can suffer ill effects if you work at a computer for long periods without observing a few sensible precautions. The standard guidelines are as follows:

-   Make sure that your equipment is properly adjusted:

    -   ensure that your lower back is well supported by adjusting the
        seat back height

    -   adjust your chair seat height so that your forearms are level
        when using the keyboard

    -   make sure that the front edge of the keyboard is at least 8-10
        cm away from the edge of the desk

    -   if you are using a mouse, have it far enough away from the edge
        of the desk so that your wrist is supported whilst you use it.
        If you can learn to use the mouse with either hand then this can
        help avoid strains

-   Do not have your screen positioned in such a way that there is glare
    or reflections from the windows or room lights on the screen.

-   Maintain good posture.

-   Take regular breaks away from the computer. It is recommended that
    you take about 10 minutes break every hour.

Most Departments will have a Display Screen Trainer or Advisor, who can offer specific advice if you are using a display screen for a substantial amount of time, or if you experience, or anticipate, specific problems.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Syllabus

## Statistical Principles

-   Recognise the difference between a statistical population, a population parameter, a sample from the population, and a point estimate of the parameter.

-   Explain what sampling error is (in non-technical terms), and understand why it is necessary to quantify this alongside a point estimate.

-   Recognise the difference between the distribution of a sample, and the sampling distribution of an estimate derived from that sample.

-   Recognise the difference between the standard deviation (a property of a sample) and the standard error (a property of a sampling distribution).

-   Calculate the standard error of a sample mean when the population distribution of a variable follows a normal distribution.

-   Understand what is meant by the term 'null hypothesis' and be able to state the null hypothesis for standard statistical tests.

-   Identify whether or not a result is 'statistically significance' by examining the *p*-value it produces. 

You are not expected to be able to explain or use the bootstrap or permutation tests---these were introduced to help you learn the principles listed above.

## Simple parametric statistics

-   Understand that a one-sample *t*-test is used to assess whether a population mean is different from a particular reference value (often 0).

-   Understand that a two-sample *t*-test is used to assess whether two population means are different from one another. 

-   Understand that a paired-sample *t*-test is used to assess whether the mean difference among paired cases is different from reference value (usually 0). 

-   Given an experimental scenario and question, choose the correct *t*-test to use to answer the question.

-   List the assumptions of the one-sample, two-sample, and paired-sample *t*-tests, and explain how you might check them for a given problem, using R.

-   Carry out a one-sample, two-sample or paired-sample *t*-test using the `t.test` function in R, and be able to interpret the output produced by `t.test`.

-   Write an informative and concise summary of the results from a one-sample, two-sample, or paired-sample *t*-test.





<!--chapter:end:000_syllabus.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Programming prerequisites

This chapter gives a quick overview of the prerequisate R skills needed for this course (we studied these last year). We will use these skills this year, so you may need to spend revising them if you feel that you're a little rusty.

```{block, type='do-something'}
Remember, if you are an Environmental Sciences student joining us from Geography we *will* help you catch up in the first few weeks. The material in this section won't make any sense to you at the moment. We promise that it will after the first few weeks.
```

## Starting an R session in RStudio

1. Open up RStudio and set your working directory. You should do this via the RStudio menu system: **Session ▶ Set Working Directory ▶ Choose Working Directory...**. Make sure that you choose a sensible location. This is where you will store your data and R scripts, so it needs to be somewhere you can find and access again each time you use R. If you want to keep life simple, it is a good idea to use the same location in every practical.

2. Open up a new R script using the RStudio menu system: **File ▶ New File ▶ R Script**. Don't create a any other kind of file.

3. There are a couple of things that need appear at the start of every script. Add these to the top of your new script before you do anything else. You should always clear the workspace with `rm`, and load up any packages you plan to use:

```{r, eval=FALSE}
# clear the workspace so that we have a 'clean sheet'
rm(list = ls())

# load and attach the packages we want to use...
# 1. 'dplyr' for data manipulation
library(dplyr)
# 2. 'ggplot2' for plotting
library(ggplot2)
```

4. Now run the preamble section of the script, i.e. highlight everything and hit **Ctrl+Enter**. If the `library` commands didn't work it suggests that you have not previously installed the relevant package. Install the package (see below) and try rerunning the script.

5. Once the pramble bit of the scruipt is working you should save the script. Look at the label of the tab the scripr lives in. This will probably be called something like *Untitled1*, and it the label will be red. This is RStudio telling you that you have not saved the file yet.  

You are now ready to start developing your new script.

## Using packages

R packages extend the basic functionality of R so that you can do more with it. A package bundles together R code, data, and documentation in a way that is easy to use and share with other users. Last year we learned how to use some of the functions provided by the `dplyr` package (for data manipulation) and the `ggplot2` package (for making plots). We are going to use the `dplyr` and `ggplot2` packages again this year, so you need to understand R's package system in order to access these. You can revise how to use the package system in the [packages](http://dzchilds.github.io/aps-data-analysis-L1/help-packages.html) topic. It isn't difficult to use, and we will obviously help you if you run into difficulties. 

Installing a package is done via the `install.packages` function, e.g.
```{r, eval=FALSE}
install.packages("dplyr")
```
Loading and attaching the package a package happens via the `library` function, e.g.
```{r, eval=FALSE}
library("dplyr")
```
The key point---which seems to cause endless confusion---is that installing a package, and then loading and attaching the package, are different activities. You only have to install it once onto your computer, but you have to load a package every time you want to use it in a new R session (i.e. every time you start up RStudio).

## Reading data into R

Last year we made extensive use of several datasets that reside inside various R packages. This was useful because it meant we could use the data without first reading it into R, meaning that we could concentrate on developing your R skills rather than fixing data input errors. We don't have the luxury of doing this when we work with our own data, and so this year, we will adopt more realistic practises. Whenever you need to work with a dataset, you will have to first download it (from MOLE), and then read it into R. Each dataset is stored as a Comma Separated Value ('CSV') text file, and so you will need to use the `read.csv` function to read it in. You can revise how all of this works in the relevant section of the [data frames]({{site.baseurl-L1}}/data-frames.html#access-data) topic. 

## Data frames

When you read data into R using a function like `read.csv`, it places that data into a data frame. The data frame is the most important type of object in R. Remember, a data frame is table-like object that collects together different variables, storing each of them as a named column. We can access the data inside a data frame by referring to particular columns and rows. You can revise how to work with data frames in the [data frames]({{site.baseurl-L1}}/data-frames.html#access-data) topic. The main things to remind yourself about are the `View` function for inspecting a data frame, and the `$` operator for accessing a single column of a data frame. You might also want to revise the `tbl_df` and the `glimpse` functions from the `dplyr` package (but this is not critical).

## Anything else? 

We will use functions from the `dplyr` package from time-to-time to manipulate data, and we will use the `ggplot2` package to make plots of our data and summarise statistical models. However, we will remind you which functions you need to use to solve a particular problem as the course unfolds, so there is no need to revise all of this material now. If you want to be extra-prepared, our advice is to work through the [quick introduction to ggplot2]({{site.baseurl-L1}}/ggplot2-intro.html#quick) to remind yourself of the logic of plotting with `ggplot2`.

<!--chapter:end:010_R_prerequisites.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Statistical Concepts {-} 

# The scientific process

> *There is something fascinating about science. One gets such a
> wholesale return of conjecture out of a trifling investment of fact.*
>
> Mark Twain
>
> *To do science is to search for patterns, not simply to accumulate
> facts.*
>
> Robert MacArthur

## Stages in the scientific process {#stages}

Science is about asking, and answering, the right questions. Within this process a number of distinct stages usually occur: making observations, asking questions, formulating hypotheses, and testing predictions. Collectively these are the building blocks of what is known as the scientific method. Exactly how they fit together, and what the philosophical and practical limitations of different approaches are, have been the subject of much debate by philosophers of science. We are not going really going to tackle those issues here, fascinating though they are, but instead try to extract a general working framework for the process of a typical scientific investigation.

### Observations {#observations}

**Observation** — *information, or impression, about events or objects*.

In general the questions we ask are not generated by pure abstract thought, but are a result of observations about the natural world. These may take the form of direct observations we make ourselves, patterns that crop up in data collected for other purposes, in non-specific surveys, and the previous work, or accumulated information, of other people.

So, while pottering around in a stream one day, you notice that the freshwater ‘shrimps’ (*Gammarus*) that abound in the stream seem to occur almost entirely under stones; you rarely seem to see them when you just watch a patch of open stream bed. Having made observations, it may be necessary to collect some more data to check that this phenomenon is not just a one-off event, or a false impression. Look under a few more
stones, watch the same species another day, or in another place, check the literature for similar observations by others.

Such observations of biological systems will lead almost automatically into asking questions.

### Questions {#questions}

**Question** — *what it is that you want to know; the scope of your investigation*.

e.g., Why does *Gammarus* spend most of its time under stones?

You should try and make your question reasonably focused - the overall aim of your study is to answer this question. The question of why the tropics are more diverse than the temperate regions is a vast topic - so
even though it is a valid (and fascinating) question, it may not be a good choice for a final year project or even a PhD!

The next stage is to formulate an hypothesis.

### Hypotheses {#hypotheses}

**Hypothesis** - *an explanation proposed to account for observed facts.*

In general, in biology, the important distinguishing feature of an hypothesis is that it specifies some biological process, or processes, which might account for the observations made. One question will often
generate more than one hypothesis:

*Gammarus* occur under stones because:

-   they need to shelter from the current

-   their food (leaf litter) gets trapped and accumulates under stones

-   they are subject to predation by visually hunting fish and need to remain out of sight

Formulating hypotheses requires more than just a restatement of the question - it usually embodies some mechanism (though in some cases this may not be fully understood) and it will often draw on additional information (e.g., the fact that *Gammarus* feed on dead leaves).

### Predictions {#predictions}

**Prediction** — *what you would expect to see if the hypothesis was true*.

Hypotheses are about proposing explanations, but they might not be directly testable; that is they may not tell you what data to collect, or what pattern to expect in the data. To be able to test an hypothesis you need to make some predictions from that hypothesis. These will be determined both by what you expect to see and what it is possible, or practical, to measure. A prediction is not simply a rephrasing of the hypothesis - it should more or less give you a statement of the experiment to conduct or observation to make, and type of data to
collect:

-   *Shelter hypothesis:* a greater proportion of *Gammarus* should be
found in the open in streams with slow flow, or in slower flowing
areas of a stream.

-   *Food hypothesis:* *Gammarus* should not aggregate under stones from
which all leaf litter has been removed; *Gammarus* should aggregate
on patches of leaf litter tethered in the open part of the stream
bed.

-   *Predation hypothesis:* *Gammarus* should aggregate under stones
more in streams where fish are present than where they are not;
*Gammarus* may spend less time under stones at night.

Ideally you are looking for a prediction that is unique to the hypothesis it is based on - so if the prediction is true only one of the hypotheses could have been responsible, but this may not always be possible and some combination of predictions may need to be used. Additionally, several processes may be operating at the same time. This makes hypothesis testing harder still. It may be necessary to consider two or more hypotheses, and their corresponding predictions, in combination. For example, *Gammarus* may be under stones because it prefers the sheltered environment, but also because food accumulates there. In this case we might expect that *Gammarus* will show a weak aggregative response to shelter alone, or food alone, and a stronger one
to them both together.

## Hypothesis testing

Once we have firmed up our questions, hypotheses, and predictions we can collect the data to evaluate our ideas. On the basis of these data we will either accept or reject the various hypotheses. The important thing to realise about the process of hypothesis testing is that, in science especially, hypotheses are either rejected, or not rejected, but an hypothesis can rarely, except in trivial cases, be proved.

This seems like an odd state of affairs! True, but it does make sense. Since you cannot be sure that you have thought of all the possible hypotheses to explain an observation, finding evidence that supports the prediction from one your hypotheses, does not guarantee that the hypothesis is the only one which could have produced the effect you find. On the other hand, if you find evidence that directly contradicts the prediction(s) from your hypothesis, you can be certain (assuming the prediction and data are not flawed) that the hypothesis cannot be true.

An hypothesis which predicted that all conifers should be evergreen could be supported by numerous observations of different conifer species in forests around the world, but is conclusively refuted by the first larch tree we encounter.

Having tested your hypothesis, by examining the evidence that its predictions are true, you may accept it as the best (so far) explanation of the observations, or you may reject it as an explanation, and turn to other hypotheses. The same procedure must then be repeated for these hypotheses.

This basic cycle of proposing hypotheses and then seeking evidence potentially capable of falsifying them, is, in essence, the idealized model of the scientific process famously proposed by the philosopher of science Karl Popper (1902-1994). It is often termed *falsificationism*.

## Don’t we ever know anything for sure? {#are-we-sure}

The method presented here provides a view of science as one in which we suggest hypotheses, then test them trying to reject them by finding conclusive counter-evidence, then replacing them with new hypotheses. It all sounds a bit frustrating. In fact of course we do ‘accept’ hypotheses all the time — that is we fail to reject them over and over again. These hypotheses become more accepted and in some sense become regarded as ‘true’ if repeated attempts to test them all fail to provide good counter evidence. In other words, we have some ideas that are doing pretty well in terms of resisting falsification, and we use these as our best estimates of the truth, with the proviso that it is still possible a better idea will come along in due course.

The simple process of falsification described above also presents a picture of scientists as wonderfully neutral, objective creatures, rationally proceeding through cycles of setting up hypotheses, testing them, rejecting them, cheerfully setting them aside and starting over again. Of course this is not a true reflection of the complex, often messy, business really involved in trying to figure out how the world works. Philosophers of science have argued long and hard about how far from this idealized process real science actually is. Various alternative philosophies suggest more ‘realistic’ processes, such as Thomas Kuhn’s view of science as periods of relative stasis, where people work within an accepted paradigm (a set of views about how things work) despite accumulating evidence that doesn’t always support the paradigm, until finally it is upset by a ‘revolution’ which rejects the entire paradigm, and proposes a new view. The philosopher Imre Lakatos proposed some resolution of these views, suggesting that scientific ideas were grouped together in ‘research programmes’ concerned with particular endeavours, and that within these there may be core ideas that are not challenged, but other related ideas which are being challenged and adjusted by falsification, and that together these make each research programme progress. Programmes that don’t progress should be abandoned in favour of those that do.

Of course that is a very over-simplified sketch of some important ideas, which are well worth reading a bit about, but in practice these philosophical arguments are really more focused on how whole areas of science develop. When you are just thinking about constructing a simple study of one problem, then the basic falsification cycle is a pretty good approach to have in your mind. Even in it’s simple form, however, it is not immune from the effect of human fallibility (see below).

The process laid out here is not a strict set of rules, but outlines an approach to scientific investigation which is widely considered to provide a rigorous and productive system. As with all such systems understanding the ’normal’ process is a prerequisite for constructively breaking the rules.

```{block, type='advanced-box'}
**It's hard to reject an hypothesis you love!**

An interesting aside on the process is given by Sutherland (1994) who suggests that, in everyday life at least, we are often very reluctant to deliberately seek evidence that might refute a hypothesis we have formulated, and often persist in holding on to the hypothesis even when the evidence is against us.  This can be seen in experiments where people are presented with a sequence of numbers (say 2, 4, 6) and have to try and establish the general rule to which the set of numbers conforms.  The subject decides on an initial rule and then can test this rule by suggesting other sets of numbers and being told whether or not those numbers conform to the rule. 

The initial guess at a rule is usually 'even numbers in ascending order', and the initial test suggestions are typically another set of even numbers ascending by two  (e.g. 16, 18, 20). These, the subjects are told, conform to the rule.  The next set of numbers suggested is then often another similar set (40, 42, 44) -- which again conforms, and sometimes another similar guess will follow. However, the suggestion of further sets of even numbers ascending by two cannot test this (most simple rules that allow 2, 4, 6 will allow the other sequences too). A better first suggestion would be to change one of the components of the initially proposed rule e.g. even numbers to odd numbers: 3, 5, 7;  or the increment:  2, 8, 14.  If these conform we can reject a specific component of our initial rule. (The rule is, in fact, simply any ascending sequence of numbers.)
```

## Further reading {#further-reading}

Barnard C, Gilbert F and McGregor P (1993) *Asking questions in biology*. Longman.

Ladyman, J (2002) *Understanding the philosophy of science*. Routledge.

Sutherland S (1994) *Irrationality*. Penguin.

<!--chapter:end:020_asking_questions.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Data and variables

> *The truth is the science of nature has been already too long made
> only a work of the Brain and the Fancy. It is now high time that it
> should return to the plainness and soundness of Observations on
> material and obvious things.*
>
> Robert Hooke (1665)
>
> *The plural of anecdote is not data.*
>
> Roger Brinner

## “Observations on material and obvious things”

As Hooke’s observation suggests, science cannot proceed on theory alone. The information we gather about a system both stimulates questions and ideas about it and, in turn, can also allow us to test these ideas. In fact the idea of measuring and counting things is so familiar to us that it is easy to start a project without giving much thought to something as apparently mundane as the nature, quantity and resolution of the data we intend to collect. It is worth considering, however, as the nature, quantity and quality of the data in a study determine both the types of analyses that can be carried out, and the confidence we can have in any conclusions that are drawn. We will spend quite a lot of time considering the statistical tools that can help you extract information from your data, but no statitical wizardry can extract information that isn’t somewhere in the data to begin with.

So what is there to say about data? The first point to note is that, properly, the word data is the plural of datum (a single, often numerical, piece of information) ...so we should say “the data are...” not “the data is...”. However, the use of the word in the singular is becoming widespread, and you will commonly hear it used in this way. [Grammar Nazis](http://www.urbandictionary.com/define.php?term=Grammar%20Nazi) really don't like this though, so it's worth knowing what the "correct" subject-verb agreement looks like if you want to avoid incurring their wrath.  

The second point is that there are many different sorts of data. Examples include spatial maps of the occurance of a particular species and environmental variables, DNA sequences or even the whole genomes of individuals, and networks of feeding relationships among species (i.e. food webs). These kinds of data can be very challenging to analyse. In this course we are concerned with relatively simple kinds of data. 

When we collect data it is typically organised as a set of one or more related statistical **variables**. Remember, statisticians use the word 'variable' to refer to any characteristic that can be measured, counted or experimentally controlled. Collectively, a set of related variables are referred to as a **data set** (or just 'the data', if we are feeling lazy). 

Confused? Let's look at an example. Consider the spatial map example above. A minimal data set might comprise two variables containing the x and y position of sample locations, a third variable denoting the presence / absence of a species, and one or more additional variables conataining information about the environmental variables we measured.

```{block, type='advanced-box'}
**Data and variables in R**

Remember what you learnt last year about data frames and vectors? When using R, we often store each data set in a **data frame**. Each column in the data frame is one of R's **vectors** --- numeric, character, etc. Remember the 'tidy data' concept from last year? The columns of a data frame should correspond to the statistical variables in our data. Each row corresponds to a single observation. This simple connection between abstract statistical concepts and the concrete objects in R is not coincidence -- R was designed first and foremost to analyse data.
```

## Revision: Types of variable {#var-types}

Again, because we handle data of one sort or another so frequently, we often don’t stop and think about exactly what kind of data we are using. Most of the time that doesn’t cause too much of a problem. However, when you come to design your own studies, and analyse your own data, it can be very important to understand what sort of data you need, or have, as it can affect what information you can extract from it.

Last year we learned that the variables that comprise a data set can be classified as being either __numeric__ or __categorical__: categorical variables have values that describe a characteristic of an observation, like 'what type' or 'which category'; numeric variables have values that describe a measurable quantity as a number, like 'how many' or 'how much'. Categorical variables can be further characterised according to whether or not they have a natural order (__nominal__ vs. __ordinal__ variables), and numeric variables can be further characterised according to the type of scale they are meaured on (__interval__ vs. __ratio__ scales).

Let's review these classifications.

### Nominal (categorical) variables

Nominal variables arise where observations are recorded as categories which have no natural ordering relative to each other. For example:

---------------------  --------- ------------------
  **Marital status**    **Sex**   **Colour morph**
        Single           Male           Red
        Married          Female         Yellow
        Widowed                         Black
        Divorced                 
---------------------  --------- ------------------  

Data of this type are common in surveys where, for example, a record is made of the species found at each site.

### Ordinal (categorical) data

Ordinal variables occur where observations can be assigned some meaningful order, but where the exact numerical relationship between items in the order are not necessarily fixed, the same, or even known. For example If you are studying the behaviour of an animal when it meets another individual it may not be possible to obtain quantitative data about these interactions, but you can score the behaviours you see in order of
aggressiveness:

  -------------------- -----------
  **Behaviour**         **Score**
  initiates attack          3
  aggressive display        2
  ignores                   1
  retreats                  0
  -------------------- -----------

Rank orderings are also ordinal data. For example the order in which runners finish a race (1st, 2nd, 3rd, etc..) is a rank ordering it doesn’t tell us whether it was a close finish or not, but still conveys important information about the result.

In both situations you can say something about the relationships between categories: in the first example, the larger the score the more aggressive the response; in the second example the greater the rank the slower the runner. However, you can’t say that the gap between the first runner and the second was the same as between the second and third (even though 2-1=3-2) and you can’t say that a score of 2 is twice as aggressive as a score of 1.

```{block, type='warning-box'}
**How should you code different categories?**

We always have to define some kind of coding scheme to represent the different categories of a nominal/ordinal variables. It was once common practise to assign numbers to different categories (e.g. Female=1, Male=2) for handling data in computerised form. This method was sensible in the early days of computer-based data analysis because it allowed data to be stored efficiently---numbers take up less space in memory than words. However, this efficiency argument is much less relevant on a modern computer with many Gb of memory. There *are* good reasons to avoid numeric coding schemes though:

-   Numeric coding makes it harder to understand your raw data and to interpret the output of a statistical analysis of those data, because you have to remember which number is associated with each category. This is particularly problematic when a variable has many categories.

-   Numeric codes are arbitrary and should not be used as numbers for mathematical operations. For example, it is meaningless to say 2 ("male") is larger than 1 ("female"), or $2 + 1 = 3$. R has a special way of representing categorical variables (called 'factors') so it assumes that any variable containing numeric values is meant to treated as a number. 

So here's the warning: **always** use words (e.g., 'female' vs. 'male'), not numbers, to describe the different categories when you are preparing your data for analysis. You are much more likely to make a silly mistake when carrying out an analysis if you don't do this, because R will try to treat the offending categorical variable as a number.
```

### Interval scale (numeric) variables

Interval scale varaibles take values on a consistent numerical scale but where that scale starts at an arbitrary point.

Temperature on the Celsius scale is a good example of interval data. You can say that 60$^{\circ}$C is hotter than 50$^{\circ}$C. You can also say that the difference in temperature between 60$^{\circ}$C and 70$^{\circ}$C is the same as that between -20$^{\circ}$C and $-10^{\circ}$C. However you cannot say that 60$^{\circ}$C is twice as hot as 30$^{\circ}$C because temperature on the Celsius scale has an artificial zero value (the freezing point of water). This point becomes obvious when you consider that temperature can equally well be measured on the Fahrenheit scale (where the freezing point of water is 32 degrees). There is a temperature scale which has a true zero: the Kelvin scale. Zero K is absolute zero, where a substance actually has no thermal energy whatsoever. So temperature in degrees K would not be interval data.

You can add and subtract data measured on an interval scale but you cannot divide or multiply such data (and get a meaningful result). 

### Ratio scale (numeric) variables

Ratio scale variables have a true zero and known and consistent mathematical relationship between any points on the measurement scale. Temperture measurements in degrees K are on a ratio scale, i.e. it makes sense to say that 60 K is twice as hot as 30 K.

These are the variables we are most used to, because physical quantities are often measured on a ratio scale. For example, length, weight, or numbers of organisms are usually measured on a ratio scale. You can add, subtract, multiply and divide this sort of data and get meaningful results.

```{block, type='advanced-box'}
**Continuous or discontinuous?**
A common confusion with numeric data concerns whether the data are on continuous or discontinuous scales. Ratio data can be either. Many biological ratio data are discrete, and therefore discontinuous (i.e. only certain discrete values are possible in the original data).  Count data are the most obvious example, e.g. the number of eggs found in a nest, the number of plants recorded in a quadrat, or number of heartbeats counted in a minute; these can only comprise whole numbers, 'in between' values are not possible. However, the distinction between continuous and discontinuous data is often not clear cut -- even 'continuous' variables such as weight are made discontinuous in reality by the fact that our measuring apparatus is of limited resolution (i.e. a balance may weigh to the nearest 0.01 g). The fact that data may be discontinuous does not mean they are necessarily ordinal data.
```

### Which is best?

All types of data can be useful but it is important to be aware that not all types can be used with all statistical models. This is one very good reason for why it is worth having an idea of the statistical tools you intend to use when designing your study.

In general, ratio data is the data type best suited for statistical analysis. But biological systems often cannot be readily represented as ratio data, or the work involved in collecting good ratio data may be vastly greater than the resources allow, or the question we are interested in may not demand ratio data to achieve a perfectly satisfactory answer.

It is this last question that should really come first when thinking about a study. What sort of data do we need to answer the question we are interested in? If it is clear at the outset that data on a rank scale will not be sufficiently detailed to enable us to answer the question then we must either develop a better way of collecting the data, or abandon that approach altogether. If you know the data you are able to collect cannot address the question, then you would be better doing something else, so it is good to work that out in advance.

And an obvious, but important point: you can always convert measurements taken on a ratio scale to an interval scale, but you cannot do the reverse. Similarly, you can convert interval scale data to ordinal data, but you cannot do the reverse. In general, it is a good idea to avoid such conversions if you can, as they inevitably result in a loss of information.

## Distributions {#revision-distributions}

We touched on the idea of a distribution last year: in statistics, a distribution is a statement about the frequency with which different values of a variable are observed. 

The mean and variance of a population are both examples of population parameters. These describe the central tendancy ('which values are most common?') and the dispersion ('how variable are the values') of a distribution.


## Accuracy and precision {#accuracy-precision}

### What do they mean?

The two terms accuracy and precision are used more or less synonymously in everyday speech, but in scientific investigation they have quite distinct meanings.

**Accuracy** – how close a measurement is to the true value of whatever it is you are trying to measure.

**Precision** – how repeatable a measure is (also to how fine a resolution a measurement can be made), irrespective of whether it is close to the actual value.

If you are measuring an insect’s weight on an old and poorly maintained balance, which measures to the nearest 0.1 g, you might weigh the same insect several times and each time get a different weight — the balance is not very precise, though some of the measurements might happen be quite close to the real weight. By contrast you could be using a new electronic balance, weighing to the nearest 0.01g, but which has been incorrectly zeroed so that it is 0.2 g out from the true weight. Repeated weighing here might yield results that are identical, but all incorrect (i.e. not the true value) — the balance is precise, but the results are inaccurate.

The analogy often used is with shooting at a target:

** NEED TO SHOW THE "targets.png"" FILE HERE **

It is obviously important to know how accurate and how precise your data are. The ideal is situation in the top left target in the diagram, but in many circumstances high precision is not possible and it is usually preferable to make measurements of whose accuracy you can be reasonably confident (bottom left), than more precise measurements, whose accuracy may be suspect (top right). Taking an average of the values for the bottom left target would produce a value pretty close to the centre; taking an average for the top right target wouldn’t help your accuracy at all (though the repeatability of the values might well give you spurious confidence in the data).

It is also worth being aware that when you state results, you are making implicit statements of the precision of the measurement.

### Implied precision - significant figures

The number of significant figures you use suggests something about the precision of the result. A result quoted as 12.375 mm implies the measurement is more precise than one quoted as 12.4 mm. A value of 12.4 actually measured with the same precision as 12.735 should properly be written 12.400. When quoting results look at the original data to decide how many significant figures to use - generally the same number of significant figures will be appropriate.

If you are working with discrete data these considerations do not apply in quite the same way, e.g. precision of measurement is not an issue in recording the number of eggs in a nest. You use 4 not 4.0, but since 4 eggs implies 4.0 eggs you would be correct to quote average clutch size from several nests as 4.3 eggs. However, even with discrete data, if numbers are large then obviously precision is an issue again ... a figure of 300 000 ants in a nest is likely to imply a precision of plus or minus 50 000. A figure of 320987 ants implies a rather improbably precise measurement (nobody will believe you actually counted them all!).

### How precise should measurements be?

The appropriate precision to use when making measurements is largely common sense. It will depend on practicality (it may not be possible to weigh an elephant to the nearest 0.001g) and the use to which you wish to put the data (if you want to know whether the elephant will cause a 10 tonne bridge to collapse then the nearest tonne will be good enough, if you want to compare the mean sizes of male and female elephants then the nearest 100 kg may be sufficient, if you want to monitor the progress of a pregnant female elephant then the nearest 10 kg or less might be desirable).

As a rough guide aim, where possible, for a scale where the number of measurement steps is between 30 and 300. So for example, in a study of the variation in shell thickness of dogwhelks on a 300 m transect up a shore, it would be adequate to measure the position of each sampling point on the transect to the nearest metre, but shell thickness will almost certainly need to be measured to the nearest 0.1 mm.

### Error, bias and prejudice

Error is present in almost all biological data, but not all error is equally problematic. Usually the worst form of error is bias. Bias is a systematic lack of accuracy, i.e. the data are not just inaccurate, but all tend to deviate from the true measurements in the same direction (situations B and D in the ‘target’ analogy above). Thus there is an important distinction in statistics between the situation where the measurements differ from the true value at random and those where they differ systematically. Measurements lacking some precision, such as the situation illustrated in C, may still yield a reasonable estimate of the true value if the mean of a number of values is taken.

Avoiding bias in the collection of data is one of the most important skills in designing biological (or other) investigations. Some forms of bias are obvious, others more subtle and hard to spot. Some sources of bias in biology include:

-   *Non-random sampling*. Many sampling techniques are selective, and may result in biased information. For example pitfall trapping of arthropods will favour collection of the very active species, which encounter traps most frequently. Studying escape responses of an organism in the lab may be biased since the process of catching organsims to use in the study may have selected for those whose escape response is poorest.

-   *Conditioning of biological material*. Organisms kept under particular conditions, especially in a laboratory, for periods of time may become acclimatised to conditions unlike those they normally encounter, or if kept in a laboratory for many generations characteristics may change through natural selection. Such organisms may give a biased impression of the behaviour of the organism in natural conditions.

-   *Interference by the process of investigation*. Often the process of making a measurement itself distorts the characteristic being measured. For example it may be hard to measure the level of adrenalin in the blood of a small mammal, without affecting the adrenalin level in the process. Pitfall traps are often filled with a preservative, such as ethanol, but the ethanol attracts species of insect that normally feed on decaying fruit and use the fermentation products as a cue to find resources.

-   *Investigator bias*. Measurements can be strongly influenced by conscious or unconscious prejudice on the part of the investigator. We rarely undertake studies without some initial idea of what we are expecting, or we form ideas about the patterns we think we are seeing as the study progresses. This can introduce bias. For example, rounding up ’in between’ values in the samples you are expecting to have large values and rounding down where a smaller value is expected, or having another ’random’ throw of a quadrat when it doesn’t land in a ’typical’ bit of habitat.

The ways in which biases, conscious and unconscious, can affect our investigations are many, often subtle, and sometimes serious. Sutherland (1994) gives an illuminating and sometimes frightening catalogue of the ways in which biases affect our perception of the world and the
judgements we make about it.

The message is that the results you get from your investigation must always be judged and interpreted with respect to the nature of the data that were used to derive them – if the data are suspect, then the results will be suspect too.



<!--chapter:end:030_data_variables.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Learning from data

> Statistics is the science of learning from data, and of measuring, controlling, 
> and communicating uncertainty; and it thereby provides the navigation essential 
> for controlling the course of scientific and societal advances
> 
> [Davidian and Louis (2012)](https://doi.org/10.1126/science.1218685)

The particular flavour of statistics we use in this course is called 'frequentist statistics'. It's not really important that remember that phrase, or indeed, from a practical perspective, that you even know you're using frequentist statistics. However, if you're the kind of person who likes to understand how things work, it *is* helpful to at least get a rough sense of how frequentist statistics works. The goal of this, and the next few chapters, is to provide this overview. It's *not* critical for all of this to make perfect sense---you certainly won't be assessed on your ability to explain how frequentist statistics works. However, if you can wrap your head around the core ideas you will find it easier to understand the output from a test.

We're going to start, in this chapter, by laying out a somwhat simplified overview of the steps involved in doing (frequentist) statistics. We'll also introduce a few key ideas and definitions along the way. Later chapters will drill down into the really important ideas--- things like sampling variation, standard errors, null hypotheses and *p*-values. 

## Populations {#populations}

When a biologist talks about a population they mean a group of individuals of the same species who interbreed. That definition, or at least something similar, should be familiar to you. What does a statistician mean when they talk about populations? The word 'population' has a different meaning in statistics. Indeed, it is a much more abstract concept: a statistical population is any group of items that share certain attributes or properties. 

This idea is best understood by example...

-   The readers of this book could be viewed as a statistical population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, and they tend to have similar educational backgrounds and career aspirations. As a consequence of these similarities, APS students tend to be more similar to one another than they would be to a randomly chosen inhabitant of the UK.

-   All the different areas of peatland in the UK comprise a statistical population. There are different peatland sites across the UK, and although their ecology varies somewhat from one location to the next, they are also very similar in many respects. For example, peatland is generally characterised by low-growing vegetation (blank bog, etc) and acidic soils. If you visit two different peatland sites in the UK, they will seem quite similar compared to, for example, a neighbouring calcareous grassland.

-   A population of plants or animals---as understood by biologists---can also be thought of as a statistical population. Indeed, this is often the kind of population organismal biologists are most interested in. The individuals that comprise a biological population share common behaviours, physiology and life history characteristics. Much of organismal biology is concerned with learning about these kinds of properties, often with the goal to explaining variation we see among individuals.

Populations are conceptualised as fixed but unknown quantities within the framework of frequentist statistics. The goal of an analysis is to learn something about one or more populations by collecting data.

Note that 'the population' is defined by the investigator, and the 'something we want to learn about' is anything we're interested in and know how to measure. Consider the examples again. A social scientist might be interested in understanding the political attitudes of undergraduates, so they might choose to survey a group of students in their university. A climate change scientist might measure the mass carbon is stored in peatland areas at sites across Scotland and northern England. A behavioural ecologist might want to understand how much time beavers spend foraging for food, so they might study one of the two Socttish populations.

What are the steps involed in these kinds of studies?

## Learning about populations

The examples we discussed above involve very different kinds of populations and questions. Nonetheless, there are fundamental commonalities in how these questions are addressed, which always involves collecting some data and applying the appropriate statistical tools. This process can be broken down into a number of different steps:

**Step 1: Refine your questions, hypotheses and predictions**

This step was discussed in [The scientific process] chapter. We won't spend much more time on it here. The key point to remember is that we should not start collecting data until we've set out the relevant questions, hypotheses and predictions. This might seem blindingly obvious, but it is surprising how often people don't get these things straight before setting out to collect data. Take our word for it, spending time collecting data without a clear scientific objective and rational for the work is a guaranteed way to waste your time.

**Step 2: Decide which populations is important**

The second step is to decide which population (or populations) we need to study. This is a more subtle problem than you might think. What constitutes 'the population' might be fairly obvious in many observational study, that is, studies that don't involve an experimental apparoach. For example, in each of the three cases considered above, the corresponding populations we choose to study might be undergraduate students in APS, peatland habitats from across the UK, and beavers in Scotland.

But what happens if we're planning an experiment? Imagine we want to test the prediction that nutrient addition reduces biodiversity in chalk grasslands. We could set up an experiment where we have two kinds of plots: 1) manipulated plots where we add fertiliser, and 2) control plots where we do nothing. Comparing these would allow us to assess the impact of adding nutrients on biodiversity. 

There are two statistical populations in this setting---control and manipulated communities. These *are defined by the experimental design* we adopted. The nutrient addition plots doesn't exist until you do the experiment, and even then, we want to be able to generalise our results beyond the one experiment. The weird mental contortion that a frequentist does is to imagine that the experimental plots are part of some larger, unobserved population of nutrient addition plots.

Don't worry too much if that is confusing (it is!). The important point is that, for any given problem, a relevant statistical population is something the investigator defines. It might be 'real', like the undergraduates in APS, or they might be something that doesn't even exist in a meanigful way, like a population of not-yet-realised experimentally manipulated plots.

**Step 3: Decide which variables to study**

The next step is to decide which features of the population we need to measure to address our question. In practise, this comes down to deciding which variable (or variables) you need to measure. In the examples above, the appropriate variables might be things like a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in the biological population.

This step is often reasonably straightforward, though some thought may be required to pick among different options. There isn't a whole of ambiguity associated with a physical variable like body mass, but something like 'political attitude' needs careful thought. Can we quantify this by studying just one thing, like voting patterns? Probably not.

Part of the art of designing a good data collection protocol is deciding what to measure. We discussed some of the considerations in the [Data and variables] chapter, but what really matters most is that we choose the right kind of variables to address the substantive research question.  

**Step 4: Decide which population parameters are relevant**

Once we have decided which variable(s) to study, we have to decide which 'population parameter' is relevant. A population parameter is simply a numeric quantity that describes a particular aspect of the variable(s) in the population. Actaully, to be more precise, it describes a feature of the distribution of the variable(s) in the population. 

A simple population parameter you are familiar with is the population mean. We often study means, because they allows us to answer questions like, "how much of something is present?". Much of this course is about asking questions of population means, though other population parameters may also be important though, e.g.

*   The goal of statistical genetics is to partition variablity among individuals---we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case, it is the population variance that we want to learn about. 

*   Sometimes we want to understand how two or more aspects of the population are related to one another. In this situation a correlation coefficient (more about this later) might be the right population parameter to focus on. 

**Step 5: Gather a representative sample**

If we could measure every object in a population we wouldn't need to use statistics. We could just calculate the quantity we needed using an exhaustive sample and we'd have our answer. In the real world we are faced with resource constraints, i.e. we have limited time and money to invest in a problem, no matter how important it is. This means we have to work with a **sample** of a population. 

A sample is just a subset of the wider population, which has been chosen so that it is representative of that population. That word 'representative' is very important. If we can't collect a representative sample it will be very difficult to infer anything useful about the population it came from. For example, if we aim to understand the reproductive characteristics of our favourite study organism, but we only sample young or old individuals, it will be impossible to generalise our findings if reproductive performance changes with age (which is almost always true).

The study of how to generate useful samples from a population is an important part of statistics. It falls under the banner of experimental design and sampling theory. These are large, quite technical topics, so it is well beyond the scope of this course to study them in any great deal. Nonethless, we will touch a few of the more important practical aspects as we move through this course, particularly in the [Experimental design] chapter.

**Step 6: Estimate the population parameter(s)**

Once we have a representative sample we can calculate something called a **point estimate** of the population parameter. Remember, the population parameter is unknown; that's why we collect samples. A point estimate is simply a number that represents our "best guess" at the true value of the parameter. For example, if we are interested in a population mean of a variable, then the obvious point estimate to use is the mean of the variable in the sample we collect (this is just "the average" you learned to calculate in school).  

By the way, people often just say/write "estimate" instead of "point estimate", because using "point estimate" all the time is tedious. The exact terminology isn't really all that important to be honest. We'll mostly use the word "estimate" from now on.

**Step 7: Quantify the uncertainty of estimate(s)**

A point estimate is virtually useless on its own. Why? Because it is always derived from a limited sample of the wider population. Even if we are very careful about how we sample a population, and we collect a really big sample, there is no way to guarantee that the composition of our sample exactly matches that of the population. Why is this important? It means that any point estimate we derive from a sample will always be imperfect, in the sense that it won't exactly match the true population value. 

So... there is always uncertainty associated with any estimate of a population parameter. What can we do about this? We have to find a way to *quantify that uncertainty*. This bit of the process can be tricky to understand, so we're going to spend a fair bit of time thinking about it in the [Sampling variation and standard error] chapter. We'll leave it there for now.

**Step 8: Answer the question!**

Once we have point estimates and measures of uncertainty we're in a position to start answering questions. We have to be very careful about how we go about this though. 

Let's say we want to answer a seemingly simple question, such as, "Are there more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?" We could go out and sample a number of sites, measure the stored carbon at each site, and then calculate the mean of these measurements. What can we conclude if that sample mean is 210 t h^-1^? Not much, at least not until we have a sense of how reliable that mean is likely to be. To answer our question, we have to know how to assess whether or not the difference we observe (210 - 200) was just a fluke. 

The tools we'll learn about in this course are designed to answer a range of different kinds of scientific question. Ultimately they all boild down to the same basic question: Is the pattern I see 'real', or is it instead likely to be a result of chance variation? To tackle this, we combine point estimates and measures of uncertainty in various ways. The good news is that statistical software like R will do all the hard work for us. We just have to learn how to interpret the results it gives us.

## A simple example {#morph-example}

```{r plant-sim-par, echo=FALSE}
set.seed(27081975)
nsamp <- 200
sampsize1 <- 20
sampsize2 <- 40
sampsize3 <- 80
index <- c(1,1,2,2,2)
prop.purp <- sum(index==1)/length(index)
```

The best way to begin getting some sense of how all this fits together is by working through an example. We'll finish this chapter by introducing this, and then skim through steps 1-6 above. The final two steps are sufficiently tricky that they need their own chapters.

Let's set up the example. Imagine we are working on a plant species that is polymorphic. There are two different 'morphs', a purple morph and a green morph. We can depict this situation visually with a map showing where the purple and green plants are located on a hypothetical landscape:

```{r plants-all, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Landscape showing the position of purple and green morphs'}
plantdata <- 
  data.frame(xloc  = runif(nsamp), 
             yloc  = runif(nsamp), 
             morph = sample(c("purple","green")[index], 100, replace = TRUE))
plttheme <- theme_get()
plttheme$axis.text <- plttheme$axis.ticks <- plttheme$axis.title <- element_blank()
baseplt <- ggplot(plantdata, aes(x = xloc, y = yloc, colour = morph, )) + 
           geom_point() + scale_color_identity() + coord_fixed() + plttheme
baseplt
```

These idealised data were generated using a simulation in R. The details of how we did this aren't important, but basically, we placed 'individuals' onto the landscape at random locations (every location is equally likely), and then assigned them purple morph status with a certain probability (we made them are green otherwise). We'll come back to the probability we used later.

Let's proceed as though this were a real situation... 

**Step 1: Refine your questions, hypotheses and predictions**

Let's assume that we've previously been studying a neighbouring population that exhibits the same polymorphism. W're fairly sure both populations were once connected, but habitat loss over the last few hundred years has significantly reduced gene flow between them. Our studies with the neighbouring population have shown that...

*   The colour polymorphism is controlled by a single gene with two alleles: a recessive mutant allele ('P') confers the purple colour, and the dominant wild-type allele ('G') makes plants green. Moreover, population genetic studies have shown that the two alleles are present in a ratio of about 1:1. 

*   There seems to be no observable fitness difference between the two morphs in the neighbouring population. What's more, about 25% of plants are purple, i.e. the alleles seem to be in [Hardy-Weinberg equilibrium](https://en.wikipedia.org/wiki/Hardy–Weinberg_principle). These two observations indicate that there is no selection operating on the polymorphism (it's 'neutral').

We've noticed that things are different in the new study population. The purple morph seems to be about as common as the green morph. What's more, some preliminary work indicates that they purple plants seem to produce more seeds.  Our hypothesis is therefore, that purple plants have a selective advantage in the new study population. The corresponding prediction is that the frequency of the purple morph will be greater than 25% in the new study population.

(This isn't the strongest test of our hypothesis by the way. Really, we need to study allele and genotype frequencies, not just phenotypes. Sadly, the government has pulled the research funding for genetic research on plant polymorphism since Brexit happened, so this is the best we can do)

**Step 2: Decide which population is important**

Our situation is made up, so questions about the statistical population are not hugely relevant to be honest. In reality, we would consider various factors, such as whether we can study the whole population or need to restrict ourselves to a smaller scale (e.g. within one corner of the population). Working at a large scale should produce a more general result, but it could also present a significant logistical challenge.

**Step 3: Decide which variables to study**

This step is easy in this example. We could measure all kinds of different attributes of our plants---biomass, height, seed production, etc---but to study the polymorphism, we only need to collect information about the colour of different individuals. This means we are going to be working with a nominal (i.e. categorical) variable, which takes two values: 'purple' or 'green'.

**Step 4: Decide which population parameters are relevant**

The prediction we want to test is about the purple morph frequency (or equivalently, the percentage or proportion of purple plants). Therefore, the relevant population parameter is the frequency of purple morphs in the wider population. We're going to collect 'data' so that we can learn about this unknown quantity.

**Step 5: Gather a representative sample**

A representive sample here is one in which every individual on the landscape has the same probability of being sampled (i.e. a 'random sample'). Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality, but it is at least easy to do in a simulation. Let's seen what happens if we sample `r sampsize1` plants at random...

```{r plants-samp1, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Sampling plants'}
sample1 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample1, colour = "red", shape = 1, size = 5)
freqs1 <- table(sample1$morph)
```

The new plot shows the original population of plants, only this time we've circled the sampled individuals in red.

**Step 6: Estimate the population parameter**

Estimating a frequency from a sample is simple enough. We can express a frequency in different ways. Let's use a percentage. We found `r freqs1["green"]` green plants and `r freqs1["purple"]` purple plants in our sample, which means our point estimate of the purple morph frequency is `r round(100*freqs1["purple"]/sampsize1)`%. This is certainly greater than 25%---the value of observed in the original population---but it isn't that far off. 

Maybe the purple plants aren't at a selective advantage after all? Or maybe they are? We're going to eventually see how to use a statistical test to rigourously evaluate our prediction, but first we need to learn a few more concepts. Time to learn about something called sampling variation...

```{r, echo=FALSE}
save(plantdata, sampsize1, sampsize2, sampsize3, 
     file = "./_rda_objects/plant_morphs.rda")
```





<!--chapter:end:040_learning_data.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Sampling error

```{r, echo=FALSE}
load(file = "./_rda_objects/plant_morphs.rda")
```

In the previous chapter we introduced the idea that every point estimate of a population parameter will be imperfect, in the sense that it won't exactly reflect the true value of that parameter. This uncertainty is always present, it's not enough to have estimated something. We have to know something about the uncertainty (i.e. the precision) of the estimate. We use the machinery of statistics to quantify this uncertainty, because once we have done that, we can start to provide meaningful answers to our scientific questions. 

We will finally get to 'the answer' step in the next chapter But to do this, we have to develop the uncertainty idea a bit more. We need to learn about sampling error, sampling distributions and standard errors.

## Sampling error

Let's carry on with the plant polymorphism example introduced in the last chapter, i.e. the green plants vs. purple plants---skim back over the example if you can't remember it. We had taken one sample of 20 plants from our hypothetical population and found the the frequency of purple plants in that sample was `r round(100*freqs1["purple"]/sampsize1)`%. This is a point estimate of purple plant frequency based on a random sample of `r sampsize1` plants. What happens if we repeat the same process, leading to a new, completely independent sample? Here's a reminder of what the population looked like, with the the new sample highlighted with red circles:

```{r plants-samp2, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Plants sampled on the second occasion'}
sample2 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample2, colour = "red", shape = 1, size = 5)
freqs2 <- table(sample2$morph)
```

This time we ended up sampling `r freqs2["green"]` green plants and `r freqs2["purple"]` purple plants, so our second estimate of the purple morph frequency is `r round(100*freqs2["purple"]/sampsize1)`%. This is quite different from the first estimate. Notice that this second estimate of purple morph is frequency actually lower than that seen the original study population. Our hypothesis that it is higher is beginning to look a little shaky...

Note that nothing about the study population changed between the first and second sample. What's more, we used a completely reliable sampling scheme to generate these samples,
. There was nothing biased or 'incorrect' about the way individuals were sampled---every individual had the same chance of being selected. The two different estimates of the purple morph frequency simply arise from chance variation in selection. This variation, which arises whenever we observe a sample instead of the whole population, has a special name. It is called the **sampling error**.

(Another name for sampling error is 'sampling error'. Which one is better? Neither really. We tend to use both terms---'sampling error' and 'sampling error'---in this book because they are both widely used)

Sampling error is the main reason why we use statistics. Any estimate you derive from a sample will be affected by it. Sampling error is not really a property of any particular sample. Sampling error in any given problem is a function of: 1. the population distribution of the variable(s) we're studying, and 2. the sampling method used to investigate this. This may all seem a little cryptic now. Don't worry, we will start to get a sense of what it really means in this chapter.

## Sampling distributions

We can develop our simple simulation example a bit more to explore the consequences of sampling error. However, rather than taking one sample at a time, this time we'll use R to simulate 1000s of independent samples. The number of plants sampled ('n') will always be `r sampsize1`. Here's the important bit: every sample is drawn from the same population, i.e. the population parameter (purple morph frequency) never changes across samples. This means all the variation we observe is due to nothing more sampling error.

Here is a summary of one such repeated simulation exercise:

```{r samp-dist-1, echo = FALSE, out.width='100%', fig.asp=0.5, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 20)'}
x <- rbinom(n = 100000, size = sampsize1, prob = prop.purp)
out <- data.frame(n.purple = factor(x, levels = as.character(0:sampsize1)))
limits <- as.character(0:sampsize1)
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = limits, drop = FALSE) + 
  xlab("No. of purple morph individuals (n = 20)") + ylab("Count")
```

This bar plot summarises the result from 100000 samples. In each sample, we took `r sampsize1` individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, ... purple individuals, all the way up to the maximum possible (`r sampsize1`). We could have converted these numbers to frequencies, but instead we're just summarising the raw distribution of purple morph counts that found.

This distribution has a special name. It is called a **sampling distribution**. The sampling distribution is just the distribution we expect a particular estimate, or statistic, to follow. In order to to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. Rather than use mathematical reasoning, we used brute-force simulation to approximate the sampling distribution of purple morph counts that arises when we sample `r sampsize1` individuals from our hypothetical population. 

What does the sampling distribution show? It shows us the range of outcomes to expect when we repeat the same sampling process over and over again. The most common outcome is 8 purple morphs, which would yield an estimate of 8/20 = 40% for the purple morph frequency. This is the frequency that was actually used to simulate the data (we didn't tell you that before). The population parameter we're trying to learn about is 40%, and that turns out to be the most common point estimate we should expect to see under repeated sampling.

(So now we know the answer to question. The purple morph frequency is 40%. Of course we cheated though, because we used information from 1000s of samples. In the real world we only have one, limited sample.)

The sampling distribution is the key to 'doing statistics'. Look at the spread (dispersion) of the sampling distribution above. The range of outcomes is roughly 2 to 15, which corresponds to estimated frequencies of the purple morph in the range of 10-75%, because we sampled `r sampsize1` individuals on each occasion. This tells us that when we sample only 20 individuals, the sampling error is expected to be quite large.

Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error (to quantify uncertainty), and we can begin to make meaningful comparisons to address scientific questions. We don't have to work any of this out for ourselves---statisticians have done the hard work for us.

Let's think about what affects the sampling error next.

## The effect of sample size

One of the most important aspect of a sampling scheme is the **sample size** (denoted 'n'). This is just the number of observations (individuals, objects, items, etc) in a sample. To see how sample size influences the sampling distribution, and to understand why it matters, let's carry on with the simulation example. 

We'll repeat the multiple sampling process, but this time we'll do it with two different sample sizes. First we'll use a sample size of `r sampsize2` individuals, and then we'll take a sample of `r sampsize3` individuals each time. As before, we'll take a total 100000 samples in both cases:

```{r samp-dist-2, echo = FALSE, out.width='100%', fig.asp=0.5, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 40)'}
x <- rbinom(n = 100000, size = sampsize2, prob = prop.purp)
out <- data.frame(n.purple = factor(x, levels = as.character(0:sampsize2)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize2, 1)), 
                   breaks = as.character(seq(0, sampsize2, 2)),
                   drop = FALSE) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

```{r samp-dist-3, echo = FALSE, out.width='100%', fig.asp=0.5, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 80)'}
x <- rbinom(n = 100000, size = sampsize3, prob = prop.purp)
out <- data.frame(n.purple = factor(x, levels = as.character(0:sampsize3)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize3, 1)), 
                   breaks = as.character(seq(0, sampsize3, 4)), 
                   drop = FALSE) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

What do these plots tell us about the effect of changing sample size? Notice that we plotted each of them over the full range of possible outcomes (the x axis runs from 0-`r sampsize2` and 0-`r sampsize3`, respectively, in the first and second plot). This is so we can meaningfully compare the spread of each sampling distribution relative to the range of possible outcomes.

The range of outcomes in the first plot (n = `r sampsize2`) is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65%. The range of outcomes in the second plot (n = `r sampsize3`) is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. This suggests that if we increase the sample size we can expect to encounter less sampling error. This makes intuitive sense: the composition of large sample should more closely approximate that of the true population than a small sample. 

How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample `r (sampsizebig <- 500)` individuals: 
```{r samp-dist-big, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 500)'}
x <- rbinom(n = 100000, size = sampsizebig, prob = prop.purp)
out <- data.frame(n.purple = x)
ggplot(out, aes(x = n.purple)) + 
  geom_histogram(binwidth = 2) + xlim(0, sampsizebig) +
  xlab("No. of purple morph individuals") + ylab("Count")
```

Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples that we just considered, but even with 500 individuals in a sample, we should still expect quite a lot of uncertainty in our estimate. The take home message is that you need a lot of data to reduce sampling error.

## The standard error

Up until this point we've been fairly relaxed about how we quantified the spread of a sampling distribution. We just estimated the approximate range of purple morph counts "by eye". This is fine for investigating general patterns, but to make rigorous comparisons, we really need a quantitative measure of this variability. This is called the **standard error**. 

The standard error is actually quite a simple idea, though its definition often causes confusion. Here is that definition: a standard error is the standard deviation of the sampling distribution of an estimate, like a mean or a frequency. Don't worry if that makes absolutely no sense. The key point is that it is a standard deviation, so it a measure of the spread, or dispersion of a distribution. The distribution in this instance is a sampling distribution of some kind of estimate. 

(It is common to use a shorthand abbreviations such "SE", "S.E.", "se" or "s.e." in place of 'standard error' when referring to the standard error in text.)

We can use a simulation in R to calculate the expected standard error of an estimate of purple morph frequency. In order to do this we have to specify the value of the population frequency, and we have to decide what sample size we want to evaluate. 

Let's assume we want to know the expected standard error when the purple morph frequency is 40% and the sample size is 80. First we set up the simulation by assigning values to different variables to control what the simulation does:
```{r}
purple_prob <- 0.4
sample_size <- 80
n_samples <- 100000
```
The value of `purple_prob` is the probability a plant will be purple (0.4 --- R doesn't use percentages), the value of `sample_size` is the sample size we'll use for each sample, and the value of `n_samples` is the number of independent samples we'll take. That's simple enough. 
```{r}
raw_samples <- rbinom(n = n_samples, size = sample_size, prob = purple_prob)
percent_samples <- 100 * raw_samples / sample_size
```
You don't have to understand how this works, but if you did A-level statistics you might be able to guess what the `rbinom` function is doing. Honestly though, the R code isn't important here. We're just showing it to you to demonstrate that seemingly complex simulations are often easy to do in R. It is more or less the same code we used to generate those plots above (the only difference is that this time we converted the numbers into proportions).  

The result is what matters. We simulated the percentage of purple morph individuals found in 100000 samples of 20 individuals, assuming the purple morph frequency is always 40%. The results are stored the result in a vector called `percent_samples`. Here are the first 50 values of that vector:
```{r}
head(percent_samples, 50)
```
These numbers are all part of the sampling distribution of morph frequency estimates. So... how to calculate the standard error? This is just the standard deviation of these numbers, so we just use the `sd` function:
```{r}
sd(percent_samples)
```
What is this useful? The standard error gives us a standardised way to compare the variability we expect to see in different sampling distributions. As long as the sampling distribution is 'well-behaved', then roughly speaking, most estimates (~95%) can be expected to lie in a range of about four standard errors. If you're not convinced, look at the second bar plot we produced above (where the sample size = 80, and the purple morph frequency = 40%). What is the approximate range of simulated values? How close is this to $4 \times 5.5$? Pretty close we think...

So in summary, the standard error gives us a way to quantify how much variability we expect to see in a sampling distributions. We said in the last chapter ([Learning from data]) that a point estimate is useless with some kind of associated measure of uncertainty. A standard error is one such measure.

## What is the point of all this!?

By this point you might (quite reasonably!) be wondering why we have spent so much time looking at properties of repeated samples from a population. After all, when we collect 
data in the real world we're only going to have a single sample to work with. We can;t just keep collecting more and more data. We also won't know anything about the population parameter of interest. This lack of knowledge is the reason for collecting the data in the first place! 

The short answer to this question is that we want to learn about frequentist inference in this book. Before we can actually apply this technique we need to have a sense of...

* how point estimates behave under repeated sampling (i.e. sampling distributions),

* and how 'sampling error' and 'standard error' relate to sampling distributions.

<!--chapter:end:050_standard_error.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Statistical significance and *p*-values

We ended the last chapter by reminding you that we use frequentist statistics in this course. To be a bit more precise, we're learning about frequentist inference. 'Inference' is just statistical jargon for a system of learning about a population using data. While a precise definition of frequentist inference is well beyond the scope of this course, we can give a rough description. Frequentist inference works by asking *what would have happened* if we were to repeat an experiment or data collection exercise many times, assuming that the relevant population parameter(s) never change. 

The choice of population parameters to work with depend on what kind of question we are asking. This varies from one application to another. The thing that is common to every frequentist technique is that we ultimately have to work out what a sampling distribution of some kind look likes. If we can do that, then we can ask, for a given scenario, how likely or unlikely a particular result is. This naturally leads onto two of the most important ideas in this course: statistical significance and *p*-values. Our goal today is to try to make sense of these ideas.

## Estimating a sampling distribution {#bootstrap}

Let's carry on with the plant polymorphism example. Our ultimate goal is to evaluate whether the purple morph frequency was greater than 25% in our new study population. The suggestion in the preamble of this chapter is that to get to this point we need to work out what the sampling distribution of the purple morph frequency estimate looks like. At first glance this seems like an impossible task. We can't use simulations as before, because we don't know the true frequency of purple morphs in the population. All we have is the one sample. The solution to this problem is surprisingly simple (or at least the basic idea is simple). Since we don't know much about the population, we use the sample to approximate some aspects of it, and then work out what the sampling distribution of our estimate should look like using this approximation.

Let's unpack this idea, and then try it out for real.

### Overviw of bootstrapping {#bootstrap-overview}

There are lot's of different ways to approximate a population from a sample. One of the simplest methods---for easy problems at least---is to *pretend the sample is the true population*. Then, all we have to do to get at a sampling distribution is draw new samples from this pretend population. That may sound a lot like cheating, but it turns out that this is a perfectly valid way to construct useful sampling distributions for many kinds of problems. 

Here is a how it works for our example, using a physical analogy. Imagine that we had written down the colour of each sampled individual on a different piece of paper, and placed all of these in a hat. We then do the following:

1. Pick a piece of paper at random, record its value (purple or green), put the paper back into the hat, and shake the hat about to mix up the bits of paper.

2. Pick another piece of paper (you might get the same one), record its value, and then put that back into the hat, remembering to shake everything up.

(The shaking here is meant to ensure that each piece of paper has an equal chance of being picked, i.e. we're taking a random sample. This might not work in reality, but let's assume it does.)

3. Repeat this process until you have a recorded new sample of colours which is the same size as your real sample.

(This process is called 'sampling with replacement'. Each artificial sample is called a 'bootstrapped sample'.)

4. For each bootstrapped sample, calculate whatever quantity is of interest (e.g. the proportion of purple plants sampled).

5. Repeat steps 1-4 until we have generated a large number of bootstrapped samples. About 10000 is sufficient for many problems.

Although it may seem like cheating (it's not!), this process really does produce an approximation of the sampling distribution of the quantity we're interest in. It is called **bootstrapping** (or 'the bootstrap'). The bootstrap was invented by a very smart statistician called [Bradley_Efron](https://en.wikipedia.org/wiki/Bradley_Efron). We're introducing it here because it allows you to see how frequentist methodology works without having to do any nasty mathematics. We're not expecting you to learn it, so don't panic if you find it tricky to understand. 

### Doing it for real

```{r, echo = FALSE, eval = FALSE}
set.seed(27081975)
samp_size <- 250
plant_morphs <- sample(c("Purple","Green"), 
                       samp_size, replace = TRUE, prob = c(4,6))
mns <- c(Purple = 760, Green = 700)[plant_morphs]
sds <- c(Purple = 160, Green = 150)[plant_morphs]
morph.data <- data.frame(Colour = plant_morphs, 
                         Weight = rnorm(samp_size, mns, sds))
write.csv(morph.weights, row.names = FALSE, 
          file = "./data_csv/MORPH_DATA.CSV")
```

```{r, echo = FALSE}
morph.colours <- read.csv(file = "./data_csv/MORPH_DATA.CSV")
```

Let's assume we've sampled 250 individuals from our new plant population. We're going to use this hypothetical sample to implement the bootstrap in R.

```{block, type='do-something'}
The best way to understand what follows is to actually work through the example...
```

A data set representing this situation is stored in a Comma Seperated Value (CSV) text file called 'MORPH_DATA.CSV'. Download the file from MOLE and place it in your working directory. Don't forget to set this first! Next, run through the following steps:

*   Read the data into an R data frame using `read.csv`, assigning the data frame the name `morph.data`.

*   Use functions like `glimpse` (from `dplyr`) and `str` (from base R) to inspect the structure of `morph.data`. How many variables are in the dataset? What are their names? What kind of variables are they?

* Use the `View` function to inspect the data. Is this what you expected? Are the values of the different variables as you would expect them to be?

The point of all this is to 'sanity check' the data, i.e. to make sure we understand the data and that there are no obvious problems with it. **Always check your data after you have read it in**. There is really no point messing about with the likes of `dplyr` and `ggplot2`, or carrying out a statistical analysis, until we have done this. If we don't understand how our data is organised, there is very real risk that we will make a lot of avoidable mistakes.

What you should have found is that `morph.data` contains 250 rows and two columns/variables: `Colour` and `Weight`. `Colour` is a categorical variable (a 'factor', in R-speak) and `Weight` is a numeric variable. The `Colour` obviously contains the colour of each plant in the sample. What about `Weight`? We don't need this yet, but we'll use it in the next chapter.

Now that we understand the data we're ready to implement bootstrapping (using R of course, no hats or paper required). We're going to introduce a few new R tricks here. We'll explain them as we go, but these aren't things you need to remember.

We want to construct a sampling distribution for the frequency of purple morphs, so the variable that matters here is `Colour`. Rather than work with this inside the data frame, we're going to pull it out using the `$` operator, assign it a name (`plant_morphs`), and then take a look at the first 20 values:
```{r}
plant_morphs <- morph.data$Colour
head(plant_morphs, 20)
```
Hopefully you followed that---`plant_morphs` is just a simple vector (a factor) containing the colour information. For convenience, we can also calculate and store the original sample size (`samp_size`), and store the number of bootstrap samples we plan to use (`n_samp`):
```{r}
samp_size <- length(plant_morphs)
n_samp <- 10000
```
OK, we are now ready to start bootstrapping....
```{r}
samp <- sample(plant_morphs, replace = TRUE)
head(samp, 20)
```
We only want one number from this sample. How do we calculate this...
```{r}
sum(samp == "Purple") / samp_size
```

```{r}
boot_out <- replicate(n_samp, {
  samp <- sample(plant_morphs, replace = TRUE)
  sum(samp == "Purple") / samp_size
})
```

...then use functions called `sample` and `replicate` to generate a bootstrapped sampling distribution for the mean (we generated 10000 samples). You don't have to understand this R code, but ask a demonstrator if you want to know more about it. 

Let's take a quick look at the first 10 values:
```{r}
#round(head(boot.samp, 10))
```
These numbers represents different values of the sample mean that we would expect to generate if we repeated the data collection exercise. We can use this bootstrapped sampling distribution in a number of ways. As always, it is a good idea to plot it first get a sense of what it looks like. A histogram is a good choice here, because we have a large number of samples:
```{r}
#plot.df <- data.frame(boot.samp) # 'ggplot' expects a data frame 
#ggplot(plot.df, aes(x = boot.samp)) + geom_histogram(binwidth = 5) 
```

The mean of the sampling distribution looks to be round about 655 grams--very close to the sample mean. We can of course calculate this in R: 
```{r}
#round(mean(boot.samp))
```
This is the same as the sample statistic (the sample mean, in this case). This will always be the case if we construct a large enough sample, as the bootstrapping procedure assumes that the 'true' population mean is equal to the sample mean.

A more useful quantity is the bootstrapped standard error (SE). Since this is the standard deviation of the sampling distribution, we just apply the `sd` function to the bootstrap sampling distribution to calculate it:
```{r}
#round(sd(boot.samp), 1)
```
This quantity is a standarised measure of uncertainty that we require. A large SE implies that our sample size was too small to reliably estimate the population mean. Whenever we report a point estimate of a mean, we should also report the standard error. For example,

> The mean dry weight biomass of purple morph plants (n = 25) was 658 grams (s.e. ± 22.3). 

Notice that we also report the sample size.

The bootstrap is a very powerful tool in the right hands. The bootstrap is actually quite an advanced technique that can be difficult to apply in many settings (e.g. analysis of complex experiments). It is for this reason that we will not apply it routinely in this course. We used it here to understand how 'frequentist' ideas can be used to quantify uncertainty in a statistic (i.e. a point estimate).

The key message to take away is that we can characterise uncertainty by working out what **would happen under repeated sampling** from a particular population. 

## Statistical significance

[[COMPLETE ME]]

## A few words about *p* values

It is important to understand the meaning of the probabilities generated by statistical tests. We have already said a *p*-value is the proportion of occasions on which you would expect to see a result at least as extreme as the one you actually observed if the null hypothesis (of no effect) was true. Conventionally (in biology at least) we accept a result as statistically significant if *p*<0.05 (also expressed as 5%). There is nothing special about this cut-off point. 

A probability of 0.05 is a chance of 1 in 20. This means that if there really was no effect of the factor we are investigating, we would expect to get a result significant at *p*=0.05 about 5 times in 100 samples. To envisage it more easily, it is slightly less than the chance of tossing a coin 4 times and getting 4 heads in a row (*p*=0.0625).

This puts a ‘significant’ result into context. Would you launch a new drug on the market or bring a prosecution for pollution on the evidence of the strength of four heads coming up in a row when a coin is tossed? Well of course such things are unlikely to hinge on a single test, but it is always worth bearing in mind what ‘significance’ actually means.

Of course the smaller the probability the more confident one can be that the effect we see is real. A probability of *p*=0.01 (1 in 100) is pretty good evidence, and *p*=0.001 (1 in 1000), or less, is better still. For this reason, in some critical applications such as drug testing the value set for accepting a result as significant may be lower (e.g. *p*=0.01). The costs of using a more stringent threshold is that this increases the possibility of false negatives (called a 'type II' error)--i.e. we are more likely to fail to detect an effect whne it is really present.

### What if is close to 0.05?

The thing to remember here is that although we tend to use *p*=0.05 as a cut-off, is really a continuous measure and *p*=0.055 is not very different from *p*=0.045. The exact value of will be affected by how well the data fulfil the assumptions of the test–-which will only be approximately with most biological data, so you shouldn’t set too much store by the difference between *p*=0.045 and *p*=0.055. It would be irrational on the one hand to reject an idea completely just on the basis of a result of *p*=0.055, while at the same time being prepared to invest large amounts of time and money implementing policies based on a result of *p*=0.045.

### Biological vs. statistical significance

A final, but vital, point: do not confuse statistical significance with biological significance. A result may be statistically highly significant (say *p*<0.001) but biologically trivial. To give a real example, in a study of the factors determining the distribution of freshwater invertebrates in a river, the pH of water was measured in the open water and in the middle of the beds of submerged vegetation. There was a statistically significant difference in pH (*p*<0.01) but the mean pH values were 7.1 in the open water and 6.9 in the weeds. This is a very small effect, and almost certainly of no importance at all to the invertebrates.

The significance of a result depends on a combination of three things (1) the size of the effect, (2) the variability of the data, (3) the number of samples. Even a tiny effect can be significant if the data have very little variation and the sample size is large. You should not automatically equate a significant result with a large effect--you need to inspect the means and consider the biological implications of the difference. The statistical results can give you some guidance in separating genuine differences from random variation, but they can’t tell you whether the difference is biologically interesting or important–-that’s your job!


<!--chapter:end:055_significance_p-values.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Comparisons, null hypotheses, and tests

Scientific enquiry involves asking questions. One of the commonest kinds of question that we ask as biologists is: ‘Is there a difference between the measurements from two samples?’ For example:

‘Do male and female locusts differ in length?’

‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’

‘Do eagle owls feed on rats of different sizes during winter and summer?’

‘Do streams running through conifer forests differ in pH from those running through deciduous forests?’

'Do purple and green plant morphs differ in their biomass?'

In order to answer these kinds of questions we need to step through the same kind of process discussed in the last few chapters. We have to: 1. decide which variables we need to measure, 2. decide which population parameters are relevant, 3. design a suitable experiment or gather representative samples, and 4. use the samples to learn about the populations.

Let's assume that we already know how to go about steps 1-3 and have already collected our samples. Our focus in this chapter is on step 4: using these samples to learn about the populations. Specifically, the problem we want to understand is how to evaluate whether or not two populations are different in some way.

## A new example {#morph-weights-eg}

As always, we'll investigate the problem by working through a simple example. We will stay with the purple morph / green morph example, but this time we'll examine a different variable: the dry weight biomass of our imaginary plants. Perhaps we suspect that the two morphs have different growth habits. One way to address this hypothesis would be to measure the biomass of individuals of each morph. Ultimately, we want to compare the mean biomass of each morph.

In order to study the biomass of individuals in our hypothetical population, we would collect a sample of dry weights. Dry weight is a numeric variable, measured on a ratio scale. The population parameters of interest now are the population mean dry weights of each morph. 

Notice that our definition of 'the population' is slightly different than before. Now we are imagining that each morph is a seperate population. Let's assume that we have sampled the dry weight (in grams) of 25 representative individuals of each morph.

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

```{block, type='do-something'}
You should work through this exampe from now on...
```

[[TRANSITION]]

The next step is to calculate some simple descriptive statistics for each morph. We need to know the sample means as these are our 'best guesses' of the population means. It may be useful to know something about the variability of the samples, which can be summarised by the sample standard deviation is a good measure of the latter. Here is a reminder of how to do this using `dplyr`:
```{r}
temporary <- group_by(morph.weights, pmorph)
summarise(temporary, mean = mean(weight), stan.dev = sd(weight))
```
This shows that the mean dry weights of purple and green morphs are 658 grams and 787 grams, respectively. The standard deviation estimates from the two samples suggest that dry weights of green morphs are a little more variable than the purple morphs. 

By the way, if you like using the `%>%` operator to chain together `dplyr` functions (the 'verbs'), then here is how to use this to achive exactly the same result as above:
```{r}
morph.weights %>% 
  group_by(pmorph) %>% 
  summarise(mean = mean(weight), stan.dev = sd(weight))
```

Using means and standard deviations to summarise samples can be tricky to understand until you are used to them. A plot of some kind is much more useful. At the moment, we are just trying to understand the distribution of our two samples. One way to do this is to construct a histogram or a dotplot of each sample distribution. We don't have much data, so a dotplot is probably the best option. Here is one way to make a dot plot for just the purple morph data:
```{r purple-dist}
purp.weights <- filter(morph.weights, pmorph == "purple") 
ggplot(purp.weights, aes(x = weight)) + geom_dotplot(binwidth = 30)
```

First, we filtered the complete dataset so that only the observations corresponding to the purple morph were retained, then we used `ggplot` with the dotplot geom to construct the figure we wanted. Once again (just as a reminder), here is how to achieve the same thing using the `%>%` operator:
```{r, fig.keep = FALSE}
morph.weights %>% 
  filter(pmorph == "purple") %>% 
  ggplot(aes(x = weight)) + geom_dotplot(binwidth = 30)
```

You should always explore your data visually before carrying out any kind of statistical analysis of it. You might learn something new about it, and at the very least, this allows you to assess whether there are any problems with it. A quick inspection of this figure suggests that there is nothing odd about the dry weight data. We only have 25 observations, so we can't say too much about its shape, but there don't seem to be any outliers. 



Last week, we looked at how to quantify uncertainty in a single estimate of the population mean biomass of a morph. This week, we want to compare the biomass samples of the two morphs to address the question: 'Do purple and green plant morphs differ in their dry weight biomass?'. However, as it is currently framed, this question is a little too vague. The first thing we have to do is define what we mean by 'different'.

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

As always, it is a very good idea to plot the data. We could do this in a variety of ways, but since we only have two groups (purple and green morphs), we may as well summarise the full sample distribution of each morph. The data are in the MORPH_WEIGHTS.CSV file, which we have read into R using the `read.csv` function. We stored the data in a data frame called `morph.weights`. Here is some `ggplot2` code to make the required plot:  
```{r two-morph-dist}
ggplot(morph.weights, aes(x = weight)) + 
  geom_dotplot(binwidth = 30) + 
  facet_wrap(~pmorph, ncol = 1)
```

(Hopefully this plot will also remind you how to use the `facet_wrap` function to make a multipanel plot, based on the values of particular variable--`pmorph` in this case).

What does this plot suggest? We are interested in the degree of similarity (or not!) of the two sample distributions. There is a lot of overlap in the dry weights of each morph, but in general we can say that: (1) green morph individuals tend to have higher dry weights than purple morphs, and (2) the green morphs seem to be more variable than purple morphs.

We can get a better handle of these patterns by calculating the sample means and standard deviations of the two samples to evaluate their central tendency and spread, respectively. We know how to use `dplyr` functions `group_by` and `summarise` to do this:
```{r}
descrip.stats <- 
  morph.weights %>% 
  group_by(pmorph) %>%
  summarise(wght.mean = mean(weight), wght.sd = sd(weight))
descrip.stats
```

These descriptive statistics back up our visual impressions of the data: green morph individuals are larger than purple morphs (sample means: 787 vs. 658 grams), and green morphs are more variable than purple morphs (sample SDs: 118 vs. 114 grams). Remember, these numbers are just point estimates derived from limited samples. If we sampled the populations again, sampling variation would ensure that we end up with different estimates. This means we are not yet in a position to conclude that green morphs are bigger than purple morphs.

Let's return to our question: 'Do purple and green morphs differ in their dry weight biomass?'. We said this question was a little too vague, and that we needed to define what we meant by 'different'. One way in which the sample distributions seem to be different is with respect to their spread, and so one possible refinement of this question would be to ask whether this difference is 'real', or simply arises from sampling variation. There are circumstances where it is scientifically interesting to compare variability. However, it is much more common to focus on differences in the central tendency of samples.

By looking at the central tendency of different samples, we can evaluate whether or not something we have measured increases or decreases, on average, among different populations. Many scientifically relevant questions are addressed by making this assessment. When someone uses a statistical test or model to 'compare samples', what they are usually doing is evaluating whether or not the central tendency of the populations are different. We typically make this assessment by evaluating the strength of evidence for the presence of **a difference in the population means** of the focal populations. 

The question we want to address is therefore: 'What is the strength of evidence for a difference in the population mean biomass of purple and green plant morphs?' In practise, this boils down to a another question: 'Is there a statistically significant difference between their means.'

We will now turn to the idea of **statistical significance**. We will also see how to evaluate statistical significance by calculating a ***p*-value** under a suitable **null hypothesis**.

```{block, type='do-something'}
**Don't panic...**

The ideas in these next couple of sections are really quite abstract and can be difficult to understand. We are not expecting you to understand them straight away, and you certainly won't be asked to explain them in an assessment. 
```

### How do we evaluate statistical significance?

```{r, echo = FALSE}
set.seed(27081975)
nperm <- 2000
perm.out <- numeric(nperm)
perm.eg <- list()
data.i <- morph.weights
ids <- morph.weights$pmorph
for (i in 1:nperm) {
  morph.labels <- sample(ids, replace = FALSE)
  perm.out[i] <- 
    mutate(data.i, pmorph = morph.labels) %>% 
    group_by(pmorph) %>% summarise(mean = mean(weight)) %$% diff(mean)
  if (i <= 3) {
    perm.eg[[i]] <- morph.weights$weight
    names(perm.eg[[i]]) <- morph.labels
  }
}
names(perm.eg) <- paste("Sample", 1:3)
```

In order to assess the strength of evidence for the presence of a difference between the population means of two groups, we have to do something that, at first glance, looks very strange. We can break this down into four steps:

1. We assume that there is actually no difference between the population means. That is, we hypothesise that all the data are sampled from a pair of populations that are characterised by a single, shared population mean.

2. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation of 'no difference between samples'. This usually involves calculating a sampling distribution of some kind of test statistic.

3. We then ask, 'if there is no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we actually observed in the true sample?'

4. If the observed difference is sufficiently improbable, then we conclude that we have found a 'statistically significant' result. A statistically significant result is therefore one that is inconsistent with the hypothesis of no difference.

(We will discuss how to define 'sufficiently improbable' once we've worked through an example.)

There are many different ways to go about realising this process. We'll look at one of these today. Regardless of the details, they all work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (e.g. a difference between means) *is absent*. That sounds odd, but if you can understand this fundamental idea you are well on your way to understanding how frequentist statistics works. 

Let's return to our example to see how this might work in practise.

### A permutation test

In our example, a hypothesis of 'no difference' between the mean dry weights of purple and green morphs has the following implication. It means both morphs are really sampled from the same population, and as such, the labels 'purple' and 'green' are meaningless. These labels may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows:

1. Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels 'purple' and 'green' to this new copy of the data. Do this in such a way that the original sample sizes are preserved.

(We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used. The process of assigning random lablels is called permutation)

2. Repeat this permutation scheme until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient.

3. For each permuted sample, calculate whatever sample statistic is of interest. In this case, we want the *difference* between the mean dry weight of purple and green morphs in each sample.

4. Compare the observed sample statistic (i.e. the difference between the mean dry weights) to the distribution of sample statistic from the randomly permutated samples.

This scheme is called a permutation test, because it involves random permutation of the group labels. Why is it useful? *Each unique random permutation yields an observation from the sampling distribution of the difference among sample means, under the assumption that this difference is really zero in the population.* This means we can assess whether an observed difference is consistent with the hypothesis of no difference by looking at where it lies relative to this sampling distribution. 

We have implemented a permutation test in R using the purple/green morph dataset for you, using `r nperm` permutations. We won't show you the R code because it uses a few tricks you haven't been taught, but we can look at a couple of permuted samples to get a sense of how this works: 
```{r, echo=FALSE}
perm.eg[1:2]
```
The data from each permutation are stored as numeric vectors, in  which each element of the vector is named (these are the labels). Notice that the set of numbers does not change among the permuted samples. The only difference between them is the labelling of the numbers. The difference between the mean dry weights in the first permutation is `r perm.out[1]`. This difference is `r perm.out[2]` in the second sample.

What we really care about here is distribution of these differences. This is an approximation to the the sampling distribution of the difference between means. Here is a histogram that summarises the `r nperm` mean differences from the permuted samples:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept = diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

Notice that this distribution is centred at zero. This makes sense--if we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. The red line shows the location of the observed difference between purple and green morph mean dry weights. The relevant feature here is the location of this observed difference within the sampling distribution.

```{r, echo=FALSE}
nlower <- sum(perm.out <=  diff(descrip.stats$wght.mean))
nhgher <- sum(perm.out >= -diff(descrip.stats$wght.mean))
```

What does this figure tell us? It looks like the observed difference is very unlikely to have arisen through sampling variation, under the assumption that the population means of the two groups are identical. We can say this because the observed difference lies at the end of one 'tail' of the sampling distribution. We need to be able to make a more precise statement than this though.

Only `r nlower` out of the `r nperm` permutations ended up being equal to, or 'more extreme' (i.e., more negative), than the observed difference. The probability of finding a difference in the means equal to or more negative than the observed difference (denoted *p*) is therefore, *p*=`r nlower/nperm` (`r 100*nlower/nperm`%). This probability has a special name. It is called the ***p*-value**. A *p*-value is defined as the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true.

We have to be careful at this point. The test we just did is called a 'one-tailed' test, because we only looked at one end (the tail) of the sampling distribution. However, we did not set out to test whether purple plants were smaller on average than green plants. We set out to assess whether they are different, but we never made a statement about the direction of the effect. This means we also have to consider the possibility of an effect in the opposite direction to what was observed. 

To do this, we have to count up the cases that fall into the upper and lower tails of the distribution, where each tail is defined by the region that lies beyond the absolute value of the observed difference, on the positive and negative halves of the x axis:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept =  diff(descrip.stats$wght.mean), colour = "red") + 
  geom_vline(xintercept = -diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

When we do this, we find that `r nlower+nhgher` out of the `r nperm` permutations lie beyond the observed difference, and so the new *p*-value is *p*=`r (nlower+nhgher)/nperm` (`r 100*(nlower+nhgher)/nperm`%). This kind of test--where we look at both tails of the sampling distribution--is called a 'two-tailed' test. We discuss the reasoning for and against using a one- or two-tailed test in this week's self-directed practical.

#### The significance of *p*-values

What are we supposed to do with the finding *p*=`r (nlower+nhgher)/nperm`? This is the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true. The hypothesis under consideration is one of no effect, and so a low *p*-value can be interpreted as evidence for an effect being present. In our example, the low *p*-value is evidence for a difference among the population mean dry weights of purple and green morphs.

One question remains: How small does a *p*-value have to be before we are happy to conclude that the effect is probably present? In practise, we do this by applying a threshold, called a **significance level**. If the *p*-value is less than the chosen significance level we say the result is said to be **statistically significant**. Most often (in biology at least), we use a significance level of *p*<0.05 (5%). Why? The short answer is that this is just a convention. We'll come back to this below.

#### The null hypothesis

Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to apply in a more complex setting. We are not expecting you be able to implement a permutation test yourself. We used it to demonstrate how frequentist statistics works. The details vary from one problem to the next, but ultimately, if we are using frequentist ideas we always have to find a way to do the following: 1) assume that there is actually no 'effect', where an effect is expressed in terms of one or more population parameters, 2) work out what would happen if we were to repeatedly take samples from a population in this hypothetical situation, 3) evaluate how likely our observation would be under the hypothesis of no effect.

When using frequentist statistics we are always asking what would happen if we continually sample from a population *in the absence of the effect we are interested in*. This idea of a hypthetical 'no effect' situation is so important that it has a special name; it is called **the null hypothesis**. Every kind of statistical test (in this course at least) has a very specific null hypothesis associated with it. You can only fully understand the resuts of a statistical test if you understand the null hypothesis it relies on. We will remind you about the null hypothesis and introduce the related concept of the alternative hypothesis at the end of this chapter.

## Hypotheses and null hypotheses--why such a negative approach?

Previously we have said that an hypothesis is a statement of a proposed process or mechanism which might be responsible for an observed pattern or effect. We have also seen that in statistics, you will encounter 'hypothesis' used in a different, and quite specific way. In particular you will frequently see the term: *null hypothesis* (often written in statistics books as H~0~).

The null hypothesis is simply statement of what we would expect to see if there is actually no effect of the factor we are looking at (e.g., plant morphology) on the variable that we measure (e.g., dry weight biomass). So in the above example our null hypothesis was *There is no difference in mean biomass of purple and green plants*. When comparing the sizes of rats eaten by eagle owls, the appropriate null hypothesis was *There is no difference between the mean size of rats eaten by eagle owls in summer and winter*.

All statistical tests you are likely to encounter in biology work by specifying a null hypothesis and then testing the observed data to see if they deviate from the null hypothesis. This may seem like a rather odd approach, but there are good theoretical and practical reasons for doing things this way.

Although you need to be aware of what a null hypothesis is, and what it is used for, in general discussion of tests we will normally refer to the effect which is the opposite of the null hypothesis - i.e. it is the effect you are actually interested in - known as the *test hypothesis*, or the *alternative hypothesis* (often denoted H~1~ in statistics books).

The alternative hypothesis is essentially a statement of the effect you are interested in evaluating, e.g., purple and green plants differ in their mean size. It is a statement of whatever is implied if the null hypothesis is not true.

Having got all the types of hypothesis sorted out, we can then use a particular statistical technique (such as a permutation test) to evaluate the observed result against that expected if the null hypothesis was true. The test gives us a probability (*p*-value) telling us how likely it is that we would have got the result we observe if the null hypothesis was, indeed, true.

If the value is sufficiently small (conventionally if *p*<0.05) we judge it unlikely that we would have seen this result if the null hypothesis was true and consequently we *reject the null hypothesis* (i.e. reject the notion that there is no difference) and instead *accept the alternative hypothesis* (that there is a difference). Note that this is not the same as 'proving' the alternative hypothesis is true. You can't prove anything by collecting data or carrying out an experiment.

If the probability is large, then it is quite likely that we could have got the observed result if the null hypothesis was true, and in this case we cannot reject the null hypothesis. Note that in this situation we *'do not reject the null hypothesis'*, but this is not quite the same as accepting that the null hypothesis is true, paradoxical though this may seem. One obvious reason for this is that if we only have a small sample then there may be an effect of the factor we are looking at, but we simply can’t detect it because we don’t have enough data.




<!--chapter:end:060_comparisons_p-values.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Introduction to Parametric Statistics {-} 

# What are "parametric statistics"?

Most of the statistical tools that we will teach you in this course are different examples **parameteric statistics** techniques. The word 'parametric' refers to the fact that most of the statistical models and tests that we will learn about are based on some kind of mathematical model of the population. The precise details of the models are defined by their parameters. We aren't going to study these models in any great detail, because this is supposed to be a practical course.

We will talk about the assumptions of the underlying models though. These are important and we can't afford to ignore them.

## Mathematical models {#math-models}

[[FINISH ME]]

## The normal distribution {#parametric-stats}

In this course, this assumption is essentially always the same: we assume that the variable follows a **normal distribution**. If you studied A-level statistics you should know all about this. If not, you may have come across it without realising: the normal distribution is sometimes called the 'Gaussian distribution' or more colloquially, 'the bell-shaped curve'.

Unfortunately, we don't have enough time in this course to really study the normal distribution in much detail. There are however, many good online resources to help you learn you about it if you want to know more. Here is one relatively non-technical introduction:

http://onlinestatbook.com/2/normal_distribution/normal_distribution.html


<!--chapter:end:070_parametric_statistics.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Standard errors and confidence intervals

## The standard error of the mean {#se-parametric}

Let's return to our example. In order to construct our bootstrap sampling distribution we pretended that the sample was the true population. A 'parametric' version of this procedure starts by making an assumption about the mathematical form of the population distribution of focal variable. We then use estimates of the parameters which describe the population to understand what would happen if we were to repeatedly resample it. Those estimates are derived from the sample.

[[NORMALITY AGAIN]]

So what can we say about the standard error of the sample mean, when it is calculated for a normally distributed variable? It turns out, that when we resample from a normally distributed variable, there is a very simple formula for the standard error of the mean. Here it is:
$$
SE = \frac{\text{Standard deviation of the sample}}{\sqrt{\text{Sample size}}} = \frac{SD}{\sqrt{n}}
$$
Notice that this only depends on the properties of the original sample, that is, the sample standard deviation and the sample size. This means that as long as we are happy with the normality assumption, we can go ahead and calculate a standard error for a point estimate of the mean without resorting to fancy tools like the bootstrap. Here is how to do this using R: 
```{r}
# n <- length(purp.weights) # get the sample size
# round(sd(purp.weights)/sqrt(n), 1)
```
That's all we need to do. This returns a value of 22.8, which is reassuringly close to the bootstrapped value of 22.3. They aren't identical, because each estimate of the SE relies on a different procedure, but they are very close.

<!--chapter:end:080_SE_and_CI.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# One sample *t*-tests

[[PREAMBLE]]

## The t-distribution

A statistician called W.G. Gosset showed that when we take samples from a normally distributed variable and calculate their means, the sampling distribution of these means has a particular form---it follows a Student's t-distribution. The same is true if you take two samples from a normal distribution, calculate their means, and then subtract these from one another. The sampling distribution of the differences among means also follows a Student's t-distribution.

(Why is it called Student's t? W.G. Gosset was a statistician employed by the Guinness Brewery, who published his statistical work under the pseudonym of 'Student'. He didn't use his real name because Guinness would have claimed ownership of his work.)

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's t-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a t-distribution.

We are not going to delve any deeper into the t-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the standard error of the difference. Different estimates of the standard error are available.

(This generates a test statistic)

4. Compare the test statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the observed difference.

Thsi procedure is called a two sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### So what exactly is a one-sample t-test?

The one-sample t-test allows us to compare the mean from a sample with an expected value. More precisely, it allows us to use the sample evaluate whether or not the unknown population mean is likely to be different from an expected value. This value might be something predicted from theory or some other prespecified value you are interested in. Here are some examples:

-   You have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If you have data on the actual time spent by 15 animals observed foraging in the patch, then you could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test.

-   You are monitoring sea pollution and have a series of water samples from along a beach. You wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach as a whole can be regarded as greater, or less than the legislated limit. You have a sample, which has variation, and a single value; a one-sample t-test will enable you to test whether the mean value for the beach as a whole, exceeds the limit (of course the question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue).

-   You are in charge of packaging seed samples from a horticultural firm, and your sales literature says that the packs will contain an average of 40 seeds. You select 20 packets off the production line at random and count the seeds in each. You could use a one-sample t-test to test whether this claim was true (or perhaps more importantly your competitors could!)

You can see that a paired-sample t-test, testing a set of observed differences against an expected value of zero (what we’d expect if there was no systematic difference across all the pairs), is simply another thing you can do with a one-sample test. It happens to be an extremely useful thing to be able to do, because it can allow you to design some very powerful experiments, and in practice you will probably end up using using one-sample tests for testing differences among pairs more often than any other use.

### Assumptions of the one-sample *t*-test

There are a number of assumptions that need to be met in order to use a one-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by the *t*-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It generally doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales. 

The one-sample *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the distribution of the sample. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

#### Assumptions: The example

[[WRITE THIS]]

### Carrying out a one-sample *t*-test in R

```{block, type='advanced-box'}
**A bit more about degrees of freedom**

[[UPDATE THIS]]
```

### Summarising the result of a one-sample *t*-test

[[REWRITE THIS SECTION]]

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistic should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = [[FIX ME]]). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.

```{block, type='warning-box'}
**p = 0.0000? It’s impossible!** 

Some computer packages (e.g. Minitab) will sometimes give a probability of p=0.000. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, you should never say this. If you actually know something is impossible for good biological, or other, reasons then you do not need to test it using statistics. When a computer package such as Minitab says p=0.000 it just means that the probability was 'very small'.
```

<!--chapter:end:090_t-tests_one_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Two-sample *t*-test

[[PREAMBLE]]

## Using a *t*-test to compare two means

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
tmod.equlv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = TRUE )
tmod.diffv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = FALSE)
```

Think back to the permutation test we carried out in chapter [[LINK ME]]. In order to construct the test, we "pretended" that the two samples were both drawn from one population. We used the combined purple morph + green morph sample to mimic the process of drawing new samples from this hypothetical population, each time assigning arbitary labels to generate artificial observations. 

The two-sample *t*-test can be viewed as a "parametric" version of this procedure. It starts by making an assumption about the mathematical form of the population distribution. The key assumption is that the two samples are **normally distributed** in the population.

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's *t*-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a *t*-distribution.

We are not going to delve any deeper into the *t*-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the *standard error of the difference*.

(This generates the *t*-test statistic)

4. Compare the test statistic to the theoretical predictions of the *t*-distribution to assess the statistical significance of the observed difference.

This procedure is called a two-sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### Assumptions of the *t*-test

There are a number of assumptions that need to be met in order to use a two-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by a *t*-test will not be reliable. Even mild non-independence can be a serious problem. This is why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It really doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales.

The *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. The 4^th^ assumption only matters if you plan to use the equal variance version of the two-sample *t*-test (the original Student's *t*-test). This version of the *t*-test is potentially a little more powerful than Welch's version, but not by much, and it is only correct if the population variances really are identical. Since we can never verify this, it is safer to just use the unequal variance version. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the sample distribution of each group. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

If you learned about the two-sample *t*-test at some point in the past you may have been told that variances of the samples need be the same. What about the **equal variance** assumption? This isn't really correct. The original version of Student's two-sample *t*-test was derived by assuming that the *population variance* of each group was identical, so it is the population variances, not the sample variances, that matter. This isn't the critical point though. What matters from a practical perspective is that R uses the "Welch" version of the *t*-test by default (Welch was another statistician, in case you're wondering). Welch's version of the two-sample *t*-test does not make the equal variance assumption, so as long as you stick with this version of the *t*-test, the equal variance assumption isn't one you need to worry about.

### Carrying out a two-sample *t*-test in R

You should work through the example in this section. If you haven't already done so, you will need to download the MORPH_WEIGHTS.CSV file from MOLE and place it in your working directory (this is the location you just set). Next, read the data in MORPH_WEIGHTS.CSV into an R data frame, giving it the name `morph.weights`:

```{r, eval = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

We looked at the sample distributions of the green and purple morphs in chapter [[LINK ME]]. It looks like they may have different variances, but as we have just seen, this isn't something we need to worry about. The sample size of each group is 25. This is fairly small (though not bad), so we should keep an eye on the normality assumption. The two dot plots we produced earlier suggest that there is nothing too 'non-normal' about their distributions, so it should be fine to go ahead and use a *t*-test.

It is very straightforward to carry out a two-sample *t*-test in R. We'll work with our plant morph example to demonstrate how to do it. The function we need to use is called `t.test` (no suprises there). Remember, we read the data into a data frame called `morph.weights`. This has two columns: `weight` contains the dry weight biomass of each plant, and `pmorph` is an index variable that indicates which group (plant morph) an observation belongs to. Here is the R code to carry out a two-sample *t*-test 
```{r, eval = FALSE}
t.test(weight ~ pmorph,  morph.weights)
```
We have surpressed the output for now as we want to focus on how to use `t.test` function. We have to assign two arguments (remember function arguments?--these control what a function does):

1. The first argument is a **formula**. We know this because it includes a 'tilde' symbol: `~`. The variable name on the left of the `~` should be the variable that contains the actual data (i.e. the numbers we want to compare). The variable on the right should be the indicator variable that says which group each observation belongs to. These are `weight` and `pmorph`, respectively.

2. The second argument is the name of the data frame that contains the two variables listed in the formula.

That's it. Let's take a look at the output:
```{r}
t.test(weight ~ pmorph,  morph.weights)
```

The first line reminds us what kind of *t*-test we have used. This says: `Welch two-sample *t*-test`, so we know that we have used the version of the two-sample *t*-test that accounts for unequal variance in the samples.

The next line just reminds us about the data. This says: `data: weight by pmorph`, which is R-speak for 'we compared the means of the `weight` variable, where the groups are defined by the values of the `pmorph` variable'.

The third line of text is the most important. This says: `t = 2.9381, df = 39.523, p-value = 0.005487`. The first part of this, `t = 2.9381`, is the test statistic (i.e. the value of the t statistic). The second part, `df = 39.523`, summarise the 'degrees of freedom'. This is essentially a measure of how much power our statistical test has (see the box below). The third part, `p-value = 0.005487`, is the all-important p-value. This says that there is a statistically significant difference in the mean dry weight biomass of the two morphs, because *p*<0.05.

The fourth line of text (`alternative hypothesis: true difference in means is not equal to 0`) just reminds us what the alternative to the null hypothesis is. We will discuss the meaning of this at the end of this chapter.

The next two lines show us the 95% confidence interval for the difference between the means. We don't really need this information, but you can think of this interval as a summary of the likely values of the true difference (a confidence interval is more complicated than that in reality).

The last few lines just summarise the sample means of each group. This is only useful if you did not bother to summarise these already (which you should always do!). 

<div class="advanced-box">
#### A bit more about degrees of freedom (again)
<div class="box-text">
In the original version of the *t*-test (which assumes equal variances) the degrees of freedom of the test are give by (n~a~-1) + (n~b~-1)  where n~a~ is the number of measurements from sample a and n~b~ the number of measurements from sample b. The plant morph data has measurements for 25 male and 25 females, so if we had used the original version of the test we would have (25-1) + (25-1) = 48 df. However, the R default version of the *t*-test reduces the numbers of degrees of freedom using a formula which takes into account the difference in variance in the two samples--the greater the difference in variances the smaller the number of degrees of freedom. This usually results in degrees of freedom that are not whole numbers.

In either case, a test with high degrees of freedom is more powerful than one with low degrees of freedom. 
</div>
</div>

### Summarising the result of a *t*-test

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistics should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = `r round(tmod.diffv$p.value, 6)`). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.

<!--chapter:end:100_t-tests_two_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Paired-sample *t*-tests

[[PREAMBLE]]

## The paired sample t-test {#paired-t}

### What is a paired-sample design?

In the last session you carried out t-tests to compare two means. The two samples you had in each case were entirely separate sets of measurements (purple and green plant morphs, rat skulls from pellets collected in winter and summer, fungal infection in beans on two different soil types). One measurement in one data set (e.g. the first green plant) has no link to any particular measurement in the second data set—-the measurements are said to be independent. The particular flavour of t-test you used in this situation is called a two-sample t-test (logically enough).

However, there are situations in which data may naturally form pairs, i.e., the first value in a sample may be linked in some way with the first value in the second sample. This is known, not surprisingly, as a *paired-sample* design.

The commonest example of a paired sample design is the situation where you have a set of organisms, and you record some measurement from each organism before and after a treatment. For example, if you were studying heart rate in relation to position (sitting vs. standing) you might measure the heart rate of a number of people in both positions. In this case the heart rate of a particular person when sitting is paired with the heart rate of the same person when standing.

### What’s the point of a paired-sample design?

```{r, echo = FALSE}
drug_data <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```

A paired sample design can be very useful in biology because we often have the problem that there is a great deal of variation between individual organisms. In some cases there may be so much variation between the organisms within each sample that the effect of any difference between the samples is obscured.

Consider the following example. A drug company wishes to test two drugs for their effectiveness in treating a rare illness in which glycolipids are poorly metabolised. The company is only able to find 8 patients willing to cooperate in the early trials of the two drugs. The 8 patients vary greatly in their age, body weight, severity of symptoms and other health problems. Five are female and three are male.

If the group of 8 patients was randomly assigned to one or other drug treatments and their performance monitored it would be very difficult to detect significant differences between the treatments. This is because we have an experiment which provides very little replication, yet we should expect considerable variability from one person to another both in the levels of glycolipid before any treatment. There may also be problem if male and female patients respond differently (even if in the same direction), since one treatment will have more males and fewer females assigned to it.

One solution to all these problems is to treat each patient with both drugs in turn and record the glycolipid concentrations in the blood, for each patient, after a period taking each drug. We'll see why shortly.

The best arrangement would be for four patients to start with drug A and four with drug B, and then after a suitable break from the treatments, they could be swapped over onto the other drug. This would give us eight replicate observations on the effectiveness of each drug and we can determine for each patient which drug is more effective. This kind of experimental design is called a cross-over study. It can be problematic if, for example, "carry-over" effects occur, e.g., the effect of one drug is altered when the other drug has previously been administered. We won't worry about these problems here though.

The experimental design, and one possible outcome, can be schematically represented as in the diagram below... where each patient is represented by a number (1-9). The order does not matter, i.e. it doesn’t mean that Drug A was tested before Drug B just because Drug A appears first.

```{r drug-linked, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Data from glycolipid study, showing paired design. Each patient is denoted by a unique number.'}
ggplot(drug_data, aes(x = Drug, y = Glycolipid, 
                      group = Patient, label = Patient)) + 
  geom_line() + 
  geom_label(alpha = 0.5, size = 3)
```

Notice that there is a lot of variability both in the glycolipid levels of each patient, and also in the amount by which the drugs differ in their effects (e.g. for patient C the drugs have equal effects, while for patient A drug B is more effective).

What is obvious from this pattern is that although the glycolipid level varies a good deal between patients, as far as each individual patient goes, Drug B does appear to reduce glycolipid levels more than Drug A in all but one case (patient C). This is important information about the performance of the drugs.

The advantage to using a paired sample design in this case is clear if we look at the results we might have obtained on the same patients but by dividing them into two groups of four and giving one group Drug A and one group Drug B:

```{r drug-not-linked, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Data from glycolipid study, ignoring paired design.'}
ggplot(drug_data, aes(x = Drug, y = Glycolipid, label = Patient)) + 
  geom_point(size = 4, alpha = 0.5)
```

The patients and their glycolipid levels are identical to those in the previous diagram, but only patients B, D, E and H (selected at random) were given Drug A, while only patients A, C, F, and G were given Drug B. Clearly, if you calculated the means of the two groups, they would be very similar and a t-test of the two samples, given the variation between patients, would almost certainly find no significant difference between them.

So, it would be quite possible to end up with two groups where there was no clear difference in the mean glycolipid levels between the two drug treatments even though (as we have seen in the previous diagram) Drug B seems to be rather more effective in almost all patients. What the pairing is doing is allowing us to factor out (i.e. remove) the variation among individuals, and concentrate on the differences between
the two treatments. The result is a much more sensitive test.

### How do you do a t-test on paired samples?

It should be clear why a paired-sample design might be useful, but how do you actually do the test?

The thing to note here is that we can forget about the original data values for the two samples, and just concentrate on the differences between each pair of values. So in the case of the glycolipid levels
illustrated in the first diagram, we noted that there was a greater decrease of glycolipids in all but one patient using Drug B compared with Drug A.

If we had calculated the actual differences (i.e. subtracted the value for Drug A from the value for Drug B) for each patient we might have got something like...

    -3, -4,  0, -2, -3, -1, -2, -1

Notice that there isn't a single positive value in this sample of differences. The mean difference is -2, i.e. on average, for individual patients, glycolipid levels are lower with Drug B. Another way of stating this observation is that *within subjects* (patients), the mean difference between drug B and drug A is -2.  

If, on the other hand, the two drugs had had similar effects then what would we expect to see? Well obviously we would expect no consistent difference in glycolipid levels between the Drug A and Drug B treatments. Glycolipid levels are unlikely to remain exactly the same over time, but there shouldn’t be any pattern to these changes with respect to the drug treatment, some patients will show slight increases, some slight decreases and some no change at all. The mean of the differences in this case should be somewhere around zero.

So, what we do to carry out a t-test on paired-sample data is to find the mean difference of all the pairs and test this to see if it is significantly different from zero. You already know how to do this! **This is just an application of the one-sample t-test**. The thing to remember here is that although you started out with two sets of values, the data you are actually evaluating is the mean differences between pairs. This is just one set of numbers that you can think of as a sample of the differences.

When used to analyse paired data in this way, the test is sometimes referred to as a paired-sample t-test. This is not wrong, but it important to remember that a paired-sample t-test is is really it is just a one-sample t-test applied to the differences between pairs of associated observations. Just don't forget that a one-sample t-test can be used for things other than testing differences---it is a useful test in its own right.

Many computer packages do offer the option of a ‘paired sample t-test’ to save you the effort of calculating differences. The computer finds the differences between pairs for you as part of the test, but it is still just doing a one-sample test on those differences. Just remember, you could just calculate the differences yourself, then do a one-sample test.

R offers a dedicated procedure for doing paired-sample t-tests. We are still going to show you how to do it the ‘old fashioned’ way first---calculating the differences and running a one-sample test. This is a good idea because it will help you understand how the test works.

### Carrying out a t-test on paired-sample data using R

<div class="exercise-box">
#### Work through the example
<div class="box-text">
You should work through the one sample t-test example in this section. You will need to download the GLYCOLIPID.CSV file from MOLE and place it in your working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name `glycolipid`.
```{r, echo=FALSE}
glycolipid <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```
</div>
</div>

Staying with the problem of trials of two drugs for controlling glycolipid levels, the serum glycolipid concentration data from such a trial (not those used in the schematic illustration above) are stored in the file, 'GLYCOLIPID.CSV'. We have read this into an R data frame, giving it the name `glycolipid`. As always, we should start by looking at the raw data. We'll use `glimpse` to do this:
```{r, echo=FALSE}
glimpse(glycolipid)
```
You may also wish to examine the data with the `View` function to ensure that you understand how it is organised. 

First we need to calculate the differences between each pair. We can do this with the `dplyr` functions `group_by` and `summarise`:
```{r}
glycolipid.diffs <- 
  glycolipid %>%
  group_by(Patient) %>%
  summarise(Difference = diff(Glycolipid))

glycolipid.diffs
```
Don't worry too much if that looks cryptic to you. What we did was group the data by the values of `Patient`, and then used a function called `diff` (you haven't seen this before) to calculate the difference between the two Glycolipid concentrations. We stored the result of this calculation in a new data frame called `glycolipid.diffs`. This is the data we'll use it to carry out the paired-sample t-test.

<div class="advanced-box">
#### The assumptions of a paired-sample t-test
<div class="box-text">
The assumptions of a  paired-sample t-test are straightforward. There is no requirement for the original data to be normal, or have equal variances. The only assumptions are that the data are on interval or ratio scales and that the differences are approximately normally distributed. This is very useful because even where the original data are not normally distributed, the differences between pairs can often be acceptably normal.
</div>
</div>

Check, as far as you can, that the differences are not seriously non-normal in their distribution. Normality is quite hard to assess with only 8 observations though.

If the data seem OK then carry out a one-sample t-test on the calculated differences, where the null hypothesis is one where the population mean is zero. This is very easy to in R:
```{r}
t.test(glycolipid.diffs$Difference)
```
Notice that do not have to set the `data` argument to carry out a one-sample t-test on the differences. We just pass along one argument, which is numeric vector of differences, extracted from `glycolipid.diffs` using the `$` operator. If you find this confusing, try breaking the calculation up into two steps:
```{r}
gdiffs <- glycolipid.diffs$Difference
t.test(gdiffs)
```

The output has much the same structure as with two-sample t-tests...

The first line reminds us what kind of test we did, and the second line reminds us what data we used to carry out the test. 

It is the third line that really matters: `t = -2.6209, df = 7, p-value = 0.03436`. This gives the t-statistic, the degrees of freedom, and the all-important p-value associated with the test. Make sure you understand what the p-value is telling you.

The next line (`alternative hypothesis: true mean is not equal to 0`) reminds us that R has tested whether the population mean is equal to a value of zero, versus the alternative possibility that it is not equal to (i.e. greater or less than) zero.

We now need to express these results in a clear sentence incorporating the relevant statistical information to indicate whether we accept or reject our test hypothesis:

> Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A  (t = 2.62, d.f. = 7, p = 0.034).

There are a few things to point out in interpreting the result of such a test.

1.  By convention t-values are quoted as positive values whether the value of from the calculation is positive or negative — it is only the absolute size of that matters.

2.  The degrees of freedom for a one-sample test are one less than the number of differences (or number of pairs); not one, or two, less than the total number of data values.

3.  Note that since we have used a paired-sample design our conclusion stresses the fact that the use of the Drug B results in a lower glycolipid level in individual patients; it doesn’t say that the use of Drug B resulted in lower glycolipid concentrations for everyone given Drug B than for anyone given Drug A.

### The power of pairing

The paired t-test is a very powerful technique. You can get an idea of the value of the paired-sample test by seeing what would have happened if we had ignored the pairing structure of data and analysed it with an unpaired, two-sample approach (N.B.---This analysis is wrong!):
```{r}
t.test(Glycolipid ~ Drug, data = glycolipid)
```

<div class="well">
**MOLE question**

What result do you get and how does this compare with the paired-sample test?
</div>

### Using the `paired = TRUE` argument

As mentioned earlier R does have a built in procedure for doing paired sample t-tests. Now you’ve done it the hard way, try running the data you’ve just analysed through the test using that procedure to confirm it really does so the same thing. All you have to do is set the `paired` argument of the `t.test` function to `TRUE`: 
```{r}
t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE)
```
Notice that you work with the original `glycolipid` data frame, not the `glycolipid.diffs` data frame that we constructed above. R takes care of the differencing for us.

R certainly makes it easy to do paired sample t-test. It is up to you which method you use for doing standard paired-sample t-tests, just don’t forget it is really only a one-sample test wearing fancy dress.




<!--chapter:end:110_t-tests_paired_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# One-tailed vs. two-tailed tests

[[PREAMBLE]]

## One-tailed *t*-tests {#one-tailed}

In all the *t*-tests we have considered so far we been testing hypotheses of the type: ‘male and female locusts differ in length’, ‘eagle owls capture rats of different sizes during summer and winter’, ‘two drugs differ in their effectiveness at reducing glycolipid concentrations’. In such cases we simply want to know whether there is any difference at all between our two samples. Before doing the test we didn’t have specific ideas about which sample mean should be greater than the other. If our test revealed a difference between samples we were interested in it, whichever way round it was. We learned during the permutation test example that a test of this sort is called a *two-tailed test*.

However, there are occasions when we may wish to test more specific hypotheses. For example if we have two samples A and B we might only be interested in whether the mean of A is bigger than the mean of B. Testing this sort of directional hypothesis can be done using a *one-tailed *t*-test*.

is that a one-tailed test is not a different sort of *t*-test from those you have already seen, it simply refers to the use of any of the *t*-tests (two-sample, paired-sample ...) to examine an hypothesis where the direction of the effect is specified (if this doesn’t make sense just now it should start to as we go on!).

### An example of a one-tailed hypothesis

A farmer has been persuaded to try a new pesticide called Toxic Death on his broad bean crop. He sprays 20 fields of beans with Toxic Death and leaves 20 fields untreated. To test the effectiveness of Toxic Death he is only interested in detecting a positive response in his crop. It makes no difference to the farmer if the pesticide has no effect or proves to reduce the crop yield: in either case the product is a waste of money to the farmer and he will not use it again.

The farmers’ test hypothesis is quite specific, in terms of the direction of the effect that is being tested for: ’Toxic Death increases the mean yield of broad beans’.

This is what is meant by a one-tailed test. In a one-tailed test we may be testing for a positive response, or for a negative response - but not both.

### So how do we perform a one-tailed *t*-test?

We are interested in testing the hypothesis: ‘Toxic Death increases the mean yield of broad beans’ since this is the hypothesis of greatest relevance to the farmer. If the yield of beans in the two treatments was as illustrated below...

```{r, echo=FALSE}
set.seed(27081975)
n_rep <- 20
trts <-  rep(c("No Toxic Death", "With Toxic Death"), n_rep)
df <- data.frame(Pesticide = trts, Yield = 4 + rnorm(n_rep, mean = c(0, 1))) 
ggplot(df, aes(x = Yield)) + 
  geom_histogram(binwidth = 0.5) + 
  facet_wrap(~ Pesticide, ncol = 1) +
  xlab("Yield (t/ha)")
```

...we would not even need to perform a test. The mean yield in the Toxic Death treatment is actually lower than in the control — we can automatically reject the hypothesis that treated fields have higher yield. The Toxic Death salesman might be in for some of his own medicine!

However, if the results indicated that the mean yield was greater where Toxic Death was used we would then want to perform the test to determine how confident we can be that this is a real increase rather than a chance outcome.

To carry out a one-tailed *t*-test we do exactly the same as we did for the previous *t*-tests, except that when we come to find the probability (*p*) value to judge the significance of the test, the correct probability for a one-tailed test is half that found for the two-tailed test.

So, suppose we had performed a two-tailed test (i.e. a test of the hypothesis "Toxic Death changes the yield of broad beans"" – no direction specified) and found a positive effect of Toxic Death, but with a probability *p*=0.08. Performing a one-tailed test of the hypothesis "Toxic Death increases the yield of broad beans" would give a probability of exactly half this (*p*=0.04).

In this case using the two-tailed test we would have concluded that there was no significant effect of Toxic Death on the yield of broad beans (*p*=0.08), whereas with the one-tailed test we would conclude that Toxic Death significantly increased the yield of broad beans (*p*=0.04).

This is a rather striking change of conclusion, which may seem like a fiddle. It is not a fiddle (at least not if used properly) but because using a one- rather than two-tailed test can alter the conclusion you draw, such tests should be used with caution, and the rules about how and when to use them strictly adhered to. These rules are discussed below, but first we will see how to actually do a one-tailed test in R.

### Carrying out one-tailed *t*-tests in R

Remember that we said one-tailed tests were not a different sort of *t*-test to those you’ve seen so far, you can do one-tailed tests with any of the *t*-tests you’ve seen so far. We'll show you how to do it using one example, the paired sample *t*-test, applied to drug data.

#### Doing a one-tailed paired-sample *t*-test

```{r, echo = FALSE}
glycolipid <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```

Let’s go back to the data on glycolipid concentrations in eight patients being treated with Drugs A and B. Imagine now that rather than A and B being two new drugs, Drug A is the existing treatment for the disease, while Drug B is a new type of drug being tested for effectiveness. In this case the drug company is obviously only interested in whether the new drug causes a greater reduction in the glycolipid levels of individual patients than the old one. If it has the same effect, or if it is less effective than the existing treatment it will not be worth spending time and money developing to the production stage.

So the company’s test hypothesis is: ‘The new drug (B) is more effective than the existing treatment (A) at reducing glycolipid levels’. Let’s test this. In order to do this we have to set one more argument in the `t.test` function. This one, called `alternative`, can take one of three values: "two.sided", "less", or "greater". We pick a one-side test with an associated direction of effect by choosing "less" or "greater". Here is how it works:

```{r}
t.test(Glycolipid ~ Drug, data = glycolipid, 
       paired = TRUE, alternative = "greater")
```

Are you confused--why did we set the alternative to "greater"? We wanted to assess whether drug B really leads to **lower** glycolipid concentrations. Look at the `mean of the differences` in the output. R has assumed that we wanted to examine at the 'Drug A - Drug B' differences because Drug A appears first in the data frame. It doesn't actually matter which way round we calculate the differences--the t-statistic and p-value will be the same. However, we do have to be careful to make sure that the direction of the alternative hypothesis that we choose is correct. It is easy to get this wrong if you are not paying attention. This is why R always prints the means. This is also another reason why it is important to know your data before you start analysing it.

Compare this with the output from the previous (two-tailed) test on the glycolipid data. You should find that it differs in two respects..

1.  The hypothesis now specifies that it is a one-tailed test: means that R has tested whether the mean of our sample is greater than zero, as opposed to equal to or greater than zero. Remember, the differences are 'the wrong way around' so this is the right test.

2.  The probability is half the value it was before, i.e. the result is ‘more significant’ when done as a one-tailed test.

Our conclusion from this test would be:

> Individual patients had significantly lower serum glycolipid concentrations when 
> treated with Drug B than when treated with Drug A (one-tailed test: t=2.62, d.f.=7, p=0.017).

Note two things about the conclusion. First, you should specify that a one-tailed test was used. If no information is given it is conventional to assume a two-tailed test has was used. Second, it is sensible to put the actual probability level, rather than just giving the category for *p*. This is because if anyone does then want to see what the significance of the two-tailed test would have been, they can easily double the probability, which cannot be done if we just say *p*<0.05.

Also note that in this case, the drug effect was significant in both one- and two-tailed tests, but it is not always so.

### When to use, and not to use, one-tailed *t*-tests

As we saw above, whether you use a one- or two-tailed tests can sometimes appear to change our conclusions rather dramatically. There is an obvious temptation here! It would be easy to collect your data and see which mean value is larger and then test for differences in that direction using a one-tailed test, since this would increase the apparent significance of the results. Why? Well, if you do this you are implicitly doing a two-tailed test (you are going to test the effect whichever direction it goes in) while using the extra power of a one-tailed test by pretending only one direction is being considered. It is very important to get clear in your mind what one-tailed tests do and when (if ever) you might legitimately use them.

The key principle is that the direction of the predicted effect must be specified before the data are collected. You are then effectively forfeiting the right to test for differences in the opposite direction to that predicted. You are putting all your statistical eggs in one basket---if the result is the other way round you are saying you are not interested in testing it.

What this means is that one-tailed tests are often not that useful in investigative research. Just because you have an idea about which direction you think an experimental result might go in is not a good reason to just test for that and effect in that direction. Instead you need to ask whether or not you would genuinely be prepared to ignore a result in the other direction. Usually the answer is no.

For example, if we dose the soil in which experimental plants are growing with a low concentration of a particular compound we suppose will be toxic to them, we might expect that their growth will be reduced. However, if in fact they show higher growth with the compound than without it, we would almost certainly want to test to see if this was a genuine effect (perhaps the compound also contains important trace nutrients, or affects the microbial community in the soil) or whether the compound is really having no effect and the difference we see is just chance variation between the two samples. We would, therefore, be better off using a two-tailed test.

Similarly in the case of testing a new drug against an existing treatment, on the face of it we may primarily be interested in whether the new treatment is better than the old one, and might consequently think of a one-tailed test. However, we may also be interested in whether the new treatment is actually worse that the old one, rather than simply the same - i.e. an effect in the opposite direction to that we predict. Why? Well perhaps the new treatment has fewer side effects, so even if it is only of the same efficacy as the old one, it may still be preferable - but we would most definitely want to ensure that it was no worse! So a two-tailed test might still be the most appropriate analysis.

**So if there are so many problems why use them?** There are many situations where you are interested in the direction of the effects, rather than understanding mechanisms. Here one-tailed tests are a useful tool. Testing medical or veterinary products for efficacy might be one (as discussed above). Another is in situations such as industrial processes: if you are in charge of managing the production of blood test kits, and you are considering a change to the production process you might want to sample the production line under the old and new systems and test whether the new system has a lower failure rate. You are only interested in an improvement – if the change has a higher failure rate, or simply makes no difference, you are not going to convert the entire production process to the new system. Here the extra power to detect an effect in a specified direction would certainly be worth considering.

One-tailed tests have their uses (and you will see them in statistics books and used in biological studies, so you need to know what they are) but they should be used with caution. The default procedure should be to use a two-tailed test unless there are very good reasons for doing otherwise.

<!--chapter:end:120_one_vs_two-tailed_tests.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Associations {-} 

# Simple regression

The simple statistical tests we encountered so far have been concerned with how to the compare mean(s) of numeric variables in some way. We now know how to...

-   compare one mean to any particular value, via the one-sample *t*-test

-   compare means among two groups or experimental conditions, via the two-sample *t*-test. 

(What about the paired-sample *t*-test? This also evaluates differences among two conditions, but is really a special type of one-sample *t*-test because it evaluates whether the mean change *within* objects is different from 0)

One way to think about the two-sample and paired-sample *t*-tests is that they evaluate whether or not a numeric variable changes, an average, among two groups or experimental conditions. We saw that in R, the two different groups/conditions can be encoded by the values of a categorical variable. We use a formula involving the numeric (`num_var`) and categorical (`cat_var`) variables to set up the test (e.g. `num_var ~ cat_var`). There is a good reason for doing things this way. It reflects the fact that we can conceptualise the two-sample and paired-sample *t*-tests as considering a *relationship* between between a numeric and categorical variable. 

(By the way, it is perfectly possible to evaluate differences among means in more than two categories, but we should do this using a more sophisticated tool called Analysis of Variance [ANOVA]. We'll learn about ANOVA in later chapters...) 

This chapter is about a different kind of relationship. The question we want to address is---what should we do when numeric variables have been measured (or manipulated) from the objects/individuals in a study? One very powerful tool 

## Introduction to the problem

We often need to explore the relationship between numeric variables. For example...

-   we might sample fish and measure their length and weight

-   we could survey grassland plots and measure soil pH and species diversity

-   we may manipulate temperature and measure enzyme activity in bed bugs

In each of these settings we want to understand the relationship between the numeric variables we measured. We may be interested in knowing:

```{r, echo = FALSE}
x <- data.frame(x = seq(-2, 2, length.out = 50))
set.seed(27081975)
```

Are the variables related or not? There's not much point studying a relationship that isn't there.

```{r, echo = FALSE, fig.width=6}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Related"),
          mutate(x, y = 3 +     rnorm(n(), sd = 1.0), labs = "Unrelated")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

Is the relationship positive or negative? Sometimes we can answer a substantive scientific question just by knowing the direction of a relationship.

```{r, echo = FALSE, fig.width=6}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Positive"),
          mutate(x, y = 3 - x + rnorm(n(), sd = 0.5), labs = "Negative")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

Is the relationship a straight line or a curve? The form of a relationship may be important if we want to make predictions.

```{r, echo = FALSE, fig.width=6}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Straight"),
          mutate(x, y = 4.5 + .8*x - .6*x^2 + rnorm(n(), sd = 0.45), labs = "Curved")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

Although sometimes it may be obvious that there is a relationship between two variables from a plot of one against the other, at other times it may not.

```{r, echo=FALSE, fig.width=6}
x <- data.frame(x = rnorm(50))
bind_rows(mutate(x, y = 3 + 0.3 * x + rnorm(n(), sd = 0.5), labs = "Related"),
          mutate(x, y = 3 + 0.2 * x + rnorm(n(), sd = 0.5), labs = "Unrelated")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

You might not be very confident in judging which, if either, of these plots provides evidence of a positive relationship between the two variables. Maybe the pattern the we perceive can just be explained by our old friend sampling variation, or maybe it can't. Clearly it would be useful to have a measure of how likely it is that the relationship we think we see could have arisen as a result of sampling variation. In addition to judging the statistical significance of a relationship, we may also be interested in describing the relationship mathematically – i.e. finding the equation of the best fitting line through the data. 

A linear regression analysis allows you to do all this (and more). 

## What does linear regression do?

**Simple linear regression** finds the straight-line relationship which best describes the dependence of one variable (the **dependent variable**) on the other (the **independent variable**). 

-   What does the word 'simple' mean here? A simple linear regression is a regression model which only accounts for one independent variable. If more than one independent variable is considered, the correct term to describe the resulting model is 'multiple regression'. We'll only look at simple regression in this chapter.

-   What does the word linear mean here? [[FINISH ME]]

It soon becomes tedious reading/writing 'simple linear regression' all the time, so to simplify things, we'll often just write 'regression'. Just keep in mind that we're always talking about simple linear regression in this chapter. 

A simple linear regression describes the response of the dependent variable to a unit change in the value of the independent variable. It is also conventional to label the dependent variable as '$y$' and the independent variable as '$x$'. When we present such data graphically, the independent variable goes on the $x$-axis and the dependent variable on the $y$-axis. Try not to forget these conventions!

How to select which is to be used as the dependent and which as the independent variable is fairly straightforward in an experimental setting. 

[[Explain the experiment]]

Temperature was manipulated in this experiment, so it absolutely must be designated the independent variable. Moreover, *a priori* (before conducting the experiment), we can reasonably suppose that changes in temperature may cause changes in enzyme activity, but the reverse seems pretty unlikely.

However, it may not always so clear cut when we are working with data from an observational study. These problems will be discussed in more detail later. There is one important point to be aware of now however: in regression it matters which way round you choose the dependent and independent variables. If you have two variables A and B, the relationship you find from a regression will not be the same for A against B as for B against A.

```{block, type='do-something'}
**'dependent vs. independent'  or 'response vs. predictor'?**

Another way to describe linear regression is that it allows us to predict how one variable (the **response variable**) responds to another (the **predictor variable**). The dependent vs. independent  and response vs. predictor conventions for variables in a regression are essentialy equivalent. The only differ in the nomenclature they use to describe the variables involved. 

To avoid confusion, we will stick with dependent vs. independent naming convention in this course.
```

## How does it work?

If you draw a straight line through a set of points on a graph then, unless they form a perfect straight line, some points will lie close to the line and others further away. The vertical distances between the fitted line and each point (i.e. measured parallel to the $y$-axis) have a special name. They are called the *residuals*. 

The residuals are the ‘bits left over’ after the line has been fitted---i.e. the distances from individual points to the fitted line---and they give an indication of how well the line fits the data. If all the points lay in a perfect straight line there would be no residuals as the line would pass through all the points. If many of the residuals are large then this indicates that the data are quite scattered around the line.

Regression works by finding the line which leaves the smallest total of the residuals. In fact, to be strictly correct, it minimises the sum of the squared residuals. The following illustration indicates the principle of this process:

```{r, fig.width=4, echo = FALSE}
set.seed(27081976)

exp.data <- 
  data.frame(x = seq(-2, +2, length = 12)) %>% 
  mutate(y = x + rnorm(n(), sd = 1), y = y - mean(y))

lm.mod <- lm(y ~ x, data = exp.data)

mod.data <- data.frame(x = seq(-2.2, +2.2, length.out = 25))
all.mod.data <- list()

all.mod.data[[1]] <- 
  mod.data %>% 
  mutate(y = mean(exp.data$y), labs = "A")
all.mod.data[[2]] <- 
  mod.data %>% 
  mutate(y = 0.5*x*coef(lm.mod)[2], labs = "B")
all.mod.data[[3]] <- 
  mod.data %>% 
  mutate(y = 1.6*x*coef(lm.mod)[2], labs = "D")
all.mod.data[[4]] <- 
  mod.data %>%  
  mutate(y = predict.lm(lm.mod, newdata = .), labs = "C")

all.mod.data <- bind_rows(all.mod.data)

ggplot(all.mod.data, aes(x = x, y = y)) + 
  geom_point(data = exp.data, colour = "blue") + 
  geom_line() + facet_wrap(~labs, nrow = 2)
```

The data are identical in all four graphs, but in the top left left hand graph a horizontal line (i.e. no effect of $x$ on $y$) has been fitted, while on the remaining three graphs sloping lines of different magnitude have been fitted. To keep the example simple, we assume we know the true intercept of the line, which is at $y=0$, so all four lines pass through $x=0$, $y=0$ (i.e. the 'origin').

```{block, type='do-something'}
**Which line is best?**

One of the four lines is the 'line of best' fit from a regression analysis. Spend a few moments looking at the four figures. Which line seems to fit the data best? Why do you think this line is 'best'?
```

Regression works by finding the intercept and slope that minimises the vertical distances between the line and each observation. Actually, it minimises something called the 'sum of squares' of these distances---more on that in a moment. First, we should visualise these distances:
```{r, fig.width=4, echo = FALSE}
all.exp.data <- list()

all.exp.data[[1]] <- 
  exp.data %>% 
  mutate(yend = mean(exp.data$y), labs = "A")
all.exp.data[[2]] <- 
  exp.data %>% 
  mutate(yend = 0.5*x*coef(lm.mod)[2], labs = "B")
all.exp.data[[3]] <- 
  exp.data %>% 
  mutate(yend = 1.6*x*coef(lm.mod)[2], labs = "D")
all.exp.data[[4]] <- 
  exp.data %>%  
  mutate(yend = predict.lm(lm.mod, newdata = .), labs = "C")

all.exp.data <- bind_rows(all.exp.data)

ggplot(all.exp.data, aes(x = x, y = y)) + 
  geom_segment(colour = "darkgrey",
               aes(xend = x, y = y, yend = yend)) + 
  geom_line(data = all.mod.data) + 
  geom_point(data = exp.data, colour = "blue") + 
  facet_wrap(~labs, nrow = 2)
```

Notice we said that it is the vertical distance that matters, not the perpendicular distance from the line. That's...

You should be able to see that, for the horizontal line, taking all the residual distances, squaring these, and adding them up will produce a greater residual sum of squares than doing the same with the sloping lines. This suggests that the sloping lines fit the data better. The line with the *lowest* residual sum of squares is the best line, because it ‘explains’ more of the variation in the dependent variable.

Which one is best though? To understand this we need to calculate the residual sum of squares for each line. These are...
```{r, echo = FALSE}
all.exp.data %>% 
  rename(Line = labs) %>% group_by(Line) %>% 
  summarise('   Residual Sum of Squares' = sum((y-yend)^2)) %>% as.data.frame
```
So it looks like the line in panel C is the best fitting line among the candidates--in fact, it is the best fit line among all possible candidates. Did you manage to guess this by looking at the lines and the raw data? If not, think about why you got the answer wrong (and ask a demonstrator if you can't see why you got it wrong).

A final comment. It is really important that you understand what a residual from a regression represents. Residuals pop up all the time when evaluating statistical models (not just regression). For example, the assumptions of linear regression models can be evaluated by looking at their residuals. We're going to learn how to do this in the next chapter.

## What do you get out of a regression?

The results from a regression analysis will tell us (among other things):

-   whether there is a statistically significant relationship between the $x$ and $y$ variables. That is, a regression analysis will tell you whether the association is likely to be real, or just a chance outcome resulting from sampling variation. 

-   the equation of the straight line that best describes how the $y$ (dependent) variable *depends on* the $x$ (independent) variable. To put it in slightly more technical terms, it describes the $y$ variable as a function of the $x$ variable.

What is the form of the relationship? The equation for a straight line relationship is...

$$y = a + b \times x$$

...where $y$ is the dependent variable, $x$ is the independent variable, $a$ is the value at which the line crosses the $y$ axis, $b$ is the slope of the line. The $a$ and the $b$ are referred to *parameters* or *coefficients* of the line.

The slope of the line is usually the thing we care about most. It tells us the amount by which $y$ changes for a change of one unit in $x$. If the value of $b$ is positive (i.e. a plus sign in the above equation) this means the line slopes upwards to the right. A negative slope ($y = a - bx$) means the line slopes downwards to the right. The diagram below shows the derivation of an equation for a straight line.

```{r, fig.width=4, echo = FALSE}
I <- 1
S <- 2/3
data.frame(x    = c(0, 3), 
           y    = c(I, I), 
           xend = c(3, 3), 
           yend = c(I, I + 3 * S)) %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
  geom_segment(colour = "darkgrey") + 
  geom_abline(intercept = I, slope = S, linetype = 2) + 
  scale_x_continuous(limits = c(-0.25, 3.5)) + 
  scale_y_continuous(limits = c(-0.25, 4.0)) + 
  xlab("Independent variable (x)") + ylab("Dependent variable (y)") + 
  annotate("text", x = 0.2, y = 3.9, hjust=0, parse = TRUE, label = "b==Delta*y/Delta*x") + 
  annotate("text", x = 1.6, y = 3.9, hjust=0, parse = TRUE, label = "b==2/3") +
  annotate("text", x = 0.2, y = 3.5, hjust=0, parse = TRUE, label = "a==1") + 
  annotate("text", x = 0.2, y = 2.9, hjust=0, parse = TRUE, label = "y==1+0.667*x") + 
  annotate("text", x = 1.5, y = 0.7, hjust=0, parse = TRUE, label = "Delta*x==3") + 
  annotate("text", x = 2.1, y = 1.8, hjust=0, parse = TRUE, label = "Delta*y==2") +
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0)
rm(I, S)
```

Having the equation for a relationship allows you to predict the value of the $y$ variable for any value of $x$. For example, in the enzyme example, our physiologist might be interested in getting an equation which would allow her to predict the amount of glucose released at any temperature, even if she had not carried out experiments at that precise temperature (e.g. 5C). Such predictions can be made by hand (see below) or using R (details later).

In the above diagram, the regression equation is: $y = 1 + 0.66 x$. So to find the value of $y$ at $x = 2$ you get: $y = 1 + (0.667 \times 2) = 2.32$. Obviously, by finding $y$ values for 2 (or preferably 3) different $x$ values from the equation, the actual line can easily be plotted on a graph by hand if required---plot the values and join the dots!

N.B.: The relationship you are looking at may only be linear over a certain range of the data (i.e. we would be surprised if amylase activity continued to increase at the same rate up to 100C). So when fitting a straight line you should not normally extend the line beyond the range of the original data, and be very wary of using the equation to predict $y$ values outside the range of the $x$ variable from which the line was derived.

## The assumptions of regression

Simple linear regression is a type of parametric model, which means it makes a number of assumptions (some of which are often ignored at times in the biological literature!). Let's skim through them and then consider each one in more detail:

1.    **Independence.** The residuals must be independent.

2.    **Measurement scale.** The dependent $y$ variable is measured on an interval or ratio scale.

3.    **Linearity** The relationship between the independent $x$ variable and the dependent $y$ variable is linear.

4.    **Normality.** The residuals are drawn from a normal distribution.

5.    **Constant variance.** The variance of the residuals is constant.

6.    **Measurement error.** The values of the independent $x$ variable are determined with negligible error.

What are these all about? There seem to be lots of assumptions, but most are straightforward to deal with. 

1.    Another way of stating the first assumption is that the value of each residual does not depend on the value of any others. This can be difficult to check. If the data are from an experiment and we made sure to properly randomise the treatments, it should be OK. If the data are observational, then we need to be more careful. The reason this assumption matters is because, if the residuals are not independent, any p-values we generate will not be reliable (they will typically be too small).

2.    The data type of $y$ should be straightforward.

3.  Obviously there is little point in fitting a straight line to data which clearly don’t form a straight line relationship. There may also be circumstances in which it is theoretically unlikely (or impossible) for a relationship to be linear, e.g. the length and weight of an animal will not be well described by a linear relationship because weight is generally a cubic function of length. If the data fail this assumption (not uncommon with biological data) then sometimes applying a mathematical transformation of $x$ (and maybe $y$ too) can help. We will discuss this idea later in the course.

4.    This is a bit less obvious. It essentially means that for each value of $x$ we would expect there to be a range of responses in $y$---e.g. for a given temperature we wouldn’t expect all beans to release exactly the same amount of glucose---which follow a normal distribution. In fact, it is the distribution of the deviations of $y$ from the fitted line (the residuals) that are assumed to be normal. This means that we can generally only test this assumption *after* the line has been fitted. It does not make sense to evaluate this assumption by looking at the raw $y$ values.

5.    This assumption essentially means the variance of the residuals is not related to the value of the independent $x$ variable. It is violated if the scatter of the data (size of the residuals) increases or decreases markedly as $x$ gets larger. If the data fail this assumption (not uncommon with biological data) then sometimes applying a mathematical transformation of $y$ will help-–-but we have to be careful not to introduce other problems. We will discuss this idea later in the course.

6.    It is often hard to obtain the $x$ values with absolutely no error (random variation), but hopefully the error will at least be smaller than that in the $y$ values. So for example, in this experiment, the temperature values (set by the experimenter) almost certainly have little error, but the glucose release is likely to have much greater error, both through measurement error, and also because of real biological variation between beans.

```{block, type='advanced-box'}
**Checking the assumptions of regression**

Assumption 2 (measurement scale) is easy to evaluate. Assumptions 1 (independence) and 6 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data; you have to think about the data and see if there are any obvious reasons why they might not be valid. 

There are a special set of tools, called 'regression diagnostics', that allow us to evaluate the remaining assumptions. We are going to study these in the next chapter, so for now we will rely on simple, but less effective means: assumptions 3 (linearity) and 5 (equal variance) can be informally evaluated by looking at a scatter plot of the data; assumption 4 (normality) can be checked by looking at the distribution of the residuals from the fitted the regression model.
```

## An example {#regression-example}

A plant physiologist studying the process of germination in the broad bean (*Vicia faba*) is interested in the relationship between the activity of the enzyme amylase, and the temperature at which the germinating beans are kept. As part of this work she carries out an experiment to find the relationship between glucose release (from the breakdown of starch by amylase) and temperature (over the range 2 - 20C). The data obtained from such an experiment are given below.

||||||||||||
|:---------------------------------------|:--|:----|:----|:----|:----|:----|:----|:----|:----|:----|
| Temperature ($C$)                      | 2 | 4   | 6   | 8   | 10  | 12  | 14  | 16  | 18  | 20  |
| Glucose ($\mu g$ $mg^{-1}$ dry weight) | 1 | 0.5 | 2.5 | 1.5 | 3.2 | 4.3 | 2.5 | 3.5 | 2.8 | 5.6 |

The data are in a CSV file called GLUCOSE.CSV. The first column (`Temperature`) contains the information about the experimental temperature treatments, and the second column (`Glucose`) contain the glucose measurements.

(Notice that we're referring to the different temperatures as 'treatments'. This is perfectly reasonable here, as these data are from an experiment where temperature was controlled by the investigator.)

```{block, type='do-something'}
**Work through the germination example**

You should work through the *Vicia* germination example from this point. Start by downloading the GLUCOSE.CSV file from MOLE and placing it in your working directory.
```

```{r, echo=FALSE}
vicia.germ <- read.csv(file = "./data_csv/GLUCOSE.CSV")
```

Once you have downloaded the data read it into an R data frame, giving it the name `vicia.germ`
As always, make sure you use `View`, `glimpse`, etc to examine the data before you proceed. Run through all the usual questions... How many variables (columns) are in the data? How many observations (rows) are there? What kind of variables are we working with?

We should visuaise the data next so that we understand it more. We just need to produce a simple scatter plot, which is easy to do with `ggplot2`:
```{r, fig.width=4}
ggplot(vicia.germ, aes(x = Temperature, y = Glucose)) + 
  geom_point()
```

The plot clearly suggests that there is some sort of positive relationship which might reasonably be described by a straight line.

What we want to do is work out whether there a statistically significant relationship between temperature and glucose release (and hence, presumably, amylase activity). If there is, we might also want to find the equation that describes the relationship. We will return to the mechanics of actually doing this later. But first, keeping the example in mind, we will consider the basic ideas of what regression does, and for what sort of data it is suitable.

```{block, type='warning-box'}
**Variables and axes**

Be careful when you produce a scatter plot to summarise data in a regression analysis. You need to make sure the two variables are plotted the right way around with respect to the $x$ and $y$ axes. We place the 'dependent' (or 'response') variable on the $y$ axis and the 'independent' (or 'predictor') variable on the $x$ axis. We will explain these terms a bit more in a moment. For now, just make a mental note of the fact that there is a right and wrong way to plot the data for a regression. Nothing says "I don't know what I'm doing" as effectively as plotting the variables on the wrong axes.
```

## Carrying out regression with R {#regression-R}

OK, let's carrry on with our example and see how to carry out a simple regression in R.

### Checking the assumptions

The data are on ratio (glucose release, $\mu g$ $mg^{-1}$ dry weight) and interval (temperature, °C) scales, the assumption of negligible measurement error seems perfectly reasonable, and without more knowledge of the experimental design, we have to assume that the independence assumption is met. As noted above, it is better to evaluate the remaining assumptions using regression diagnostics, but let's proceed using rough checks that rely on inspection of a scatter plot of the data:
```{r, fig.width=4}
ggplot(vicia.germ, aes(x = Temperature, y = Glucose)) + 
  geom_point()
```

The scatter plot suggests that the relationship between $x$ and $y$ is linear, and the scatter in $y$ neither increases nor decreases substantially with increasing values of $x$. It is quite had to judge whether there is anything obviously wrong with the normality assumption using this plot---we will check this assumption more carefully in a moment by looking at the residuals. 

### Model fitting and significance tests

Now that we understand roughly how simple linear regression works, and since the data appear to meet the requirements of this model, we can finally move on to some results. Carrying out a regression analysis in R is really no different from ANOVA. It is a two step process. The first step is a model fitting step. This is where R calculates the best fit intercept and slope, along with additional information needed to generate the results in step two.

Once again, we carry out the model fitting step using the `lm` function (remember, the data are in `vicia.germ`, `Temperature` is the independent variable, and `Glucose` is the dependent variable):
```{r} 
vicia.model <- lm(Glucose ~ Temperature, data = vicia.germ)
```
This should appear very familiar by now. We have to assign two arguments:

1. The first argument is a **formula** (i.e. it includes a 'tilde' symbol: `~`). The variable name on the left of the `~` should be the dependent variable and the variable name on the right should be the independent variable (`Glucose` and `Temperature`, respectively).

2. The second argument is the name of the data frame that contains the two variables listed in the formula (`vicia.germ`).

```{block, type='advanced-box'}
**How does R knows we want to carry out a regression?**

How does R know we want to use regression? After all, we didn't specify this anywhere. The answer is that R looks at what type of variable `Temperature` is. It is numeric, and so R automatically carries out a regression. If it had been a factor (i.e. a categorical variable) R would have carried out a different kind of analysis, called a one-way ANOVA. We'll learn about ANOVA later. 

Most of the models that we examine in this course are very similar, and can all be fitted using the `lm` function. The only thing that really distinguishes them is type of variables that appear to the right of the `~` in a formula: if they are factors (categorical variables) we end up carrying out ANOVA, while numeric variables lead to a regression. The key message is that you have to keep a close eye on the type of variables you are modelling to understand what kind of model R will fit.
```

As usual, we assigned the result a name (`vicia.model`) so that `vicia.model` now refers to a fitted model object. When looking at ANOVA, we said that printing this object to console wasn't very useful. What happens if we print a regression model object to the console?
```{r}
vicia.model
```
Just as with ANOVA, this prints a summary of the model we fitted and some information about the 'coefficients' of the model. The coefficients, when working with a simple regression, are the intercept and slope of the fitted line: the intercept is always labelled `(Intercept)` and the slope is labelled with the name of the independent variable (`Temperature` in this case). We'll come back to these coefficients once we have looked at how to compute *p*-values.

```{r, echo=FALSE}
anova.out <- capture.output(anova(vicia.model))
```

We usually fit a regression model so that we can determine whether the slope (and perhaps, the intercept) is significantly different from zero---i.e. we want to know if the relationship between the $x$ and $y$ variables are really associated. Surprisingly, we use the `anova` function to do this. Why do we use the `anova` function? It turns out that the basic idea of ANOVA---the assessment of statistical significance of model terms via comparisons of variance---can be applied to any kind of model object created by `lm`, including a simple regression model.

As usual, all we have to do is pass the `anova` function one argument: the name of the fitted regression model object:
```{r}
anova(vicia.model)
```
The first line just informs us that we are looking at an ANOVA table, i.e. a table of statistical results from an analysis of variance. Just remember, this doesn't necessarily mean we are dealing with an ANOVA model---i.e., an ANOVA model is one in which the independent variable(s) is (are) categorical.

The second line reminds us what variable we analysed (the dependent variable). The important part of the output is the table at the end:
```{r, echo = FALSE}
invisible(sapply(anova.out[4:6], function(line) cat(line, "\n")))
```
This summarises the parts of the analysis of variance calculations, as it applies to a regression model. These should be familiar to you: `Df` -- degrees of freedom, `Sum Sq` -- the sum of squares, `Mean Sq` -- the mean square, `F value` -- the *F*-statistic (i.e. variance ratio), `Pr(>F)` -- the p-value).

The *F*-statistic (variance ratio) is the most important term. When working with a regression model, this is related to how much variability in the data is explained when we include the best fit slope term in the model. Larger values indicate a stronger relationship between $x$ and $y$. The p-value gives the probability that the relationship could have arisen through sampling variation, if in fact there were no real association: a p-value of less than 0.05 indicates a less than 1 in 20 chance of the result being due to chance, and we take this as evidence that the relationship is real.

#### Extracting a little more information

```{r, echo = FALSE}
summary.out <- capture.output(summary(vicia.model))
```

There is a second function, called `summary`, that can be used to extract a little more information from the fitted regression model:
```{r}
summary(vicia.model)
```
This is easiest to understand if we step through the constituent parts of the output. The first couple of lines just remind us about the model we fitted
```{r, echo = FALSE}
invisible(sapply(summary.out[2:3], function(line) cat(line, "\n")))
```
The next couple of lines aren't really all that useful---they summarise some properties of the residuals--so we'll ignore these.

The next few lines comprise a table that summarises some useful information about the coefficients of the model (the intercept and slope):
```{r, echo = FALSE}
invisible(sapply(summary.out[9:12], function(line) cat(line, "\n")))
```
The `Estimate` column shows us the estimated the intercept and slope of the regression. We saw these earlier when we printed the fitted model object to the console.

Staying with this table, the next three columns (`Std. Error`, `t value` and `Pr(>|t|)`) show us the standard error associated with each coefficient, the corresponding *t*-statistics, and the *p*-values. Remember standard errors? These are a measure of the variability of the sampling distribution associated with something we estimate from a sample. We discussed these in the context of sample means. It turns out that one can calculate a standard error for many different kinds of quantities, including the intercept and slope of a regression model. And just as with a mean, we can use the standard errors to evaluate the significance of the coefficients via *t*-statistics.

In this case, the *p*-values associated with these *t*-statistics indicate that the intercept is not significantly different from zero (*p*>0.05), but that the slope is significantly different from zero (*p*<0.01). Notice that the *p*-value associated with the slope coefficient is the same as the one we found when we used the `anova` function. This is not a coincidence---`anova` and `summary` test the same thing when working with regression models.

The only other part of the output from summary that is of interest now is the line containing the `Multiple R-squared` value:
```{r, echo=FALSE}
invisible(sapply(summary.out[17], function(line) cat(line, "\n")))
```
This shows the $R$-squared ($R^{2}$) of our model. It tells you what proportion (sometimes expressed as a percentage) of the variation in the data is explained, or accounted for, by the fitted line. If $R^{2}=1$ the line passes through all the points on the graph (all the variation is accounted for) and if $R^{2}\approx 0\%$ the line explains little or none of the variation in the data.

(The `Adjusted R-squared:` value can be ignored in this analysis---it is used when doing a form of regression called *multiple regression*, in which there is more than one $x$ variable. We will not be dealing with multiple regression here.)

The $R^{2}$ value here is 0.64. This is very respectable, but still indicates that there are other sources of variation (differences between beans, inaccuracies in the assay technique, etc.) which remain unexplained by the line.

#### Residual analysis

There are various situations in which we need to be able to extract the residuals from a fitted model. We'll look at one example now. Remember the 5th assumption: the residuals are drawn from a normal distribution. How might we examine the distributional assumptions of the regression? We can do this with a dot plot or histogram of the residuals, if know how to extract these from a model. This is easy in R---we use the `resid` function:
```{r}
resid(vicia.model)
```
This just extracts a numeric vector containing the residuals and prints them to the console. In order to plot these we need to put them inside a data frame (`ggplot2` only works with data frames), and store the result:
```{r}
resid.data <- data.frame(Residuals = resid(vicia.model))
```

Once we have extracted the residuals into a data frame we just use `ggplot2` in the usual way to plot them. We'll use a dot plot, as there aren't many residuals:
```{r, fig.width=4, }
ggplot(resid.data, aes(x = Residuals)) + geom_dotplot(binwidth = 0.3)
```
It is hard to know if these are normally distributed when we only have 10 observations, but there is nothing that screams 'non-normal' here.

## Presenting results {#present-results}

From the preceding analysis we can conclude...

> There is a significant positive relationship between the incubation temperature (°C) and glucose released ($\mu g mg^{-1}$ dry weight) in germinating bean seeds ($y=0.52+0.20x$,  F=14, d.f.=1,8, *p*<0.01).

Don't forget to quote both degrees of freedom in the result. These are obtained from the ANOVA table produced by `anova` and should be given as the slope degrees of freedom first (which is always 1), followed by the error degrees of freedom.

If the results are being presented only in the text it is usually appropriate to specify the regression equation as well as the significance of the relationship as this allows the reader to see in which direction and how steep the relationship is, and to use the equation in further calculations. It may also be useful to give the units of measurement---though these should already be stated in the Methods.

###

Often, however, you will want to present the results as a figure, showing the original data and the fitted regression line. In this case, most of the statistical detail can go in the figure legend instead.

We already know how to make a scatter plot. The only new trick we need to learn is how to add the fitted line. Remember the output from the summary table---this gave us the intercept and slope of the best fit line. We could extract these (the is a function called `coef` that does this), and using our knowledge of the equation of a straight line, use them to then calculate a series of points on the fitted line. However, there is an easier way to do this using the `predict` function. 
```{block, type='do-something'}
Don't worry too much if this next segment on predictions is confusing. It looks more complicated than it is, but you may have to come back to it a few times before it all sinks in. At first reading, try to focus on the logic of the calculations without worrying too much about the details.
```

In order to use `predict` we have to let R know the values of the independent variable for which we want predictions. In the bean example the temperature was varied from 2-20 °C, so it makes sense to predict glucose concentrations over this range. Therefore the first step in making predictions is to generate a sequence of values from 2 to 20, placing these inside a data frame:
```{r}
pred.data <- data.frame(Temperature = seq(2, 20, length.out = 25))
```
We learned about the `seq` function last year. Here, we used it to make a sequence of 25 evenly spaced numbers from 2 to 20. If you can't remember what it does, ask a demonstrator to explain it to you (and use `View` to look at `pred.data`). Notice that we gave the sequence the exact same name as the independent variable in the regression (`Temperature`). **This is important**: the name of the numeric sequence we plan to make predictions from has to match the name of the independent variable in the fitted model object.

Once we have set up a data frame to predict from (`pred.data`) we are ready to use the `predict` function:
```{r}
predict(vicia.model, pred.data)
```
This take two arguments: the first is the name of the model object (`vicia.model`); the second is the data frame (`pred.data`) containing the values of the independent variable at which we want to make predictions. The predict function generated the predicted values in a numeric vector and printed these to the console.

To be useful, we need to capture these somehow, and because we want to use `ggplot2`, these need to be kept inside a data frame. We can use mutate to do this:
```{r}
pred.data <- mutate(pred.data, Glucose = predict(vicia.model, pred.data))
```

Look at the first 10 rows of the resulting data frame:
```{r, echo = FALSE}
head(pred.data, 10)
```
The `pred.data` is set out much like the data frame containing the experimental data. It has two columns, called `Glucose` and `Temperature`, but instead of data, it contains predictions from the model. Plotting these predictions along with the data is now easy:
```{r, fig.width=4}
ggplot(pred.data, aes(x = Temperature, y = Glucose)) + 
  geom_line() + geom_point(data = vicia.germ) + 
  xlab("Temperature (°C)") + ylab("Glucose concentration")
```
Notice that we have to make `ggplot2` use the `vicia.germ` data (i.e. the raw data) when adding the points. 

Let's summarise what we did: 1) using `seq` and `data.frame`, we made a data frame with one column containing the values of the independent variable we want predictions at; 2) we then used the `predict` function to generate these predictions, adding them to the prediction data with `mutate`; 3) finally, we used `ggplot2` to plot the predicted values of the dependent variable against the independent variable, remembering to include the data.

```{r, eval = FALSE, echo = FALSE}
data.frame(Temperature = seq(2, 20, length.out = 25)) %>% 
  mutate(Glucose = predict(vicia.model, .)) %>% 
  ggplot(aes(x = Temperature, y = Glucose)) + 
    geom_line() + geom_point(data = vicia.germ) + 
    xlab("Temperature (°C)") + ylab("Glucose concentration")
```

## What about causation? {#causation}

No discussion of regression would be complete without a little homily on the fact that just because you observe a (significant) relationship between two variables this does not necessarily mean that the two variables are causally linked. If we find a negative relationship between the density of oligochaete worms (the dependent variable) and the density of trout (the independent variable) in a sample of different streams, this need not indicate that the trout reduce the numbers of oligochaetes by predation - in fact oligochaete numbers are often very high in slow-flowing, silty streams where they live in the sediments, trout prefer faster flowing, well oxygenated, stony streams - so a negative correlation could occur simply for that reason. There are many situations in biology where a relationship between two variables can occur not because there is a causal link between them but because each is related to a third variable (e.g. habitat).

This difficulty must always be borne in mind when interpreting relationships between variables in data collected from non-experimental situations. However, it is often assumed that because of this problem regression analysis can never be used to infer a causal link. This is incorrect. What is important is how the data are generated, not the statistical model used to analyse them. If a set of ten plants were randomly assigned to be grown under ten different light intensities, with all other conditions held constant, then it would be entirely proper to analyse the resulting data (for, let us say, plant height) by a regression of plant height ($y$) against light level ($x$) and, if a significant positive straight-line relationship was found, to conclude that increased light level caused increased plant height. 

Of course this conclusion still depends on the fact that another factor (e.g. temperature) isn’t varying along with light and causing the effect. But the fact that you are experimentally producing an effect, in plants randomly allocated to each light level (i.e. plants in which it is highly improbable that the heights happened to be positively related to light levels at the start) which gives you the confidence to draw a conclusion about causality.



<!--chapter:end:130_simple_regression.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Regression diagnostics






## Miscellaneous

The good news is that regression is generally quite a robust technique---i.e. it gives us reasonable answers even where the assumptions are not perfectly fulfilled. Be aware of the assumptions, but don’t get too worried by them–--if the violations are modest, proceed, but interpret your results with care.



<!--chapter:end:140_regression_diagnostics.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Correlation

## Why two different methods?

Correlation and regression are both concerned with describing relationships between two variables, but they are somewhat different techniques, and each is appropriate under different circumstances. This often causes confusion. Which technique is required for a particular data set depends on both the form of the data, and on the purpose of the analysis, since the two techniques make different assumptions about the data, and also yield different information. We will return to the problem of how to decide which technique is appropriate after you have become familiar with the essential features of both.



<!--chapter:end:150_correlation.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (APPENDIX) Supplementary Material {-} 

# Exercises 

## Week 2: Data and variables

### What kind of variable is it?

The following table gives a number of measurements taken in the course of a study of a woodland ecosystem. What type of variable results from the measurements taken in each case?

```{r, echo = FALSE}
table_data <- read.csv(file = "./tables_csv/variable_types.csv")
knitr::kable(
  table_data, booktabs = TRUE,
  caption = 'Examples of different kinds of variable.'
)
```

## Week 2: Statistical concepts

### Sampling distributions

Spend a few minutes looking at the distribution of purple morph counts we simulated. See if you can answer the following questions:

*   What is the most common purple morph count we should expect to find in a sample of 20 individuals, when the population frequency is thought to be 40%?

*   Imagine someone told you that they thought 40% of the plants were purple. If you sampled 20 individuals and found that 4 of them were purple, would you be convinced of their assertion? What if you found only 2 purple plants?

#### How does sample size influence the standard error?

Think back to the purple morph / green morph example. We can use a simulation in R to calculate the (approximate) standard error of purple morph frequencies. For example, if we want to know the standard error when we use a sample size of 20, and the purple morph frequency is 40% (i.e., the proportion of purple morph plants is 0.4), we can use this snippet of R code:
```{r}
purple.prob <- 0.4
sample.size <- 20
samples <- 100 * rbinom(n = 100000, size = sample.size, prob = purple.prob) / sample.size
sd(samples)
```
You **do not** have to understand exactly how this works. If you did A-level statistics you might be able to guess what the `rbinom` function is doing. Essentially, what we did was to simulate the percentage of purple morph individuals found in 100000 samples of 20 individuals (stored in `samples`), and then used the `sd` function to calculate the standard error--remember, the standard error is the standard deviation of the sampling distribution of a statistic (= morph frequency). Ask a demonstrator if you want to know more. We just want you to use this R code to do the next exercise.

Use the above code to vary the sample size from 20 to 320, doubling the sample size each time (i.e., use samples of 20, 40, 80, etc). You only need to vary the value of `sample.size` to do this. Make sure you do this in your script, not at the command line. If you are feeling ambitious, store the results of your investigation in a data frame and use `ggplot2` to help you visualise them. You don't have to do this though; it is fine just to write down the numbers.

See if you can work out how the standard error changes as the sample size increases. Does the standard error halve when you double the sample size, or is the relationship more complicated? If you think the relationship is more complicated, what form does it take?

ANSWER:

The more important insight relates to the form of this relationship. What you should have noticed is that doubling the sample size does not halve the standard error. In fact, doubling the sample size only changes the standard error by a factor of 1/√2, which is less than 1/2 (don't worry if you did not spot this).

The somewhat depressing conclusion from this investigation is that we have to increase the size of a sample by a factor of 4 to halve the uncertainty associated with an estimate of a population parameter. This result isn't a peculiarity of the morph frequency example; it is very general.

### How big does a bootstrap sample need to be?

Use the R code above to investigate how large a bootstrap sample needs to be to reliably estimate the standard error. Start with a bootstrap sample size of 10, and gradually increase it, calculating the standard error each time you do so. How many bootstrap samples are needed to reliably estimate the SE to one decimal place?

## Week 3: *t*-tests

You should work through the exercises step-by-step, following the instructions carefully. At various points we will interupt the flow of instructions with a question. Make a note of your answer so that you can complete the associated MOLE quiz, which is called 't-tests 1'.   

### Eagle owls and Norway rats

```{r, eval = FALSE, echo = FALSE}
ratskulls <- read.csv("./data_csv/RATSKULL.CSV")
ggplot(ratskulls, aes(x = Weight)) + 
  geom_dotplot() + facet_wrap(~Season, ncol = 1)
t.test(Weight ~ Season, data = ratskulls)
```

A dataset containing information about the sizes of Norway rat skulls in the pellets of Scandinavian eagle-owls is available in the RATSKULL.CSV file (you may have come across this before). The data comprise a column of rat skull sizes (measured in grams) and a column of codes indicating the season when a particular skull sample was taken. These data were collected in order to evaluate whether there is a difference between sizes of rats eaten in summer and winter. That is, we want to know if there is a statistically significant difference between the mean rat skull sizes in the winter and summer samples.

Download the RATSKULL.CSV file from MOLE and **place it in your working directory** (this is the location you set at the beginning of this practical). Read the data in RATSKULL.CSV into an R data frame, remembering to assign the data frame a name. Ask a demonstrator to remind you how to do this if you have forgotten, or look over the instructions for reading in the MORPH_WEIGHTS.CSV file from the last practical.

As always, we should always start by *looking at the data* — both visually and in terms of its descriptive statistics:

**Inspection.** Use the `View` function and `dplyr` function `glimpse` to visually inspect the raw data. What are the names given to rat skull size variable and the season indicator variable? What values does the season indicator variable take?

**Descriptive statistics.** Use the appropriate `dplyr` functions (`group_by` and `summarise`) to calculate the sample size, sample mean and standard deviation of each sample. HINT: you will need to use the `mean`, `sd` and `length` functions to help you do this.

**Graphs.** Use `ggplot2` to construct a pair of dot plots, one above the other, to summarise the winter and summer skull size distributions. HINT: you will need to use `geom_dotplot` and the `facet_wrap` functions to do this; look over the plant morph example from the beginning of this practical to see how to use these.

Using the dot plots, and the descriptive statistics, conduct an informal evaluation of the assumptions of the t-test. You should re-read the relevant section above if you can't remember what these are.

```{block, type='do-something'}
**MOLE question**

Do you feel the data conform acceptably to the assumptions? If not, explain why.
```

Let's carry on, assuming that we are confident that it is OK to use a two sample t-test to compare the sample means. Use the R `t.test` function to carry out this evaluation now.

```{block, type='do-something'}
**MOLE question**

Write a concise but complete conclusion summarizing the results of the test.

Is this what you expected from looking at the distributions of data in the two samples?

Suggest two possible biological reasons for the result you observe.
```

### Fungal infection in French beans

```{r, echo=FALSE, eval=FALSE}
sandy <- c(2.3, 2.4, 2.5, 2.6, 2.8, 2.7, 3.1, 2.3, 2.5)
clay  <- c(2.3, 2.5, 2.8, 3.2, 2.9, 3.1, 3.2)
t.test(sandy, clay)
beans <- data.frame(glucosamine = c(sandy, clay),
                    soil = rep(c("sandy","clay"), c(length(sandy), length(clay))))
write.csv(beans, file = "./course-data/FRENCH_BEANS.CSV", row.names = FALSE)
```

A plant pathologist noticed that fungal infection in roots of French beans (*Phaseolus vulgaris*) was rather variable among crops and hypothesized that infection might be affected by the soil type: in particular whether the beans were grown on clayey or sandy soils. Root samples were taken from beans growing in each soil type and fungal infection was measured indirectly by measuring the amount of glucosamine in the roots. Glucosamine is a fungal sugar which is polymerised into chitin which forms the cell walls of most fungi.

The glucosamine concentrations ($\mu$g g$^{-1}$ root dry weight) recorded from the samples were:

  ------------ ----- ----- ----- ----- ----- ----- ----- ----- -----
  Sandy soil    2.3   2.4   2.5   2.6   2.8   2.7   3.1   2.3   2.5
  Clay soil     2.3   2.5   2.8   3.2   2.9   3.1   3.2        
  ------------ ----- ----- ----- ----- ----- ----- ----- ----- -----

Download the FRENCH_BEANS.CSV file from MOLE and place it in your working directory. Read the data into an R data frame (remember to give this a name!), inspect the data, generate some summary statistics (means and SDs) and then plot the data, just as you did in the last exercise. This should be quick to achieve--just copy and paste the code you produced, and edit this where required.

Use a t-test to decide whether there is a significant difference between the amount of infection of bean roots in the two different soils.

```{block, type='do-something'}
**MOLE question**

Make a note of the results:

Mean for plants on clay soil = ?

Mean for plants on sandy soil = ?

t = ?

d.f. = ? 

p = ? 
```

```{block, type='do-something'}
**MOLE question**

Write a statement of the result of the test suitable for inclusion in the results section of the plant pathologist's report.
```

### Sheep, grass and nature reserves

The management committee of a nature reserve wants to manage some large grassland areas of the reserve using low density sheep grazing to prevent the grass becoming too long and making the habitat unsuitable for some of the low-growing herbaceous plants for which the reserve is important. Before implementing the plan they conduct a pilot experiment using some fenced plots on the reserve, to test whether low density sheep grazing affects various species of plants.

One problem is that the area is very variable - some parts are wetter than others, and the plants of interest are not particularly evenly distributed. There is also a limit to the number of plots (and sheep) they can use in the experiment. In order to make the maximum use of the resources and, take some account of the variability in the habitat the experiment is set up by randomly placing eight fenced plots around the reserve, with each plot being divided in half by a fence down the middle. Sheep are introduced to one half of each plot (the half being randomly selected in each case), and allowed to graze for the appropriate period of the year. The other half is left ungrazed.

```{block, type='do-something'}
**MOLE question**

Why is this a better design than just having separate grazed and ungrazed plots positioned at random?
```

Over the next 2 years, the abundances of various plants in the in the plots are surveyed.

The data below give the numbers of gentians from each of the eight half-plots with sheep, and the corresponding ungrazed halves after one year of the experiment. 

  ----------- -------- -------- -------- -------- -------- -------- -------- --------
  Treatment                                                                  
               Plot 1   Plot 2   Plot 3   Plot 4   Plot 5   Plot 6   Plot 7   Plot 8
  Grazed         27       1        16       8        10       19       30       9
  Ungrazed       14       6        17       5        0        11       21       6
  ----------- -------- -------- -------- -------- -------- -------- -------- --------
  
These data are stored in GENTIANS_GRAZING.CSV. Test whether there is any evidence for an effect of sheep grazing on the numbers of gentians.

```{block, type='do-something'}
**MOLE question**

What is your conclusion?
```

```{block, type='do-something'}
**MOLE question**

What other comparison would it be useful to be able to make in order to reach a satisfactory conclusion about the effects of grazing?  What test would you do for this?
```

## Regression

### Partridges and hedgrows {#exercise}

```{block, type='do-something'}
Make a note of your answers so that you can complete the associated MOLE quiz, which is called 'regression 1'.   
```

```{r, echo = FALSE, eval = FALSE}
partridge <-read.csv(file = "./course-data/PARTRIDG.CSV")
ggplot(partridge, aes(x = Hedgerow, y = Partridge)) + geom_point()
partridge.model <- lm(Partridge ~ Hedgerow, data = partridge)
anova(partridge.model)
summary(partridge.model)
plot(partridge.model)
```

Hedgerows are the main nesting habitat of the grey partridge (*Perdix perdix*). A survey was carried out to establish whether the abundance of hedgerows in agricultural land had an effect on the abundance of grey partridge. From an area of agricultural land covering several farms, twelve plots were selected which had land uses as similar as possible but differed, as evident from preliminary inspection, in the density of hedgerows (km hedgerow per km^2^). Plots were deliberately selected to cover a wide range of hedgerow densities. The total hedgerow lengths, and exact plot areas, were measured by use of large scale maps. The density of partridges was established by visiting all fields in a study plot once immediately after dawn and once just before dusk, when partridges are feeding and therefore most likely to be seen. Counts of birds observed were made on each visit and the dawn and dusk data were averaged to give a value for partridge abundance for each study plot.

The data are stored in a CSV file PARTRIDG.CSV. The density of hedgerows (km per km^2^) is in the `Hedgerow` variable and the density of partridges (no. per km) is in the `Partridge` variable. Read in the data and take a look at it using the `View` function.

```{block, type='do-something'}
**MOLE question**

Which way round should the variables be?

-   Independent ($x$):

-   Dependent ($y$):
```

Make a scatter plot to allow you to evaluate the assumptions.

```{block, type='do-something'}
**MOLE question**

1.  If there is a relationship does it look linear? 

2.  Ratio or interval data? 

3.  Independent variable $y$ likely to be normally distributed for each $x$?

4.  Variance increases or decreases markedly with increasing $x$?

5.  Errors in $x$ likely to be small compared to those in $y$?
```

If everything is OK, or roughly so, then carry out a regression on the relationship between hedgerow density and partridge density.

```{block, type='do-something'}
**MOLE question**

Summarise the results of your analysis in words.
```

Finish up by preparing a figure that summarises the data and the best fit line estimated from the regression.



<!--chapter:end:910_exercises.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Tips and FAQs {#tips-faqs}

## Common error messages

If you forget to do this, you will see a lot of errors along the lines of "Error: could not find function..." when you try to use `dplyr` or `ggplot`.

## Fixing erros 

If you find that you are running into errors when you work on a particular exercise, go back to the beginning of the R code that addresses the exercise, and run it one chunk at a time, paying close attention to the output at the Console. You need to find out where the error first happens in order to fix it.

Use the demonstrators for help! If you are stuck, or not really sure what an exercise is asking you to do, ask us. That's what we are here for. There is a lot of expertise in the room, and no one is expecting you to be able to do everything in a practical without a little help now and again.

## Staying organised

**Make sure you add copious comments to your R script**

As well as using comments to remind you what you are doing, use comments such as `## t-test exercise ##` to delineate different parts of your script. Trust us, if you don't do this, you will have no idea what a script was meant to be doing when you return to it several weeks/months/years later.

**Use white space to help organise your R script**

R ignores blank lines, tabs and newlines so we can use these to make a script easier *for us* to read. It is really hard to read R code when it is all bunched up together. Remember that you can split a single R expression over multiple lines. If you're not sure how to do this look at the examples in the book---we do this a lot when using `dplyr` and `ggplot2`.

We generally put each part of a `dplyr` 'pipeline' (each seperated by `%>%`) onto a new line, e.g

```{r, eval=FALSE}
summarised_data <- 
  my_great_data %>% 
  group_by(x_variable) %>% 
  summarise(mean = mean(y_variable), samp_size = n())
```

We generally put each part of a `ggplot2` plot (each seperated by `+`) onto a new line, e.g.

```{r, eval=FALSE}
ggplot(my_great_data, aes(y = y_variable)) +
  geom_histogram(fill = "grey") +
  facet_wrap(~ x_variable)
```

**Clean up failed experiments**

Learning 'by doing' is much better than learning by just reading or listening to lectures. That's why we encourage you to work through the examples in the book and where possible, to experiment with your code to learn how it works. However, make sure that you remove failed 'experiments' and duplicate code. By the end of a session you want to have a well-organised, functioning R script. You should be able to highlight the whole thing, submit it to the console, and watch it run without any errors occuring.


<!--chapter:end:920_tips_faq.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Templates {#intro}

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].


<!--chapter:end:980_templates.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:999_references.Rmd-->

