--- 
title: "APS 240: Data Analysis and Statistics with R"
author: "Dylan Z. Childs"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    includes:
      in_header: extras.css
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: dzchilds/stats-for-bio
description: "Course book for Data Analysis and Statistics with R (APS 240) in the Department of Animal and Plant Sciences, University of Sheffield "
---
```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```

# Course information and overview

This is the online course book for the __Data Analysis and Statistics with R__ ([APS 240](https://www.shef.ac.uk/aps/currentug/level2/aps240)) module. You should be able to view this website in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained---it contains all the material you are expected to learn this year.

[Dylan Childs](https://www.shef.ac.uk/aps/staff-and-students/acadstaff/childs) is the course co-coordinator. Please [email him](mailto:d.childs@sheffield.ac.uk?Subject=APS%20133%20general%20query) if you have have any general queries about the course. [Andrew Beckerman](https://www.shef.ac.uk/aps/staff-and-students/acadstaff/beckerman) is the second course instructor. The Teaching Assistants this year are Ross Booton, Rob Goodsell, Bethan Hindle, and Simon Mills.

## Why do a data analysis course? {#why-data-analysis}

To do science yourself, or to understand the science other people do, you need some understanding of the principles of experimental design, data collection, data presentation and data analysis. That doesn’t mean becoming a maths wizard, or a computer genius, it means knowing how to take sensible decisions about designing studies and collecting data, and then being able to interpret those data correctly. Sometimes the methods required are extremely simple, sometimes more complex. You aren’t expected to get to grips with all of them, but what we hope to do in the course is to give you a good practical grasp of the core techniques that are widely used in biology and environmental sciences. You should then be equipped to use these techniques intelligently and, equally importantly, know when they are not appropriate, and when you need to seek help to find the correct way to design or analyse your study.

You should, with some work on your part, acquire a set of skills which you will use various stages throughout the remainder of your course, in practicals, field courses and in your project work. These same skills will almost certainly also be useful after your degree, whether doing biology, or something completely different. We live in a world that is increasingly flooded with data, and people who know how to make sense of this are in high demand. The R statistical programming environment underpins much of this analysis, both in academic and commercial settings. Learning the basic principles of data analysis, implemented in R, can only improve your employment prospects and opportunities for further study following your undergraduate training. 

## Course overview {#overview}

### Aims {#aims}

This course has two main, and equal, aims. The first is to provide a basic training in the use of statistical methods and computers (R!) to analyse biological data. The second is to introduce some of the principles of experimental design, sampling, data interpretation, graphical presentation and scientific writing relevant to the biological and environmental sciences.

### Objectives {#objectives}

By the end of the course you should be familiar with the principles and use of a range of basic statistical techniques and be able to use the R programming language to carry out appropriate analyses of different types of biological data, and make sensible interpretation of the results. You should be able to relate the ways in which data are collected (by different designs of sampling or experiment) to the types of statistical methods that can be used to analyse those data. In combination with the skills you developed in [APS 135](https://www.shef.ac.uk/aps/currentug/level1/aps135), you should be able to decide on appropriate ways of presenting sets of data graphically and be able to produce good quality scientific figures and incorporate these, along with statistical results, into a formal report, using the correct format and style.

### Assumed background {#assumed-background}

You are assumed to be familiar with the use of personal computers on the University network, and with the use of R for data input, manipulation and plotting introduced in [APS 135](https://www.shef.ac.uk/aps/currentug/level1/aps135). If you are unsure about these basic methods, then you will need to revise the material covered in the Level 1 IT practicals.

**Don't worry if you are one of the Environmental Sciences students joining us from Geography. We will provide extra sessions at the beginning of the course to help you learn the fundamentals of R needed in this course.**

### Methods {#methods}

The course is designed as a ‘self-teaching’ module, allowing you flexibility in the rate at which you work through the material each week. The course runs over semester 1 (weeks 1-12). The first eight weeks consists of one 1.5h hour supported IT practical per week, one self-study practical per week, and one extra reading per week. The remaining four weeks are devoted to the short data analysis project, general revision, and the final exam.

The **supported IT practicals** take place in the APS IT rooms, Perak IT labs and B56. In each of these sessions there will be help available from demonstrators and staff, but it is up to you to work through the practical. You may use either a University computer or your own laptop to complete these practicals (but see the warning below). You are expected to make use of the computing facilities around the University or your own laptop to complete practical work if you do not manage to finish them during a timetabled session.

You will work on the **self-study practicals** in your own time, using either the University computing facilities or your own laptop (again, see the warning below). These practicals are designed to consolidate and extend the work covered in the supported IT practicals. You are free---encouraged, in fact---to work through these in groups. Demonstrators and staff can also answer any questions you have about the self-study practicals during the timetabled sessions.

You are welcome to use your own computer to complete the supported and self-study IT practicals. However, keep in mind that the University computers are the only "officially supported" platform. If you run into a problem using your own computer, the demonstrators and staff will try to help resolve these if there is time. Unfortunately, if these prove to be intractable, you will have to use the University computing facilities. It is not fair on other students for teaching staff to spend valuable contact time trying to solve installation/setup problems.

There is also one **extra reading** to complete in most weeks. These will usually be relatively short, focusing on ideas and concepts, rather then using R. They provide background information, or more detailed discussion, relating to the topics you are covering at that point in the course. These do not usually contain computer-based exercises. However, they are an integral part of the course and you should take time to work through them and ensure you understand the ideas within.

#### Non-assessed material

Although every practical session and extra reading topic is important---in the sense that each contains material that will help you analyse the data you collect in field courses, practicals and projects---we want to avoid creating too much of an assessment burden in this course. To this end, the material in some of the self-study practicals and extra readings will not be formally assessed. These parts of the course are flagged as "Not Assessed" on the [course schedule](./{{site.course-schedule}}).

### What is required of you? {#what-is-required-of-you}

A willingness to learn and to take responsibility for your learning! Data analysis is not the easiest subject in the world, but neither is is the most difficult, and what you learn in this course will form the basis for much of what you do in field course, practical and project work that follows. 

The minimum requirement for the course is that you:

-   attend your designated practical session each week (please ensure that you arrive on time and sign the register, or you will be recorded as absent)

-   complete the self-study practicals and keep on top of the extra reading

-   complete the teaching material for each week before the next practical class

-   check through your answers to questions in the teaching material using the MOLE self-test exercises provided each week

How you work through the practical material is fairly flexible, but in each session you should aim to complete most, if not all, of the tutorial in that practical session. If you don’t then you should finish off the tutorial in your own time before the next practical. Remember, the self-study practicals are designed to consolidate and extend the work covered in the supported IT practicals. These are not optional and each one needs to be completed in the week where it is listed.

If you have problems with any of the work, then staff will help you during the practical sessions, even if it is not the topic designated for that session, so if you need to catch up, or get help with something you didn’t understand there will be opportunities to do so. You can also welcome to ask questions about the self-study practicals and the extra reading during the practical sessions.

A word of advice: Don’t let the flexibility of the course tempt you into letting a backlog of work build up. This will compromise your ability to do the assessed work when it is set.

Remember, **the total study time nominally associated with a 10 credit module is 100 hours**. This means there is an expectation that you will spend significant time outside the timetabled practical classes working on this module. You should aim to spend about 6 hours each week completing the IT practicals, working through the self-study IT sessions, reading the extra material, and later, working on the assessed project. This leaves you about 30 hours to revise for the formal exam.

### Assessment {#assessment}

Assessment of the course will have two components. The first is a short data analysis project, the second an open book, assessed, practical class (well, yes, OK, it’s an exam!). Both will take place towards the end of the course, and full details will be given as the course progresses.

## How to use the teaching material {#how-to-use-the-teaching-material}

### The online course book {#printed-material}

All of the teaching material will be made available through a single online course book (this website). You should bookmark this now if you haven't already done so. There are a couple of good reasons for delivering the course material this way: 

-   **Practicality**: Many of the exercises that you will complete require you to build a previous example in the course material. Copying the relevant R code from the course website and pasting it into your script is much more efficient, and less error-prone, than copying by eye from a printed page. A website also allows us to cross-reference topics and link to outside sources of reading.

-   **Permanence**: Experience suggests that many of you will want to refer to the material in this course after you graduate. However, bits of paper are easy to lose, and because the R landscape is always changing, some elements of the course may be less relevant in a few years time. By putting everything on a website, we can ensure that you will always be able to access a familiar, but up-to-date data analysis course.

There are three different sorts of page on the course website, covering the supported IT practicals, self-study IT practicals, and the extra readings. These are ordered such that, together, they form a complete introduction to data analysis text book. Each page of the website is effectively a chapter of this book. You should work through these in the appropriate order, given on the [course schedule](./{{site.course-schedule}}). In most weeks we start with a supported IT practical, which is then followed by a self-study practical and an extra reading. The only exception is week 1, when the supported IT practical occurs place in between two readings. 

In addition, we will provide a template R script for each practical session, which is linked to at the beginning of the relevant page. These are designed to help you organise the R script that you will need to prepare in each practical.

### Printed material: what you get and what you do with it {#printed-material}

There is a small amount of printed material in this course:

-   **Cheat sheets**: We will supply you with copies of the `dplyr` and `ggplot2` [cheat sheets](https://www.rstudio.com/resources/cheatsheets/) produced by the people who build RStudio . It may help you to refer to these when you need use either the `dplyr` or `ggplot2` packages in a practical.

-   **Assessment information**: Although much of the assessment will be done on the computer, any information relating to the assessments will be produced in printed form on exciting pink paper, so you can’t miss it!

### How to make best use of the teaching material {#how-to-make-best-use-of-it}

DO:

-   Take each page from the start and work through it, following the instructions, but also **think about what you are doing!** Work at your own pace; you are not being assessed on whether you can do the practical in a particular time.

-   Ask teaching staff in the practicals if there are things that you don’t follow, or when things don’t seem to come out the way they should---that’s what we’re there for!

-   Be prepared to experiment to solve any problems that you encounter. You can't break your R or RStudio by generating errors (well, one can, but it's quite hard to do). When you run into a problem, go back to the line of code that generated the first error and try making a change.

-   Collaborate! If you are not sure you understand something feel free to discuss it with a friend — more often than not this is exactly how scientists resolve and clarify problems in their experimental design and analysis.

-   Complete each week’s work before the next week’s session. You may be able to complete some sessions quite quickly, others may take more time and require more work on your own outside the timetabled periods.

DON’T:

-   Just copy what someone else tells you to do without understanding why you are doing it. You need to understand it for yourself (and you’ll be on your own in the exam).

-   Skip practicals or extra reading and get behind schedule — there is too much material to assimilate all at once when you get to the assessments. Like all skills it is something you have practice.

### Conventions used in the course material {#conventions}

The teaching material, as far as possible, uses a standard set of conventions to differentiate between various sorts of information, action and instruction:

#### Text, instructions, and explanations

Normal text, instructions, explanations etc. are written in the same type as this document, we will tend to use bold for emphasis and italics to highlight specific technical terms when they are first introduced (italics will also crop up with Latin names from time to time, but this is unlikely to produce too much confusion!)

When we want to say something REALLY IMPORTANT---e.g., we are summarising the key learning outcomes or giving you a set of instructions---we place the text inside a grey filled box, like this one: 

<div class="well">
Here is some important text telling you to do something or remember something important.
</div>

Don't ignore these.

We use block quotations to indicate an example of how a particular statistical result should be presented when you write it in a report: e.g.

> The mean lengths of male and female locusts differed significantly (t=4.04, df=15, p=0.001), with males being significantly larger.

At various points in the text you will come across text in different coloured boxes. These follow the same conventions introduced in the R component of APS 135:

```{block, type='exercise-box'}
#### Green boxes
These contain one or more __exercises__. You should complete these exercises as you work through each IT practical and self-study session. They serve to introduce new ideas or consolidate one that has already been introduced.
```

```{block, type='advanced-box'}
#### Blue boxes
These contain __supplementary information__ or an __advanced topic__. They aim to offer a not-too-technical discussion of how or why something works the way it does. These are things that it may be useful to know, or at least know about. These are not generally meant to be read as part of the main text.
```

```{block, type='warning-box'}
#### Yellow boxes
These contain a __warning__ or flag a common __gotcha__ that may trip you up. These boxes aim to highlight potential pitfalls and show you how to avoid them. These are not generally meant to be read as part of the main text. You will avoid mistakes now and in the future if pay close attention to these.
```

#### R code, files and RStudio

`This typeface` is used to distinguish R code within a sentence of text: e.g. "We use the `summary` function to obtain information about an object produced by the `lm` function."

A sequence of selections from an RStudio menu is indicated as follows: e.g. **File ▶ New File ▶ R Script**

File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV.

### Feedback {#feedback}

There are a number of ways in which you can obtain feedback on how well you understand the course material

#### Self-assessment questions:

At various points in the course material there are questions for you to answer. When you reach one of these, you should be in a position to answer the question —- so make a note of the answer! When you’ve completed the session, you can then check your answers using the ‘self-test’ for that particular session on MOLE. You will see if you have the correct answer and in some cases you will also get some additional explanation as to why that answer is right (or wrong!).

#### Each other:

Discussing what you are doing with someone as you go along, or working through a problem with someone else, can help clarify your understanding. Please bear in mind, however, that you learn little or nothing by simply copying information from someone else, and when it comes to the assessed project, it must be your own work.

#### Staff:

In the practicals you will have opportunities to ask questions and discuss what you are doing with staff and teaching assistants. They are not just there to help you with the practical. You should use them to help you work through any problems you have with the course material, both "conceptual" and "practical". There will also be an opportunity to have topics you raise discussed in later practicals.

### Help sessions {#help}

We will run an open 'help' session every Wednesday from 12-2.00pm, in the B56 IT Room in APS. An instructor will be on hand during this period to answer specific questions about the course material. This room holds about 40 students, so please only attend if you require one-to-one assistance, i.e, don't just use this session to complete unfinished practicals (unless you are stuck).

### Overall… {#overall}

We hope that the material is clear and easy to use, and that you find the course useful, or even enjoy it!

In a text of this size, which is continually being improved and updated, errors do creep in; if you find something you think is wrong please tell us. If it’s not wrong we will be happy to explain why, and if it is then you will save yourself and others a lot of confusion. Similarly, if you have any comments or suggestions for improving the teaching materials please let us know.

## Health and safety using display screen equipment {#health-and-safety}

Although using a computer may not seem like a particularly risky activity you should be aware that you can suffer ill effects if you work at a computer for long periods without observing a few sensible precautions. The standard guidelines are as follows:

-   Make sure that your equipment is properly adjusted:

    -   ensure that your lower back is well supported by adjusting the
        seat back height

    -   adjust your chair seat height so that your forearms are level
        when using the keyboard

    -   make sure that the front edge of the keyboard is at least 8-10
        cm away from the edge of the desk

    -   if you are using a mouse, have it far enough away from the edge
        of the desk so that your wrist is supported whilst you use it.
        If you can learn to use the mouse with either hand then this can
        help avoid strains

-   Do not have your screen positioned in such a way that there is glare
    or reflections from the windows or room lights on the screen.

-   Maintain good posture.

-   Take regular breaks away from the computer. It is recommended that
    you take about 10 minutes break every hour.

Most Departments will have a Display Screen Trainer or Advisor, who can offer specific advice if you are using a display screen for a substantial amount of time, or if you experience, or anticipate, specific problems.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Syllabus and assessment

## Principles (I)

* Recognise the correct definition of a statistical population, a population parameter, a sample, and a point estimate (also called a statistic).

* Explain what sampling error is (in non-technical terms), and explain why it is important to quantify this.

* Recognise the difference between the distribution of a sample, and the sampling distribution of an estimate (or statistic) derived from that sample.

* Explain, in non-technical terms, the difference between the standard deviation (a propery of a sample) and the standard error (a property of a sampling distribution).

* Calculate the standard error of a sample mean when the population distribution of the focal variable follows a normal distribution.

You are **not** expected to be able to explain or use the bootstrap.

## Principles (II)



<!--chapter:end:000_syllabus.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Prerequisites {-} 

# Programming prerequisites

Before you work through the rest of this course, it will be useful to revise some of the basic R skills you developed last year:

<div class="well">
Don't worry if you are an Environmental Sciences student joining us from Geography. We will run extra catch-up sessions for the first few weeks to help you learn the basic R skills needed to do this course, though we won't cover everything the APS students did last year. The material in this section won't make much sense at the moment, but it will after the first few weeks (we promise).
</div>

## Starting an R session

[[COMPLETE ME]]

## Using packages

R packages extend the basic functionality of R so that you can do more with it. A package bundles together R code, data, and documentation in a way that is easy to use and share with other users. Last year we learned how to use some of the functions provided by the `dplyr` package (for data manipulation) and the `ggplot2` package (for making plots). We are going to use the `dplyr` and `ggplot2` packages again this year, so you need to understand R's package system in order to access these. You can revise how to use the package system in the [packages](http://dzchilds.github.io/aps-data-analysis-L1/help-packages.html) topic. It isn't difficult to use, and we will obviously help you if you run into difficulties. 

Installing a package is done via the `install.packages` function, e.g.
```{r, eval=FALSE}
install.packages("dplyr")
```
Loading and attaching the package a package happens via the `library` function, e.g.
```{r, eval=FALSE}
library("dplyr")
```
The key point---which seems to cause endless confusion---is that installing a package, and then loading and attaching the package, are different activities. You only have to install it once onto your computer, but you have to load a package every time you want to use it in a new R session (i.e. every time you start up RStudio). Your scripts should 

## Reading data into R

Last year we made extensive use of several datasets that reside inside various R packages. This was useful because it meant we could use the data without first reading it into R, meaning that we could concentrate on developing your R skills rather than fixing data input errors. We don't have the luxury of doing this when we work with our own data, and so this year, we will adopt more realistic practises. Whenever you need to work with a dataset, you will have to first download it (from MOLE), and then read it into R. Each dataset is stored as a Comma Separated Value ('CSV') text file, and so you will need to use the `read.csv` function to read it in. You can revise how all of this works in the relevant section of the [data frames]({{site.baseurl-L1}}/data-frames.html#access-data) topic. 

## Data frames

When you read data into R using a function like `read.csv`, it places that data into a data frame. The data frame is the most important type of object in R. Remember, a data frame is table-like object that collects together different variables, storing each of them as a named column. We can access the data inside a data frame by referring to particular columns and rows. You can revise how to work with data frames in the [data frames]({{site.baseurl-L1}}/data-frames.html#access-data) topic. The main things to remind yourself about are the `View` function for inspecting a data frame, and the `$` operator for accessing a single column of a data frame. You might also want to revise the `tbl_df` and the `glimpse` functions from the `dplyr` package (but this is not critical).

## Anything else? 

We will use functions from the `dplyr` package from time-to-time to manipulate data, and we will use the `ggplot2` package to make plots of our data and summarise statistical models. However, we will remind you which functions you need to use to solve a particular problem as the course unfolds, so there is no need to revise all of this material now. If you want to be extra-prepared, our advice is to work through the [quick introduction to ggplot2]({{site.baseurl-L1}}/ggplot2-intro.html#quick) to remind yourself of the logic of plotting with `ggplot2`.

<!--chapter:end:010_coding_prerequisites.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Data, variables and distributions

> *The truth is the science of nature has been already too long made
> only a work of the Brain and the Fancy. It is now high time that it
> should return to the plainness and soundness of Observations on
> material and obvious things.*
>
> Robert Hooke (1665)
>
> *The plural of anecdote is not data.*
>
> Roger Brinner

## “Observations on material and obvious things”

As Hooke’s observation suggests, science cannot proceed on theory alone. The information we gather about a system both stimulates questions and ideas about it and, in turn, can also allow us to test these ideas. In fact the idea of measuring and counting things is so familiar to us that it is easy to start a project without giving much thought to something as apparently mundane as the nature, quantity and resolution of the data we intend to collect. It is worth considering, however, as the nature, quantity and quality of the data in a study determine both the types of analyses that can be carried out, and the confidence we can have in any conclusions that are drawn. We will spend quite a lot of time considering the statistical tools that can help you extract information from your data, but no statitical wizardry can extract information that isn’t somewhere in the data to begin with.

So what is there to say about data? The first point to note is that, properly, the word data is the plural of datum (a single, often numerical, piece of information) ...so we should say “the data are...” not “the data is...”. However, the use of the word in the singular is becoming widespread, and you will commonly hear it used in this way. [Grammar Nazis](http://www.urbandictionary.com/define.php?term=Grammar%20Nazi) really don't like this though, so it's worth knowing what the "correct" subject-verb agreement looks like if you want to avoid incurring their wrath.  

The second point is that there are many different sorts of data. Examples include spatial maps of the occurance of a particular species or environmental variable, DNA sequences or even the whole genomes of individuals, and networks of feeding relationships among species (i.e. food webs). These kinds of data can be very challenging to analyse. In this course we are concerned with relatively simple kinds of data. 

When we collect data it is typically organised as a set of one or more related statistical **variables**. Remember, statisticians use the word 'variable' to refer to any characteristic that can be measured, counted or experimentally controlled. Collectively, a set of related variables are referred to as a **data set** (or just 'the data', if we are feeling lazy). So in the spatial map example, a minimal data set might comprise two variables containing the x and y position of sample locations, along with a third variable denoting the presence / absence of a species.

```{block, type='advanced-box'}
#### Data and variables in R
Remember what you learnt last year about data frames and vectors? When using R, we often (not always!) store each data set as a **data frame**. Each column in the data frame is one of R's **vectors** (numeric, character, etc). These columns correspond to a statistical variables in our data set. This simple connection between abstract statistical concepts and the concrete objects in R is not coincidence -- R was designed first and foremost to analyse data.
```

## Revision: Types of variable {#var-types}

Again, because we handle data of one sort or another so frequently, we often don’t stop and think about exactly what kind of data we are using. Most of the time that doesn’t cause too much of a problem. However, when you come to design your own studies, and analyse your own data, it can be very important to understand what sort of data you need, or have, as it can affect what information you can extract from it.

Last year we learned that the variables that comprise a data set can be classified as being either __numeric__ or __categorical__: categorical variables have values that describe a characteristic of an observation, like 'what type' or 'which category'; numeric variables have values that describe a measurable quantity as a number, like 'how many' or 'how much'. Categorical variables can be further characterised according to whether or not they have a natural order (__nominal__ vs. __ordinal__ variables), and numeric variables can be further characterised according to the type of scale they are meaured on (__interval__ vs. __ratio__ scales).

Let's review these classifications.

### Nominal (categorical) variables

Nominal variables arise where observations are recorded as categories which have no natural ordering relative to each other. For example:

---------------------  --------- ------------------
  **Marital status**    **Sex**   **Colour morph**
        Single           Male           Red
        Married          Female         Yellow
        Widowed                         Black
        Divorced                 
---------------------  --------- ------------------  

Data of this type are common in surveys where, for example, a record is made of the species found at each site.

### Ordinal (categorical) data

Ordinal variables occur where observations can be assigned some meaningful order, but where the exact numerical relationship between items in the order are not necessarily fixed, the same, or even known. For example If you are studying the behaviour of an animal when it meets another individual it may not be possible to obtain quantitative data about these interactions, but you can score the behaviours you see in order of
aggressiveness:

  -------------------- -----------
  **Behaviour**         **Score**
  initiates attack          3
  aggressive display        2
  ignores                   1
  retreats                  0
  -------------------- -----------

Rank orderings are also ordinal data. For example the order in which runners finish a race (1st, 2nd, 3rd, etc..) is a rank ordering it doesn’t tell us whether it was a close finish or not, but still conveys important information about the result.

In both situations you can say something about the relationships between categories: in the first example, the larger the score the more aggressive the response; in the second example the greater the rank the slower the runner. However, you can’t say that the gap between the first runner and the second was the same as between the second and third (even though 2-1=3-2) and you can’t say that a score of 2 is twice as aggressive as a score of 1.

```{block, type='warning-box'}
#### How should you code different categories?

We always have to define some kind of coding scheme to represent the different categories of a nominal/ordinal variables. It was once common practise to assign numbers to different categories (e.g. Female=1, Male=2) for handling data in computerised form. This method was sensible in the early days of computer-based data analysis because it allowed data to be stored efficiently---numbers take up less space in memory than words. However, this efficiency argument is much less relevant on a modern computer with many Gb of memory. There *are* good reasons to avoid numeric coding schemes though:

-   Numeric coding makes it harder to understand your raw data and to interpret the output of a statistical analysis of those data, because you have to remember which number is associated with each category. This is particularly problematic when a variable has many categories.

-   Numeric codes are arbitrary and should not be used as numbers for mathematical operations. For example, it is meaningless to say 2 ("male") is larger than 1 ("female"), or $2 + 1 = 3$. R has a special way of representing categorical variables (called 'factors') so it assumes that any variable containing numeric values is meant to treated as a number. 

So here's the warning: **always** use words (e.g., 'female' vs. 'male'), not numbers, to describe the different categories when you are preparing your data for analysis. You are much more likely to make a silly mistake when carrying out an analysis if you don't do this, because R will try to treat the offending categorical variable as a number.
```

### Interval scale (numeric) variables

Interval scale varaibles take values on a consistent numerical scale but where that scale starts at an arbitrary point.

Temperature on the Celsius scale is a good example of interval data. You can say that 60$^{\circ}$C is hotter than 50$^{\circ}$C. You can also say that the difference in temperature between 60$^{\circ}$C and 70$^{\circ}$C is the same as that between -20$^{\circ}$C and $-10^{\circ}$C. However you cannot say that 60$^{\circ}$C is twice as hot as 30$^{\circ}$C because temperature on the Celsius scale has an artificial zero value (the freezing point of water). This point becomes obvious when you consider that temperature can equally well be measured on the Fahrenheit scale (where the freezing point of water is 32 degrees). There is a temperature scale which has a true zero: the Kelvin scale. Zero K is absolute zero, where a substance actually has no thermal energy whatsoever. So temperature in degrees K would not be interval data.

You can add and subtract data measured on an interval scale but you cannot divide or multiply such data (and get a meaningful result). 

### Ratio scale (numeric) variables

Ratio scale variables have a true zero and known and consistent mathematical relationship between any points on the measurement scale. Temperture measurements in degrees K are on a ratio scale, i.e. it makes sense to say that 60 K is twice as hot as 30 K.

These are the variables we are most used to, because physical quantities are often measured on a ratio scale. For example, length, weight, or numbers of organisms are usually measured on a ratio scale. You can add, subtract, multiply and divide this sort of data and get meaningful results.

```{block, type='advanced-box'}
#### Continuous or discontinuous?
A common confusion with numeric data concerns whether the data are on continuous or discontinuous scales. Ratio data can be either. Many biological ratio data are discrete, and therefore discontinuous (i.e. only certain discrete values are possible in the original data).  Count data are the most obvious example, e.g. the number of eggs found in a nest, the number of plants recorded in a quadrat, or number of heartbeats counted in a minute; these can only comprise whole numbers, 'in between' values are not possible. However, the distinction between continuous and discontinuous data is often not clear cut -- even 'continuous' variables such as weight are made discontinuous in reality by the fact that our measuring apparatus is of limited resolution (i.e. a balance may weigh to the nearest 0.01 g). The fact that data may be discontinuous does not mean they are necessarily ordinal data.
```

### Which is best?

All types of data can be useful but it is important to be aware that not all types can be used with all statistical models. This is one very good reason for why it is worth having an idea of the statistical tools you intend to use when designing your study.

In general, ratio data is the data type best suited for statistical analysis. But biological systems often cannot be readily represented as ratio data, or the work involved in collecting good ratio data may be vastly greater than the resources allow, or the question we are interested in may not demand ratio data to achieve a perfectly satisfactory answer.

It is this last question that should really come first when thinking about a study. What sort of data do we need to answer the question we are interested in? If it is clear at the outset that data on a rank scale will not be sufficiently detailed to enable us to answer the question then we must either develop a better way of collecting the data, or abandon that approach altogether. If you know the data you are able to collect cannot address the question, then you would be better doing something else, so it is good to work that out in advance.

And an obvious, but important point: you can always convert measurements taken on a ratio scale to an interval scale, but you cannot do the reverse. Similarly, you can convert interval scale data to ordinal data, but you cannot do the reverse. In general, it is a good idea to avoid such conversions if you can, as they inevitably result in a loss of information.

## Distributions {#revision-distributions}

We touched on the idea of a distribution last year: in statistics, a distribution is a statement about the frequency with which different values of a variable are observed. 

The mean and variance of a population are both examples of population parameters. These describe the central tendancy ('which values are most common?') and the dispersion ('how variable are the values') of a distribution.


## Accuracy and precision {#accuracy-precision}

### What do they mean?

The two terms accuracy and precision are used more or less synonymously in everyday speech, but in scientific investigation they have quite distinct meanings.

**Accuracy** – how close a measurement is to the true value of whatever it is you are trying to measure.

**Precision** – how repeatable a measure is (also to how fine a resolution a measurement can be made), irrespective of whether it is close to the actual value.

If you are measuring an insect’s weight on an old and poorly maintained balance, which measures to the nearest 0.1 g, you might weigh the same insect several times and each time get a different weight — the balance is not very precise, though some of the measurements might happen be quite close to the real weight. By contrast you could be using a new electronic balance, weighing to the nearest 0.01g, but which has been incorrectly zeroed so that it is 0.2 g out from the true weight. Repeated weighing here might yield results that are identical, but all incorrect (i.e. not the true value) — the balance is precise, but the results are inaccurate.

The analogy often used is with shooting at a target:

** NEED TO SHOW THE "targets.png"" FILE HERE **

It is obviously important to know how accurate and how precise your data are. The ideal is situation in the top left target in the diagram, but in many circumstances high precision is not possible and it is usually preferable to make measurements of whose accuracy you can be reasonably confident (bottom left), than more precise measurements, whose accuracy may be suspect (top right). Taking an average of the values for the bottom left target would produce a value pretty close to the centre; taking an average for the top right target wouldn’t help your accuracy at all (though the repeatability of the values might well give you spurious confidence in the data).

It is also worth being aware that when you state results, you are making implicit statements of the precision of the measurement.

### Implied precision - significant figures

The number of significant figures you use suggests something about the precision of the result. A result quoted as 12.375 mm implies the measurement is more precise than one quoted as 12.4 mm. A value of 12.4 actually measured with the same precision as 12.735 should properly be written 12.400. When quoting results look at the original data to decide how many significant figures to use - generally the same number of significant figures will be appropriate.

If you are working with discrete data these considerations do not apply in quite the same way, e.g. precision of measurement is not an issue in recording the number of eggs in a nest. You use 4 not 4.0, but since 4 eggs implies 4.0 eggs you would be correct to quote average clutch size from several nests as 4.3 eggs. However, even with discrete data, if numbers are large then obviously precision is an issue again ... a figure of 300 000 ants in a nest is likely to imply a precision of plus or minus 50 000. A figure of 320987 ants implies a rather improbably precise measurement (nobody will believe you actually counted them all!).

### How precise should measurements be?

The appropriate precision to use when making measurements is largely common sense. It will depend on practicality (it may not be possible to weigh an elephant to the nearest 0.001g) and the use to which you wish to put the data (if you want to know whether the elephant will cause a 10 tonne bridge to collapse then the nearest tonne will be good enough, if you want to compare the mean sizes of male and female elephants then the nearest 100 kg may be sufficient, if you want to monitor the progress of a pregnant female elephant then the nearest 10 kg or less might be desirable).

As a rough guide aim, where possible, for a scale where the number of measurement steps is between 30 and 300. So for example, in a study of the variation in shell thickness of dogwhelks on a 300 m transect up a shore, it would be adequate to measure the position of each sampling point on the transect to the nearest metre, but shell thickness will almost certainly need to be measured to the nearest 0.1 mm.

### Error, bias and prejudice

Error is present in almost all biological data, but not all error is equally problematic. Usually the worst form of error is bias. Bias is a systematic lack of accuracy, i.e. the data are not just inaccurate, but all tend to deviate from the true measurements in the same direction (situations B and D in the ‘target’ analogy above). Thus there is an important distinction in statistics between the situation where the measurements differ from the true value at random and those where they differ systematically. Measurements lacking some precision, such as the situation illustrated in C, may still yield a reasonable estimate of the true value if the mean of a number of values is taken.

Avoiding bias in the collection of data is one of the most important skills in designing biological (or other) investigations. Some forms of bias are obvious, others more subtle and hard to spot. Some sources of bias in biology include:

-   *Non-random sampling*. Many sampling techniques are selective, and may result in biased information. For example pitfall trapping of arthropods will favour collection of the very active species, which encounter traps most frequently. Studying escape responses of an organism in the lab may be biased since the process of catching organsims to use in the study may have selected for those whose escape response is poorest.

-   *Conditioning of biological material*. Organisms kept under particular conditions, especially in a laboratory, for periods of time may become acclimatised to conditions unlike those they normally encounter, or if kept in a laboratory for many generations characteristics may change through natural selection. Such organisms may give a biased impression of the behaviour of the organism in natural conditions.

-   *Interference by the process of investigation*. Often the process of making a measurement itself distorts the characteristic being measured. For example it may be hard to measure the level of adrenalin in the blood of a small mammal, without affecting the adrenalin level in the process. Pitfall traps are often filled with a preservative, such as ethanol, but the ethanol attracts species of insect that normally feed on decaying fruit and use the fermentation products as a cue to find resources.

-   *Investigator bias*. Measurements can be strongly influenced by conscious or unconscious prejudice on the part of the investigator. We rarely undertake studies without some initial idea of what we are expecting, or we form ideas about the patterns we think we are seeing as the study progresses. This can introduce bias. For example, rounding up ’in between’ values in the samples you are expecting to have large values and rounding down where a smaller value is expected, or having another ’random’ throw of a quadrat when it doesn’t land in a ’typical’ bit of habitat.

The ways in which biases, conscious and unconscious, can affect our investigations are many, often subtle, and sometimes serious. Sutherland (1994) gives an illuminating and sometimes frightening catalogue of the ways in which biases affect our perception of the world and the
judgements we make about it.

The message is that the results you get from your investigation must always be judged and interpreted with respect to the nature of the data that were used to derive them – if the data are suspect, then the results will be suspect too.



<!--chapter:end:020_data_variables.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Statistical Concepts {-} 

# The scientific process

> *There is something fascinating about science. One gets such a
> wholesale return of conjecture out of a trifling investment of fact.*
>
> Mark Twain
>
> *To do science is to search for patterns, not simply to accumulate
> facts.*
>
> Robert MacArthur

## Stages in the scientific process {#stages}

Science is about asking, and answering, the right questions. Within this process a number of distinct stages usually occur: making observations, asking questions, formulating hypotheses, and testing predictions. Collectively these are the building blocks of what is known as the scientific method. Exactly how they fit together, and what the philosophical and practical limitations of different approaches are, have been the subject of much debate by philosophers of science over many years. We are not going really going to tackle those issues here, fascinating though they are, but instead try to extract a general working framework for the process of a typical scientific investigation.

### Observations {#observations}

**Observation** — *information, or impression, about events or objects*.

In general the questions we ask are not generated by pure abstract thought, but are a result of observations about the natural world. These may take the form of direct observations we make ourselves, patterns that crop up in data collected for other purposes, in non-specific surveys, and the previous work, or accumulated information, of other people.

So, while pottering around in a stream one day, you notice that the freshwater ‘shrimps’ (*Gammarus*) that abound in the stream seem to occur almost entirely under stones; you rarely seem to see them when you just watch a patch of open stream bed. Having made observations, it may be necessary to collect some more data to check that this phenomenon is not just a one-off event, or a false impression. Look under a few more
stones, watch the same species another day, or in another place, check the literature for similar observations by others.

Such observations of biological systems will lead almost automatically into asking questions.

### Questions {#questions}

**Question** — *what it is that you want to know; the scope of your investigation*.

e.g., Why does *Gammarus* spend most of its time under stones?

You should try and make your question reasonably focused - the overall aim of your study is to answer this question. The question of why the tropics are more diverse than the temperate regions is a vast topic - so
even though it is a valid (and fascinating) question, it may not be a good choice for a final year project or even a PhD!

The next stage is to formulate an hypothesis.

### Hypotheses {#hypotheses}

**Hypothesis** - *an explanation proposed to account for observed facts.*

In general, in biology, the important distinguishing feature of an hypothesis is that it specifies some biological process, or processes, which might account for the observations made. One question will often
generate more than one hypothesis:

*Gammarus* occur under stones because:

-   they need to shelter from the current

-   their food (leaf litter) gets trapped and accumulates under stones

-   they are subject to predation by visually hunting fish and need to remain out of sight

Formulating hypotheses requires more than just a restatement of the question - it usually embodies some mechanism (though in some cases this may not be fully understood) and it will often draw on additional information (e.g., the fact that *Gammarus* feed on dead leaves).

### Predictions {#predictions}

**Prediction** — *what you would expect to see if the hypothesis was true*.

Hypotheses are about proposing explanations, but they might not be directly testable; that is they may not tell you what data to collect, or what pattern to expect in the data. To be able to test an hypothesis you need to make some predictions from that hypothesis. These will be determined both by what you expect to see and what it is possible, or practical, to measure. A prediction is not simply a rephrasing of the hypothesis - it should more or less give you a statement of the experiment to conduct or observation to make, and type of data to
collect:

-   *Shelter hypothesis:* a greater proportion of *Gammarus* should be
found in the open in streams with slow flow, or in slower flowing
areas of a stream.

-   *Food hypothesis:* *Gammarus* should not aggregate under stones from
which all leaf litter has been removed; *Gammarus* should aggregate
on patches of leaf litter tethered in the open part of the stream
bed.

-   *Predation hypothesis:* *Gammarus* should aggregate under stones
more in streams where fish are present than where they are not;
*Gammarus* may spend less time under stones at night.

Ideally you are looking for a prediction that is unique to the hypothesis it is based on - so if the prediction is true only one of the hypotheses could have been responsible, but this may not always be possible and some combination of predictions may need to be used. Additionally, several processes may be operating at the same time. This makes hypothesis testing harder still. It may be necessary to consider two or more hypotheses, and their corresponding predictions, in combination. For example, *Gammarus* may be under stones because it prefers the sheltered environment, but also because food accumulates there. In this case we might expect that *Gammarus* will show a weak aggregative response to shelter alone, or food alone, and a stronger one
to them both together.

## Hypothesis testing

Once we have firmed up our questions, hypotheses, and predictions we can collect the data to evaluate our ideas. On the basis of these data we will either accept or reject the various hypotheses. The important thing to realise about the process of hypothesis testing is that, in science especially, hypotheses are either rejected, or not rejected, but an hypothesis can rarely, except in trivial cases, be proved.

This seems like an odd state of affairs! True, but it does make sense. Since you cannot be sure that you have thought of all the possible hypotheses to explain an observation, finding evidence that supports the prediction from one your hypotheses, does not guarantee that the hypothesis is the only one which could have produced the effect you find. On the other hand, if you find evidence that directly contradicts the prediction(s) from your hypothesis, you can be certain (assuming the prediction and data are not flawed) that the hypothesis cannot be true.

An hypothesis which predicted that all conifers should be evergreen could be supported by numerous observations of different conifer species in forests around the world, but is conclusively refuted by the first
larch tree we encounter.

Having tested your hypothesis, by examining the evidence that its predictions are true, you may accept it as the best (so far) explanation of the observations, or you may reject it as an explanation, and turn to other hypotheses. The same procedure must then be repeated for these hypotheses.

This basic cycle of proposing hypotheses and then seeking evidence potentially capable of falsifying them, is, in essence, the idealized model of the scientific process famously proposed by the philosopher of science Karl Popper (1902-1994). It is often termed *falsificationism*.

## Don’t we ever know anything for sure? {#are-we-sure}

The method presented here provides a view of science as one in which we suggest hypotheses, then test them trying to reject them by finding conclusive counter-evidence, then replacing them with new hypotheses. It all sounds a bit frustrating. In fact of course we do ‘accept’ hypotheses all the time — that is we fail to reject them over and over again. These hypotheses become more accepted and in some sense become regarded as ‘true’ if repeated attempts to test them all fail to provide good counter evidence. In other words, we have some ideas that are doing pretty well in terms of resisting falsification, and we use these as our best estimates of the truth, with the proviso that it is still possible a better idea will come along in due course.

The simple process of falsification described above also presents a picture of scientists as wonderfully neutral, objective creatures, rationally proceeding through cycles of setting up hypotheses, testing them, rejecting them, cheerfully setting them aside and starting over again. Of course this is not a true reflection of the complex, messy, business really involved in trying to figure out how the world works. Philosophers of science have argued long and hard about how far from this idealized process real science actually is. Various alternative philosophies suggest more ‘realistic’ processes, such as Thomas Kuhn’s view of science as periods of relative stasis, where people work within an accepted paradigm (a set of views about how things work) despite accumulating evidence that doesn’t always support the paradigm, until
finally it is upset by a ‘revolution’ which rejects the entire paradigm, and proposes a new view. The philosopher Imre Lakatos proposed some resolution of these views, suggesting that scientific ideas were grouped together in ‘research programmes’ concerned with particular endeavours, and that within these there may be core ideas that are not challenged, but other related ideas which are being challenged and adjusted by
falsification, and that together these make each research programme progress. Programmes that don’t progress should be abandoned in favour of those that do.

Of course that is a very over-simplified sketch of some important ideas (which are well worth reading a bit about), but in practice these philosophical arguments are really more focused on how whole areas of
science develop. When you are just thinking about constructing a simple study of one problem, then the basic falsification cycle is a pretty good approach to have in your mind. Even in it’s simple form, however,
it is not immune from the effect of human fallibility (see below).

The process laid out here is not a strict set of rules, but outlines an approach to scientific investigation which is widely considered to provide a rigorous and productive system. As with all such systems understanding the ’normal’ process is a prerequisite for constructively breaking the rules.

```{block, type='advanced-box'}
#### It's hard to reject an hypothesis you love!
An interesting aside on the process is given by Sutherland (1994) who suggests that, in everyday life at least, we are often very reluctant to deliberately seek evidence that might refute a hypothesis we have formulated, and often persist in holding on to the hypothesis even when the evidence is against us.  This can be seen in experiments where people are presented with a sequence of numbers (say 2, 4, 6) and have to try and establish the general rule to which the set of numbers conforms.  The subject decides on an initial rule and then can test this rule by suggesting other sets of numbers and being told whether or not those numbers conform to the rule. 

The initial guess at a rule is usually 'even numbers in ascending order', and the initial test suggestions are typically another set of even numbers ascending by two  (e.g. 16, 18, 20). These, the subjects are told, conform to the rule.  The next set of numbers suggested is then often another similar set (40, 42, 44) -- which again conforms, and sometimes another similar guess will follow. However, the suggestion of further sets of even numbers ascending by two cannot test this (most simple rules that allow 2, 4, 6 will allow the other sequences too). A better first suggestion would be to change one of the components of the initially proposed rule e.g. even numbers to odd numbers: 3, 5, 7;  or the increment:  2, 8, 14.  If these conform we can reject a specific component of our initial rule. (The rule is, in fact, simply any ascending sequence of numbers.)
```

## Further reading {#further-reading}

Barnard C, Gilbert F and McGregor P (1993) *Asking questions in biology*. Longman.

Ladyman, J (2002) *Understanding the philosophy of science*. Routledge.

Sutherland S (1994) *Irrationality*. Penguin.

<!--chapter:end:030_asking_questions.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Populations and samples

> Statistics is the science of learning from data, and of measuring, controlling, 
> and communicating uncertainty; and it thereby provides the navigation essential 
> for controlling the course of scientific and societal advances
> 
> [Davidian and Louis (2012)](https://doi.org/10.1126/science.1218685)

[[FINISH ME]]

## Populations {#populations}

The word '**population**' has much a broader meaning in statistics than it does in biology. When a biologist talks about a population they are referring to a group of individuals of a particular species who interbreed. In statistics, a population is a much more abstract concept. It refers to any group of items that share certain attributes or properties. This idea is best understood by example:

-   The readers of this book are an example of a population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, they tend to have similar educational backgrounds and career aspirations. They are more similar to one another than randomly chosen members of the UK population.

-   Peatland regions in the UK is another example. There are many peatland sites in the UK. Although their ecology varies from one location to the next, they are all similar in certain respects---e.g. they are generally characterised by low-growing vegetation and acidic soils.

-   A population of plants or animals---as understood by biologists---can of course also be thought of as a statistical population. The individuals that comprise a biological population share common behaviours, physiology and life histories. Much of organismal biology is concerned with learning about these kinds of properties of individuals.

The primary goal of a statistical analysis is to learn something about a population of interest. The population is defined by the investigator, and the 'something' can be anything they know how to measure. 

For example, a social scientist might be interested in understanding the political attitudes of university students, a climate change scientist might want to know how much carbon is stored in UK peatland areas, and a behavioural ecologist might want to understand how much time individuals spend foraging for food and searching for mates.

## How do we learn about populations?

These examples involve very different populations and questions. Nonetheless, there are fundamental commonalities in how they can be addressed using statistical ideas. We can break the process down into a number of steps:

**Step 1: Refine your questions, hypotheses and predictions**

[[COMPLETE ME]]

**Step 2: Decide which variables are you interested in**

The second step is to decide which features of the population you need to learn about. In essence this comes down to refining your question, i.e. which variable (or variables) do I need to measure to address a research question? In the examples above, these would be things like, a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in a biological population. Sometimes we are interested in more than the properties of individual variables. Instead, we might want to understand the relationship between two or more variables.

**Step 3: Decide which population parameters are relevant**

Once we have decided which variable (or variables) to study, we have to decide which **population parameter** is relevant. A population parameter is a numeric quantity that describes something about the distribution of one or more variables in the population. That is, it summarises some aspect of the population.

A simple, but important, population parameter is the population mean. We often want to learn about the population mean because it allows us to answer questions such as "how many are there?" or "how much of something is present?". Much of this course is about asking questions of population means. Other population parameters may be important though, e.g.

*   The goal of statistical genetics is to partition variablity among individuals---we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case, it is the population variance that we want to learn about. 

*   If we are trying to understand how two variables are associated, some kind of correlation coefficient (we'll learn more about this later) might be the right population parameter to focus on. 

**Step 4: Gather a representative sample**

The type of statistics we use in this course is called 'frequentist statistics' (you don't have to remember that). Within this framework population parameters are conceptualised as fixed but unknown quantities. Our goal is to learn about them by collecting data.  

If we could measure every object in a population we wouldn't need statistics. We could just calculate the quantity we needed using an exhaustive sample and we'd have our answer. In the real world we face all kinds of resource constraints. We have limited time and money to invest on any given problem, no matter how important it is. This means that instead of trying to measure every item in a population, we instead study a **sample** of that population. A sample is just a subset of the wider population, which has been chosen so that it is representative of that population. 

That word "representative" is very important. If we can't collect a representative sample of the population then it is very difficult to infer anything useful about it. For example, if we want to understand the reproductive characteristics of our favourite study organism, but we only sample young or old individuals, it will be very difficult to generalise our findings to the wider population. The reproductive performance of most organisms changes as they age, so if we only measure young individuals, we haven't learnt anything about older individuals, and vice versa.

The process of generating reliable samples forms part of experimental design and sampling theory. These are large, rather technical parts of statistics. It's certainly beyond the scope of this course to study them in any great deal. Nonethless, we will touch a few of the more important practical aspects as we move through this course.

**Step 5: Estimate the population parameter(s)**

Once we have a representative sample we can calculate something called a **point estimate** of the population parameter (or parameters) we're interested in. Remember, the population parameter is unknown. That's why we collect samples. A point estimate is just a number that represents our "best guess" its true value. For example, if we are interested in a population mean, then the obvious point estimate to use is the mean of sample we collected. This is just "the average" you learned to calculate in school.

By the way, people often just say "estimate" instead of "point estimate" because writing "point estimate" all the time is tedious. It's fine to do this. People also colloquially refer to point estimates as "statistics". This is probably best avoided as it is somewhat ambiguous. The exact terminology isn't really all that important to be honest---we'll mostly use the word "estimate" from now on.

**Step 6: Quantify the uncertainty of estimate(s)**

A point estimate of a population parameter is virtually useless on its own because it is derived from a limited sample of the wider population. Even if we are very careful about how we sample the population, there is no way to guarantee that the composition of our sample exactly matches that of the population. This means that any estimate we derive from a sample will be imperfect, in the sense that it won't exactly match the true population value.

So there is always uncertainty associated with any estimate of a population parameter. What can we do about this? 

**Step 7: Answer the question!**

This means that we have to be very careful if we want to answer a seemingly simple question such as, "Is the more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?" 


[[FINISH ME]]


<!--chapter:end:040_populations_samples.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Sampling variation and standard errors

[[PREAMBLE]]

```{r plant-sim-par, echo=FALSE}
set.seed(27081975)
nsamp <- 200
sampsize1 <- 20
sampsize2 <- 40
sampsize3 <- 80
index <- c(1,1,2,2,2)
prop.purp <- sum(index==1)/length(index)
```

... we can use the machinery of statistics to help us quantify this uncertainty. Once we know something about the uncertainty of an estimate we can start to ask questions of the sort we discussed. However, before we can use these tools we first need to understand something called sampling error. The rest of this chapter will give you a flavour of this really important idea.

The easiest way to get a sense of how population parameters, point estimates and sampling error are related to one another is by working with a concrete example. Rather than collecting real data, we're going to use a computer simulation. A simulation is just an imitation of a real-world process. The advantage of using a simulation is that it allows us to study many different realisations of the same process. We'll see why that is important soon.

## The example {#morph-example}

Let's study a very simple example. Imagine we are working on a plant species that is polymorphic. There are two different morphotypes ('morphs'), and to keep life simple we'll refer to them as the purple morph and the green morph. The statistical population we're interested in is therefore a biological population of plants. We can depict this situation visually with a map showing where the purple and green individuals are located on a hypothetical landscape:

```{r plants-all, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Landscape showing the position of purple and green morphs'}
plantdata <- 
  data.frame(xloc  = runif(nsamp), 
             yloc  = runif(nsamp), 
             morph = sample(c("purple","green")[index], 100, replace = TRUE))
plttheme <- theme_get()
plttheme$axis.text <- plttheme$axis.ticks <- plttheme$axis.title <- element_blank()
baseplt <- ggplot(plantdata, aes(x = xloc, y = yloc, colour = morph, )) + 
           geom_point() + scale_color_identity() + coord_fixed() + plttheme
baseplt
```

These artificial, idealised data were generated using R. We placed 'individuals' onto the landscape at random locations---every location is equally likely---and then assigned them purple morph status with a certain probability; we made them are green otherwise. In fact, we used a probability of 0.4 to assign the purple morph status. Of course, in a real world setting we wouldn't know this.

## The question, variable, and population parameter

Let's proceed as though this were a real situation. 

[[Question]]

To study plant morphotype we need to collect information about the colour of different individuals. This means we are working with a nominal variable, taking values 'purple' or 'green'. That's step two complete: we've decided on the variable we need to study.

The prediction we want to test is about the purple morph frequency. The population parameter of interest is therefore, the morph frequency (or equivalently, the percentage or proportion). That's step three done: we've decided on the population parameter we're interested in.

## Sampling error and sampling distributions

In contrast to a real-world setting, we already know the true value of the population parameter in this example. The purple morph frequency is 40%. In the real world this is the parameter we'd be trying to estimate. If we weren't able to sample every individual, we would have to construct some kind of point estimate of the purple morph frequency. To do this, we would take a representative sample of plants from across the landscape and then calculate the percentage of purple plants in our sample. 

A representive sample in this case is one in which every individual has an equal probability of being sampled. We call this a **random sample**. Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality. Luckily it is easy to at least simulate a random sample.

Let's seen what happens if we sample `r sampsize1` plants in this way. This plot shows the original population of plants, but now we have circled the selected individuals in red.
```{r plants-samp1, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Plants sampled on the first occasion'}
sample1 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample1, colour = "red", shape = 1, size = 5)
freqs1 <- table(sample1$morph)
```
We found `r freqs1["green"]` green plants and `r freqs1["purple"]` purple plants in this first hypothetical sample, which means our estimate of the purple morph frequency is `r round(100*freqs1["purple"]/sampsize1)`%. This is not far off the true value of 40%. What happens if we repeat this process, resulting in a new, completely independent sample? Here is the sampled population:
```{r plants-samp2, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Plants sampled on the second occasion'}
sample2 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample2, colour = "red", shape = 1, size = 5)
freqs2 <- table(sample2$morph)
```
This time we ended up sampling `r freqs2["green"]` green plants and `r freqs2["purple"]` purple plants, so our new estimate of the purple morph frequency is `r round(100*freqs2["purple"]/sampsize1)`%, which is quite some way off the true value.

Nothing about the study population changed between the first and second sample. What's more, we used a completely reliable sampling scheme to generate these samples; there is nothing biased or 'incorrect' about the way individuals were sampled. The different estimates of the purple morph frequency arise from nothing more than chance variation.

This chance variation--which arises whenever we observe a sample instead of the whole population--has a special name. It is called the **sampling error** (or sampling variation). Sampling error is the main reason we have to use statistics to learn from data. It is always present, and so any estimate you derive from a sample is affected by it. Sampling error is not a property of any particular sample. It is really a property of the population distribution of the focal variable, and the sampling method used to investigate this. That statement may seem a little cryptic now, but we will start to get a sense of what it means in this, and the next, practical.

We can develop our simple simulation example to explore the consequences of sampling error. Rather than taking one sample at a time, we will use R to simulate 1000s of different samples, and for each sample, calculate the number of purple morph individuals found. Each sample is drawn from the same population, i.e., the population parameter (purple morph frequency) is the same for every sample. Here is a summary of one such repeated simulation exercise:
```{r samp-dist-1, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 20)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize1, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(0:sampsize1)) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

This bar plot summarises the result from 100000 samples. In each sample, we took `r sampsize1` individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, ... purple individuals, all the way up to the maximum possible (`r sampsize1`). It summarises the distribution of purple morph counts that we can expect when we repeat the same sampling process over and over again.

This distribution has a special name. It is called the **sampling distribution**. The sampling distribution is just the distribution we expect a particular statistic to follow. In order to to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. We just used simulation to approximate the sampling distribution of purple morph counts that arises when we sample `r sampsize1` individuals from a population that is `r 100*prop.purp`% purple. 

The sampling distribution is the key to 'doing statistics'.  

Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error (to quantify uncertainty), and we can begin to make meaningful comparisons that enable us to address scientific questions. Fortunately, we don't have to work any of this out for ourselves. Statisticians have already done this for many different situations.

### The effect of sample size

Perhaps the most important property of any sampling scheme is the **sample size**: the number of observations (objects or items) in a sample. To see how sample size influences the sampling distribution, and to understabnd why it matters, let's carry on with our simulation example. We will repeat the resampling exercise, but this time we will do it twice, first taking a sample of `r sampsize2` individuals each time, and then taking a sample of `r sampsize3` individuals each time. In both cases, we'll examine the results of taking 100000 samples overall:

```{r samp-dist-2, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 40)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize2, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize2, 1)), 
                   breaks = as.character(seq(0, sampsize2, 2))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

```{r samp-dist-3, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 80)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize3, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize3, 1)), 
                   breaks = as.character(seq(0, sampsize3, 4))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

What do these plots tell us about the effect of increasing sample size? Notice that we plotted each of them over the full range of possible outcomes (the x axis runs from 0-`r sampsize2` and 0-`r sampsize3`, respectively, in the first and second plot). We did this so that we can meaningfully compare the spread of each sampling distribution, relative to the full range of possible outcomes.

What do these figures show? The range of outcomes in the first plot is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65% (we sampled 40 individuals each time). The range of outcomes in the second plot is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. Clearly, this suggests that when we increase the sample size we expect to encounter less sampling error. This makes intuitive sense: the composition of large sample should more closely approximate that of the true population than a small sample. 

How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample `r (sampsizebig <- 500)` individuals each time we take a sample: 
```{r samp-dist-big, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 500)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsizebig, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsizebig, 1)), 
                   breaks = as.character(seq(0, sampsizebig, 50))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples that we just considered, but even with 500 individuals in a sample, we should still expect quite a lot of uncertainty in our estimate. The take home message is that you need a lot of data to reduce sampling error.

## The standard error

We were fairly relaxed about how we quantified the variability of a sampling distribution in the last chapter. All we did was extract the approximate range of purple morph counts "by eye". This is fine for investigating general patterns, but to make rigorous comparisons, we really need a quantitative measure of this variability. It is called the **standard error**. 

The standard error is actually quite a simple idea, though its definition sometimes confuses people. Here is the definition: the standard error is the standard deviation of the sampling distribution of an estimate (such as a mean or frequency). Don't worry if that makes absolutely no sense yet. The key point is that it is a standard deviation, so it quantifies the expected spread, or dispersion, of the sampling distribution.

(Note that it is common to use a shorthand abbreviations ("SE", "se" or "s.e") in place of 'standard error' when refering to it in text.)

We can use a simulation in R to calculate the expected standard error of an estimate of purple morph frequency. Let's assume we want to know the expected standard error when we use a sample size of 100 and the purple morph frequency is 40%. We can use this snippet of R code to generate 10000 samples :
```{r}
purple_prob <- 0.4
sample_size <- 100
n_samples <- 100000
raw_samples <- rbinom(n = n_samples, size = sample_size, prob = purple_prob)
percent_samples <- 100 * raw_samples / sample_size
```
This is the same R code we used to generate those histograms above. The only difference is that we converted the numbers sampled into proportions. You don't have to understand how this works, though if you did A-level statistics you might be able to guess what the `rbinom` function is doing. Really, the R code isn't important here. The result is what matters: we just simulated the percentage of purple morph individuals found in 100000 samples of 20 individuals, storing the result in a vector called `percent_samples`. Here are the first 50 values:
```{r}
head(percent_samples, 50)
```
How do calculate the required standard error? The standard error is the standard deviation of these numbers, so we just use the `sd` function:
```{r}
sd(percent_samples)
```
What does this tell us?

[[FINISH ME]]

## A quick recap

[[FINISH ME]]

By this point you might (quite reasonably!) be wondering why we have spent so much time looking at the properties of repeated samples from a population with **known** parameters. After all, when we collect real data we only have a single sample to work with and we don't know much about the population parameter of interest. This lack of knowledge is the reason for collecting the data in the first place!

The short answer to this question is that we want to learn how to use '**frequentist inference**' in this course. A precise definition of what constitutes frequentist statistics is well beyond the scope of this course, but we can give a rough description. Frequentist inference works by asking *what would have happened* if we were to repeat an experiment or data collection exercise many times, assuming that the a population parameter never changes.

[[FINISH ME]]

## Estimating the standard error {#se-bootstrap}

In order to answer that last question we need to work out what the **sampling distribution** purple morph frequency estimate looks like. At first glance, this seems like an impossible task when we only have a single sample to work with. One solution to this problem is surprisingly simple: we use the sample to approximate certain aspects of the population, and then work out what the sampling distribution of our focal estimate  looks like using this approximation.

Let's unpack this idea, and then try it out for real.

### The bootstrap {}

There are many different ways to approximate a population from a sample. One of the simplest methods---for easy problems at least---is to pretend *the sample is the true population*. We then draw new samples from this pretend population. That may sound a lot like cheating, but it turns out that this is a perfectly valid way to construct a sampling distribution for many different kinds of estimates. 

Here is how it works. Imagine that we had written down each sampled individual's colour on a different piece of paper, and placed all of these in a hat. We then do the following:

1. Pick a piece of paper at random, record its value (purple or green), put the paper back into the hat, and shake the hat about to mix up the bits of paper.

2. Pick another piece of paper (you might get the same one), record its value, and then put that back into the hat, remembering to shake everything up.

(The shaking here is meant to ensure that each piece of paper has an equal chance of being picked. This might not work in reality of course.)

3. Repeat this process until you have a recorded new sample of colours which is the same size as your real sample.

(This process is called 'sampling with replacement'. Each artificial sample is called a 'bootstrapped sample'.)

4. For each bootstrapped sample, calculate whatever quantity is of interest (i.e. the proportion of purple morph plants sampled).

5. Repeat steps 1-4 until we have generated a large number of bootstrapped samples. 10000 is often sufficient.

Although it may seem like cheating (it's not!), this process really does produce an approximation of the sampling distribution of our quantity is of interest. It is called **bootstrapping**. 

### Doing it for real

Here is how to implement it in R to construct a sampling distribution for the estimated dry weight of purple morphs (no hats or paper required):
```{r}
purp.weights <- filter(morph.weights, pmorph == "purple")$weight 
boot.samp <- replicate(10000, mean(sample(purp.weights, replace = TRUE)))
```
All we did here was extract the sample of purple morph dry weights---using `filter` to subset the data, and `$` to grab the `weights` column---and then use functions called `sample` and `replicate` to generate a bootstrapped sampling distribution for the mean (we generated 10000 samples). You don't have to understand this R code, but ask a demonstrator if you want to know more about it. 

Let's take a quick look at the first 10 values:
```{r}
round(head(boot.samp, 10))
```
These numbers represents different values of the sample mean that we would expect to generate if we repeated the data collection exercise. We can use this bootstrapped sampling distribution in a number of ways. As always, it is a good idea to plot it first get a sense of what it looks like. A histogram is a good choice here, because we have a large number of samples:
```{r}
plot.df <- data.frame(boot.samp) # 'ggplot' expects a data frame 
ggplot(plot.df, aes(x = boot.samp)) + geom_histogram(binwidth = 5) 
```

The mean of the sampling distribution looks to be round about 655 grams--very close to the sample mean. We can of course calculate this in R: 
```{r}
round(mean(boot.samp))
```
This is the same as the sample statistic (the sample mean, in this case). This will always be the case if we construct a large enough sample, as the bootstrapping procedure assumes that the 'true' population mean is equal to the sample mean.

A more useful quantity is the bootstrapped standard error (SE). Since this is the standard deviation of the sampling distribution, we just apply the `sd` function to the bootstrap sampling distribution to calculate it:
```{r}
round(sd(boot.samp), 1)
```
This quantity is a standarised measure of uncertainty that we require. A large SE implies that our sample size was too small to reliably estimate the population mean. Whenever we report a point estimate of a mean, we should also report the standard error. For example,

> The mean dry weight biomass of purple morph plants (n = 25) was 658 grams (s.e. ± 22.3). 

Notice that we also report the sample size.

The bootstrap is a very powerful tool in the right hands. The bootstrap is actually quite an advanced technique that can be difficult to apply in many settings (e.g. analysis of complex experiments). It is for this reason that we will not apply it routinely in this course. We used it here to understand how 'frequentist' ideas can be used to quantify uncertainty in a statistic (i.e. a point estimate).

The key message to take away is that we can characterise uncertainty by working out what **would happen under repeated sampling** from a particular population. 


<!--chapter:end:050_standard_error.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Making comparisons and *p*-values

Scientific enquiry involves asking questions and collecting information to answer these questions. One of the commonest kinds of question that we ask as biologists is: ‘Is there a difference between the measurements from two samples?’ For example:

‘Do male and female locusts differ in length?’

‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’

‘Do eagle owls feed on rats of different sizes during winter and summer?’

‘Do streams running through conifer forests differ in pH from those running through deciduous forests?’

'Do purple and green plant morphs differ in their biomass?'

In order to answer these kinds of questions we need to step through the same kind of process discussed last week. We have to: (1) decide which variables we need to measure; (2) decide which population parameters are relevant; (3) design a suitable experiment or gather representative samples; (4) use the samples to learn about the populations.

Let's assume that we already know how to go about steps 1-3 and have already collected our samples. Our focus in this chapter is on step 4: using these samples to learn about the populations. Specifically, the problme we want to understand is how to evaluate whether or not two populations are different in some way.

## A new example {#morph-weights-eg}

```{r, echo = FALSE, eval = FALSE}
set.seed(27081975)
nsample <- 25
prpszs <- rnorm(nsample, mean = 700, sd = 150) + 20
grnszs <- rnorm(nsample, mean = 740, sd = 160) 
pmorph <- c("purple","green")[rep(1:2, each = nsample)]
morph.weights <- data.frame(pmorph = pmorph, weight = round(c(prpszs, grnszs)))
write.csv(morph.weights, row.names = FALSE, file = "./course-data/MORPH_WEIGHTS.CSV")
```

Once again, we'll investigate the problem by working through a simple example. We will stay with the purple morph / green morph example, but this time we'll examine a different variable: the dry weight biomass of our imaginary plants. Perhaps we suspect that the two morphs have different growth habits. One way to address this hypothesis would be to measure the biomass of individuals of each morph. Ultimately, we want to compare the mean biomass of each morph.

In order to study the biomass of individuals in our hypothetical population, we would collect a sample of dry weights. Dry weight is a numeric variable, measured on a ratio scale. The population parameters of interest now are the population mean dry weights of each morph. 

Notice that our definition of 'the population' is slightly different than before. Now we are imagining that each morph is a seperate population. Let's assume that we have sampled the dry weight (in grams) of 25 representative individuals of each morph.

We have have generated a dataset to represent this situation, stored in a Comma Seperated Value (CSV) text file called 'MORPH_WEIGHTS.CSV'. 

<div class="exercise-box">
#### Exercise: Investigate the dataset
<div class="box-text">
Download the MORPH_WEIGHTS.CSV file from MOLE and place it in your working directory (this is the location you set at the beginning of this practical). Next, make sure that you can do the following:

* Read the data in MORPH_WEIGHTS.CSV into an R data frame using `read.csv`, assigning the data frame the name `morph.weights`.

(Hint: Use RStudio to help you do this. In the 'Environment' tab there is a dropdown menu called 'Import Dataset'. Click on this, select 'From Text File...', click on the MORPH_WEIGHTS.CSV file, and then then change the suggested name to 'morph.weights'. You should then need to copy the `read.csv` command that RStudio sends to the Console into your script.)

* Use the `glimpse` function from `dplyr` to inspect the structure of `morph.weights`. How many variables are in the dataset. What are their names? What kind of variables are they?

* Use the `View` function to inspect the data. How many rows are in the dataset--does this number make sense to you? Are the values of the different variables as you would expect them to be?
</div>
</div>

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

We read in the data and then carried out a few 'sanity checks' in that last exercise. **Always check your data after you have read it in**. There is no point messing about with the likes of `dplyr` and `ggplot2`, or carrying out a statistical analysis, until we have done this. If we don't understand how our data is organised, there is very real risk that we will make a lot of easily avoidable mistakes.

The next step is to calculate some simple descriptive statistics for each morph. We need to know the sample means--these are our 'best guesses' of the population means--and it may be useful to know something about the variability of the samples. The sample standard deviation is a good measure of the latter. Here is a reminder of how to do this using `dplyr`:
```{r}
temporary <- group_by(morph.weights, pmorph)
summarise(temporary, mean = mean(weight), stan.dev = sd(weight))
```
This shows that the mean dry weights of purple and green morphs are 658 grams and 787 grams, respectively. The standard deviation estimates from the two samples suggest that dry weights of green morphs are a little more variable than the purple morphs. 

By the way, if you like using the `%>%` operator to chain together `dplyr` functions (the 'verbs'), then here is how to use this to achive exactly the same result as above:
```{r}
morph.weights %>% 
  group_by(pmorph) %>% 
  summarise(mean = mean(weight), stan.dev = sd(weight))
```

Using means and standard deviations to summarise samples can be tricky to understand until you are used to them. A plot of some kind is much more useful. At the moment, we are just trying to understand the distribution of our two samples. One way to do this is to construct a histogram or a dotplot of each sample distribution. We don't have much data, so a dotplot is probably the best option. Here is one way to make a dot plot for just the purple morph data:
```{r purple-dist}
purp.weights <- filter(morph.weights, pmorph == "purple") 
ggplot(purp.weights, aes(x = weight)) + geom_dotplot(binwidth = 30)
```

First, we filtered the complete dataset so that only the observations corresponding to the purple morph were retained, then we used `ggplot` with the dotplot geom to construct the figure we wanted. Once again (just as a reminder), here is how to achieve the same thing using the `%>%` operator:
```{r, fig.keep = FALSE}
morph.weights %>% 
  filter(pmorph == "purple") %>% 
  ggplot(aes(x = weight)) + geom_dotplot(binwidth = 30)
```

You should always explore your data visually before carrying out any kind of statistical analysis of it. You might learn something new about it, and at the very least, this allows you to assess whether there are any problems with it. A quick inspection of this figure suggests that there is nothing odd about the dry weight data. We only have 25 observations, so we can't say too much about its shape, but there don't seem to be any outliers. 

We are going to focus on just the purple morph data from now on. Let's return to our original question: How do we characterise the precision of the estimated mean dry weight of purple morphs (658 grams)?


It is much easier to understand the different parts of this problem if we work with a concrete example. We'll use the purple plant / green plant situation from last week, focussing on the dry weight biomass of our imaginary plants (rather than their frequency). We hypothesised that perhaps the two morphs have different growth habits, and so we measured the biomass of individuals in a sample of each morph. The focal variable is dry weight biomass (a numeric variable, measured on a ratio scale).

Last week, we looked at how to quantify uncertainty in a single estimate of the population mean biomass of a morph. This week, we want to compare the biomass samples of the two morphs to address the question: 'Do purple and green plant morphs differ in their dry weight biomass?'. However, as it is currently framed, this question is a little too vague. The first thing we have to do is define what we mean by 'different'.

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

As always, it is a very good idea to plot the data. We could do this in a variety of ways, but since we only have two groups (purple and green morphs), we may as well summarise the full sample distribution of each morph. The data are in the MORPH_WEIGHTS.CSV file, which we have read into R using the `read.csv` function. We stored the data in a data frame called `morph.weights`. Here is some `ggplot2` code to make the required plot:  
```{r two-morph-dist}
ggplot(morph.weights, aes(x = weight)) + 
  geom_dotplot(binwidth = 30) + 
  facet_wrap(~pmorph, ncol = 1)
```

(Hopefully this plot will also remind you how to use the `facet_wrap` function to make a multipanel plot, based on the values of particular variable--`pmorph` in this case).

What does this plot suggest? We are interested in the degree of similarity (or not!) of the two sample distributions. There is a lot of overlap in the dry weights of each morph, but in general we can say that: (1) green morph individuals tend to have higher dry weights than purple morphs, and (2) the green morphs seem to be more variable than purple morphs.

We can get a better handle of these patterns by calculating the sample means and standard deviations of the two samples to evaluate their central tendency and spread, respectively. We know how to use `dplyr` functions `group_by` and `summarise` to do this:
```{r}
descrip.stats <- 
  morph.weights %>% 
  group_by(pmorph) %>%
  summarise(wght.mean = mean(weight), wght.sd = sd(weight))
descrip.stats
```

(If you are confused by this, ask a demonstrator to explain how it works).

These descriptive statistics back up our visual impressions of the data: green morph individuals are larger than purple morphs (sample means: 787 vs. 658 grams), and green morphs are more variable than purple morphs (sample SDs: 118 vs. 114 grams). Remember, these numbers are just point estimates derived from limited samples. If we sampled the populations again, sampling variation would ensure that we end up with different estimates. This means we are not yet in a position to conclude that green morphs are bigger than purple morphs.

Let's return to our question: 'Do purple and green morphs differ in their dry weight biomass?'. We said this question was a little too vague, and that we needed to define what we meant by 'different'. One way in which the sample distributions seem to be different is with respect to their spread, and so one possible refinement of this question would be to ask whether this difference is 'real', or simply arises from sampling variation. There are circumstances where it is scientifically interesting to compare variability. However, it is much more common to focus on differences in the central tendency of samples.

By looking at the central tendency of different samples, we can evaluate whether or not something we have measured increases or decreases, on average, among different populations. Many scientifically relevant questions are addressed by making this assessment. When someone uses a statistical test or model to 'compare samples', what they are usually doing is evaluating whether or not the central tendency of the populations are different. We typically make this assessment by evaluating the strength of evidence for the presence of **a difference in the population means** of the focal populations. 

The question we want to address is therefore: 'What is the strength of evidence for a difference in the population mean biomass of purple and green plant morphs?' In practise, this boils down to a another question: 'Is there a statistically significant difference between their means.'

We will now turn to the idea of **statistical significance**. We will also see how to evaluate statistical significance by calculating a **p-value** under a suitable **null hypothesis**.

### How do we evaluate statistical significance?

```{r, echo = FALSE}
set.seed(27081975)
nperm <- 2000
perm.out <- numeric(nperm)
perm.eg <- list()
data.i <- morph.weights
ids <- morph.weights$pmorph
for (i in 1:nperm) {
  morph.labels <- sample(ids, replace = FALSE)
  perm.out[i] <- 
    mutate(data.i, pmorph = morph.labels) %>% 
    group_by(pmorph) %>% summarise(mean = mean(weight)) %$% diff(mean)
  if (i <= 3) {
    perm.eg[[i]] <- morph.weights$weight
    names(perm.eg[[i]]) <- morph.labels
  }
}
names(perm.eg) <- paste("Sample", 1:3)
```

In order to assess the strength of evidence for the presence of a difference between the population means of two groups, we have to do something that, at first glance, looks very strange. We can break this down into four steps:

1. We assume that there is actually no difference between the population means. That is, we hypothesise that all the data are sampled from a pair of populations that are characterised by a single, shared population mean.

2. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation.

3. We then ask, 'if there is no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we actually observed in the sample?'

4. If the observed difference is sufficiently improbable, then we conclude that we have found a 'statistically significant' result. A statistically significant result is one that is inconsistent with the hypothesis of no difference.

(We will discuss how to define 'sufficiently improbable' below)

There are different ways to go about realising this process. We'll look at two of these today. Regardless of the details, they work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (i.e. a difference in means) is absent. If you can understand this fundamental idea you are well on your way to understanding how frequentist statistics works. Don't worry if this all seems very abstract (it is)--we are not expecting you to understand it at this point.

Let's return to our example to see how this might work in practise.

### A permutation test

In our example, a hypothesis of 'no difference' between the mean dry weights of purple and green morphs has the following implication. It means both morphs are really sampled from the same population, and as such, the labels 'purple' and 'green' are meaningless. These labels may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows:

1. Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels 'purple' and 'green' to this new copy of the data. Do this in such a way that the original sample sizes are preserved.

(We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used. The process of assigning random lablels is called permutation)

2. Repeat this permutation scheme until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient.

3. For each permuted sample, calculate whatever sample statistic is of interest. In this case, we want the *difference* between the mean dry weight of purple and green morphs in each sample.

4. Compare the observed sample statistic (i.e. the difference between the mean dry weights) to the distribution of sample statistic from the randomly permutated samples.

This scheme is called a permutation test, because it involves random permutation of the group labels. Why is it useful? *Each unique random permutation yields an observation from the sampling distribution of the difference among sample means, under the assumption that this difference is really zero in the population.* This means we can assess whether an observed difference is consistent with the hypothesis of no difference by looking at where it lies relative to this sampling distribution. 

We have implemented a permutation test in R using the purple/green morph dataset for you, using `r nperm` permutations. We won't show you the R code because it uses a few tricks you haven't been taught, but we can look at a couple of permuted samples to get a sense of how this works: 
```{r, echo=FALSE}
perm.eg[1:2]
```
The data from each permutation are stored as numeric vectors, in  which each element of the vector is named (these are the labels). Notice that the set of numbers does not change among the permuted samples. The only difference between them is the labelling of the numbers. The difference between the mean dry weights in the first permutation is `r perm.out[1]`. This difference is `r perm.out[2]` in the second sample.

What we really care about here is distribution of these differences. This is an approximation to the the sampling distribution of the difference between means. Here is a histogram that summarises the `r nperm` mean differences from the permuted samples:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept = diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

Notice that this distribution is centred at zero. This makes sense--if we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. The red line shows the location of the observed difference between purple and green morph mean dry weights. The relevant feature here is the location of this observed difference within the sampling distribution.

```{r, echo=FALSE}
nlower <- sum(perm.out <=  diff(descrip.stats$wght.mean))
nhgher <- sum(perm.out >= -diff(descrip.stats$wght.mean))
```

What does this figure tell us? It looks like the observed difference is very unlikely to have arisen through sampling variation, under the assumption that the population means of the two groups are identical. We can say this because the observed difference lies at the end of one 'tail' of the sampling distribution. We need to be able to make a more precise statement than this though.

Only `r nlower` out of the `r nperm` permutations ended up being equal to, or 'more extreme' (i.e., more negative), than the observed difference. The probability of finding a difference in the means equal to or more negative than the observed difference (denoted *p*) is therefore, *p*=`r nlower/nperm` (`r 100*nlower/nperm`%). This probability has a special name. It is called the **p-value**. A p-value is defined as the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true.

We have to be careful at this point. The test we just did is called a 'one-tailed' test, because we only looked at one end (the tail) of the sampling distribution. However, we did not set out to test whether purple plants were smaller on average than green plants. We set out to assess whether they are different, but we never made a statement about the direction of the effect. This means we also have to consider the possibility of an effect in the opposite direction to what was observed. 

To do this, we have to count up the cases that fall into the upper and lower tails of the distribution, where each tail is defined by the region that lies beyond the absolute value of the observed difference, on the positive and negative halves of the x axis:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept =  diff(descrip.stats$wght.mean), colour = "red") + 
  geom_vline(xintercept = -diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

When we do this, we find that `r nlower+nhgher` out of the `r nperm` permutations lie beyond the observed difference, and so the new p-value is *p*=`r (nlower+nhgher)/nperm` (`r 100*(nlower+nhgher)/nperm`%). This kind of test--where we look at both tails of the sampling distribution--is called a 'two-tailed' test. We discuss the reasoning for and against using a one- or two-tailed test in this week's self-directed practical.

#### The significance of p-values

What are we supposed to do with the finding *p*=`r (nlower+nhgher)/nperm`? This is the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true. The hypothesis under consideration is one of no effect, and so a low p-value can be interpreted as evidence for an effect being present. In our example, the low p-value is evidence for a difference among the population mean dry weights of purple and green morphs.

One question remains: How small does a p-value have to be before we are happy to conclude that the effect is probably present? In practise, we do this by applying a threshold, called a **significance level**. If the p-value is less than the chosen significance level we say the result is said to be **statistically significant**. Most often (in biology at least), we use a significance level of *p*<0.05 (5%). Why? The short answer is that this is just a convention. We will return to the use of p-values and the concept of statistical significance later.

#### The null hypothesis

Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to apply in a more complex setting. We are not expecting you be able to implement a permutation test yourself. We used it to demonstrate how frequentist statistics works. The details vary from one problem to the next, but ultimately, if we are using frequentist ideas we always have to find a way to do the following: 1) assume that there is actually no 'effect', where an effect is expressed in terms of one or more population parameters, 2) work out what would happen if we were to repeatedly take samples from a population in this hypothetical situation, 3) evaluate how likely our observation would be under the hypothesis of no effect.

When using frequentist statistics we are always asking what would happen if we continually sample from a population *in the absence of the effect we are interested in*. This idea of a hypthetical 'no effect' situation is so important that it has a special name; it is called **the null hypothesis**. Every kind of statistical test (in this course at least) has a very specific null hypothesis associated with it. You can only fully understand the resuts of a statistical test if you understand the null hypothesis it relies on. We will remind you about the null hypothesis and introduce the related concept of the alternative hypothesis at the end of this chapter.

## Hypotheses and null hypotheses--why such a negative approach?

Previously we have said that an hypothesis is a statement of a proposed process or mechanism which might be responsible for an observed pattern or effect. We have also seen that in statistics, you will encounter 'hypothesis' used in a different, and quite specific way. In particular you will frequently see the term: *null hypothesis* (often written in statistics books as H~0~).

The null hypothesis is simply statement of what we would expect to see if there is actually no effect of the factor we are looking at (e.g., plant morphology) on the variable that we measure (e.g., dry weight biomass). So in the above example our null hypothesis was *There is no difference in mean biomass of purple and green plants*. When comparing the sizes of rats eaten by eagle owls, the appropriate null hypothesis was *There is no difference between the mean size of rats eaten by eagle owls in summer and winter*.

All statistical tests you are likely to encounter in biology work by specifying a null hypothesis and then testing the observed data to see if they deviate from the null hypothesis. This may seem like a rather odd approach, but there are good theoretical and practical reasons for doing things this way.

Although you need to be aware of what a null hypothesis is, and what it is used for, in general discussion of tests we will normally refer to the effect which is the opposite of the null hypothesis - i.e. it is the effect you are actually interested in - known as the *test hypothesis*,
or the *alternative hypothesis* (H~1~ in statistics books).

The alternative hypothesis is essentially a statement of the effect you are interested in testing for, e.g., Purple and green plant morphs differ in mean size; Eagle owls eat different sized Norway rats in summer and winter. It is a statement of whatever is implied if the null hypothesis is not true.

Having got all the types of hypothesis sorted out, we can then use a particular statistical technique (such as a t-test) to test the observed result against that expected if the null hypothesis was true. The test gives us a probability (p-value) telling us how likely it is that we would have got the result we observe if the null hypothesis was, indeed, true.

If the value is sufficiently small (conventionally if *p*<0.05) we judge it unlikely that we would have seen this result if the null hypothesis was true and consequently we *reject the null hypothesis* (i.e. reject the notion that there is no difference) and instead *accept the alternative hypothesis* (that there is a difference). Note that this is not the same as 'proving' the alternative hypothesis is true. You can't prove anything by collecting data or carrying out an experiment.

If the probability is large, then it is quite likely that we could have got the observed result if the null hypothesis was true, and in this case we cannot reject the null hypothesis. Note that in this situation we *'do not reject the null hypothesis'*, but this is not quite the same as accepting that the null hypothesis is true, paradoxical though this may seem. One obvious reason for this is that if we only have a small sample then there may be an effect of the factor we are looking at, but we simply can’t detect it because we don’t have enough data.

## A few final words about *p* values

It is important to understand the meaning of the probabilities generated by statistical tests. We have already said a p-value is the proportion of occasions on which you would expect to see a result at least as extreme as the one you actually observed if the null hypothesis (of no effect) was true. Conventionally (in biology at least) we accept a result as statistically significant if *p*<0.05 (also expressed as 5%). There is nothing special about this cut-off point. 

A probability of 0.05 is a chance of 1 in 20. This means that if there really was no effect of the factor we are investigating, we would expect to get a result significant at *p*=0.05 about 5 times in 100 samples. To envisage it more easily, it is slightly less than the chance of tossing a coin 4 times and getting 4 heads in a row (*p*=0.0625).

This puts a ‘significant’ result into context. Would you launch a new drug on the market or bring a prosecution for pollution on the evidence of the strength of four heads coming up in a row when a coin is tossed? Well of course such things are unlikely to hinge on a single test, but it is always worth bearing in mind what ‘significance’ actually means.

Of course the smaller the probability the more confident one can be that the effect we see is real. A probability of *p*=0.01 (1 in 100) is pretty good evidence, and *p*=0.001 (1 in 1000), or less, is better still. For this reason, in some critical applications such as drug testing the value set for accepting a result as significant may be lower (e.g. *p*=0.01). The costs of using a more stringent threshold is that this increases the possibility of false negatives (called a 'type II' error)--i.e. we are more likely to fail to detect an effect whne it is really present.

### What if is close to 0.05?

The thing to remember here is that although we tend to use *p*=0.05 as a cut-off, is really a continuous measure and *p*=0.055 is not very different from *p*=0.045. The exact value of will be affected by how well the data fulfil the assumptions of the test–-which will only be approximately with most biological data, so you shouldn’t set too much store by the difference between *p*=0.045 and *p*=0.055. It would be irrational on the one hand to reject an idea completely just on the basis of a result of *p*=0.055, while at the same time being prepared to invest large amounts of time and money implementing policies based on a result of *p*=0.045.

### Biological vs. statistical significance

A final, but vital, point: do not confuse statistical significance with biological significance. A result may be statistically highly significant (say *p*<0.001) but biologically trivial. To give a real example, in a study of the factors determining the distribution of freshwater invertebrates in a river, the pH of water was measured in the open water and in the middle of the beds of submerged vegetation. There was a statistically significant difference in pH (*p*<0.01) but the mean pH values were 7.1 in the open water and 6.9 in the weeds. This is a very small effect, and almost certainly of no importance at all to the invertebrates.

The significance of a result depends on a combination of three things (1) the size of the effect, (2) the variability of the data, (3) the number of samples. Even a tiny effect can be significant if the data have very little variation and the sample size is large. You should not automatically equate a significant result with a large effect--you need to inspect the means and consider the biological implications of the difference. The statistical results can give you some guidance in separating genuine differences from random variation, but they can’t tell you whether the difference is biologically interesting or important–-that’s your job!




<!--chapter:end:060_comparisons_p-values.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (PART) Introduction to Parametric Statistics {-} 

# What are "parametric statistics"?

Most of the statistical tools that we will teach you in this course are different examples **parameteric statistics** techniques. The word 'parametric' refers to the fact that most of the statistical models and tests that we will learn about are based on some kind of mathematical model of the population. The precise details of the models are defined by their parameters. We aren't going to study these models in any great detail, because this is supposed to be a practical course.

We will talk about the assumptions of the underlying models though. These are important and we can't afford to ignore them.

## Mathematical models {#math-models}

[[FINISH ME]]

## The normal distribution {#parametric-stats}

In this course, this assumption is essentially always the same: we assume that the variable follows a **normal distribution**. If you studied A-level statistics you should know all about this. If not, you may have come across it without realising: the normal distribution is sometimes called the 'Gaussian distribution' or more colloquially, 'the bell-shaped curve'.

Unfortunately, we don't have enough time in this course to really study the normal distribution in much detail. There are however, many good online resources to help you learn you about it if you want to know more. Here is one relatively non-technical introduction:

http://onlinestatbook.com/2/normal_distribution/normal_distribution.html


<!--chapter:end:070_parametric_statistics.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Standard errors and confidence intervals

## The standard error of the mean {#se-parametric}

Let's return to our example. In order to construct our bootstrap sampling distribution we pretended that the sample was the true population. A 'parametric' version of this procedure starts by making an assumption about the mathematical form of the population distribution of focal variable. We then use estimates of the parameters which describe the population to understand what would happen if we were to repeatedly resample it. Those estimates are derived from the sample.

[[NORMALITY AGAIN]]

So what can we say about the standard error of the sample mean, when it is calculated for a normally distributed variable? It turns out, that when we resample from a normally distributed variable, there is a very simple formula for the standard error of the mean. Here it is:
$$
SE = \frac{\text{Standard deviation of the sample}}{\sqrt{\text{Sample size}}} = \frac{SD}{\sqrt{n}}
$$
Notice that this only depends on the properties of the original sample, that is, the sample standard deviation and the sample size. This means that as long as we are happy with the normality assumption, we can go ahead and calculate a standard error for a point estimate of the mean without resorting to fancy tools like the bootstrap. Here is how to do this using R: 
```{r}
n <- length(purp.weights) # get the sample size
round(sd(purp.weights)/sqrt(n), 1)
```
That's all we need to do. This returns a value of 22.8, which is reassuringly close to the bootstrapped value of 22.3. They aren't identical, because each estimate of the SE relies on a different procedure, but they are very close.

<!--chapter:end:080_SE_and_CI.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# One sample *t*-tests

[[PREAMBLE]]

## The t-distribution

A statistician called W.G. Gosset showed that when we take samples from a normally distributed variable and calculate their means, the sampling distribution of these means has a particular form---it follows a Student's t-distribution. The same is true if you take two samples from a normal distribution, calculate their means, and then subtract these from one another. The sampling distribution of the differences among means also follows a Student's t-distribution.

(Why is it called Student's t? W.G. Gosset was a statistician employed by the Guinness Brewery, who published his statistical work under the pseudonym of 'Student'. He didn't use his real name because Guinness would have claimed ownership of his work.)

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's t-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a t-distribution.

We are not going to delve any deeper into the t-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the standard error of the difference. Different estimates of the standard error are available.

(This generates a test statistic)

4. Compare the test statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the observed difference.

Thsi procedure is called a two sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### So what exactly is a one-sample t-test?

The one-sample t-test allows us to compare the mean from a sample with an expected value. More precisely, it allows us to use the sample evaluate whether or not the unknown population mean is likely to be different from an expected value. This value might be something predicted from theory or some other prespecified value you are interested in. Here are some examples:

-   You have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If you have data on the actual time spent by 15 animals observed foraging in the patch, then you could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test.

-   You are monitoring sea pollution and have a series of water samples from along a beach. You wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach as a whole can be regarded as greater, or less than the legislated limit. You have a sample, which has variation, and a single value; a one-sample t-test will enable you to test whether the mean value for the beach as a whole, exceeds the limit (of course the question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue).

-   You are in charge of packaging seed samples from a horticultural firm, and your sales literature says that the packs will contain an average of 40 seeds. You select 20 packets off the production line at random and count the seeds in each. You could use a one-sample t-test to test whether this claim was true (or perhaps more importantly your competitors could!)

You can see that a paired-sample t-test, testing a set of observed differences against an expected value of zero (what we’d expect if there was no systematic difference across all the pairs), is simply another thing you can do with a one-sample test. It happens to be an extremely useful thing to be able to do, because it can allow you to design some very powerful experiments, and in practice you will probably end up using using one-sample tests for testing differences among pairs more often than any other use.

### Assumptions of the one-sample *t*-test

There are a number of assumptions that need to be met in order to use a one-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by the *t*-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It generally doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales. 

The one-sample *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the distribution of the sample. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

#### Assumptions: The example

[[WRITE THIS]]

### Carrying out a one-sample *t*-test in R

<div class="advanced-box">
#### A bit more about degrees of freedom
<div class="box-text">

[[UPDATE THIS]]

</div>
</div>

### Summarising the result of a one-sample *t*-test

[[REWRITE THIS SECTION]]

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistic should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = [[FIX ME]]). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.

<div class="warning-box">
#### p = 0.0000? It’s impossible! 
<div class="box-text">
Some computer packages (e.g. Minitab) will sometimes give a probability of p=0.000. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, you should never say this. If you actually know something is impossible for good biological, or other, reasons then you do not need to test it using statistics. When a computer package such as Minitab says p=0.000 it just means that the probability was 'very small'.

[[R SMALL p]]
</div>
</div>

<!--chapter:end:090_t-tests_one_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Two-sample *t*-test

[[PREAMBLE]]

## Using a *t*-test to compare two means

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
tmod.equlv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = TRUE )
tmod.diffv <- t.test(weight ~ pmorph,  data = morph.weights, var.equal = FALSE)
```

Think back to the permutation test we carried out in chapter [[LINK ME]]. In order to construct the test, we "pretended" that the two samples were both drawn from one population. We used the combined purple morph + green morph sample to mimic the process of drawing new samples from this hypothetical population, each time assigning arbitary labels to generate artificial observations. 

The two-sample *t*-test can be viewed as a "parametric" version of this procedure. It starts by making an assumption about the mathematical form of the population distribution. The key assumption is that the two samples are **normally distributed** in the population.

Actually, things are a bit more complicated than that, as we have to standardise the sample means (or their differences) by a standard error to arrive at Student's *t*-distribution, but the core idea remains: means and differences between means calculated from samples drawn from a normal distribution follow a *t*-distribution.

We are not going to delve any deeper into the *t*-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the *standard error of the difference*.

(This generates the *t*-test statistic)

4. Compare the test statistic to the theoretical predictions of the *t*-distribution to assess the statistical significance of the observed difference.

This procedure is called a two-sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### Assumptions of the *t*-test

There are a number of assumptions that need to be met in order to use a two-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because you can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by a *t*-test will not be reliable. Even mild non-independence can be a serious problem. This is why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale. It really doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales.

The *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. The 4^th^ assumption only matters if you plan to use the equal variance version of the two-sample *t*-test (the original Student's *t*-test). This version of the *t*-test is potentially a little more powerful than Welch's version, but not by much, and it is only correct if the population variances really are identical. Since we can never verify this, it is safer to just use the unequal variance version. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the sample distribution of each group. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

If you learned about the two-sample *t*-test at some point in the past you may have been told that variances of the samples need be the same. What about the **equal variance** assumption? This isn't really correct. The original version of Student's two-sample *t*-test was derived by assuming that the *population variance* of each group was identical, so it is the population variances, not the sample variances, that matter. This isn't the critical point though. What matters from a practical perspective is that R uses the "Welch" version of the *t*-test by default (Welch was another statistician, in case you're wondering). Welch's version of the two-sample *t*-test does not make the equal variance assumption, so as long as you stick with this version of the *t*-test, the equal variance assumption isn't one you need to worry about.

### Carrying out a two-sample *t*-test in R

You should work through the example in this section. If you haven't already done so, you will need to download the MORPH_WEIGHTS.CSV file from MOLE and place it in your working directory (this is the location you just set). Next, read the data in MORPH_WEIGHTS.CSV into an R data frame, giving it the name `morph.weights`:

```{r, eval = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

We looked at the sample distributions of the green and purple morphs in chapter [[LINK ME]]. It looks like they may have different variances, but as we have just seen, this isn't something we need to worry about. The sample size of each group is 25. This is fairly small (though not bad), so we should keep an eye on the normality assumption. The two dot plots we produced earlier suggest that there is nothing too 'non-normal' about their distributions, so it should be fine to go ahead and use a *t*-test.

It is very straightforward to carry out a two-sample *t*-test in R. We'll work with our plant morph example to demonstrate how to do it. The function we need to use is called `t.test` (no suprises there). Remember, we read the data into a data frame called `morph.weights`. This has two columns: `weight` contains the dry weight biomass of each plant, and `pmorph` is an index variable that indicates which group (plant morph) an observation belongs to. Here is the R code to carry out a two-sample *t*-test 
```{r, eval = FALSE}
t.test(weight ~ pmorph,  morph.weights)
```
We have surpressed the output for now as we want to focus on how to use `t.test` function. We have to assign two arguments (remember function arguments?--these control what a function does):

1. The first argument is a **formula**. We know this because it includes a 'tilde' symbol: `~`. The variable name on the left of the `~` should be the variable that contains the actual data (i.e. the numbers we want to compare). The variable on the right should be the indicator variable that says which group each observation belongs to. These are `weight` and `pmorph`, respectively.

2. The second argument is the name of the data frame that contains the two variables listed in the formula.

That's it. Let's take a look at the output:
```{r}
t.test(weight ~ pmorph,  morph.weights)
```

The first line reminds us what kind of *t*-test we have used. This says: `Welch two-sample *t*-test`, so we know that we have used the version of the two-sample *t*-test that accounts for unequal variance in the samples.

The next line just reminds us about the data. This says: `data: weight by pmorph`, which is R-speak for 'we compared the means of the `weight` variable, where the groups are defined by the values of the `pmorph` variable'.

The third line of text is the most important. This says: `t = 2.9381, df = 39.523, p-value = 0.005487`. The first part of this, `t = 2.9381`, is the test statistic (i.e. the value of the t statistic). The second part, `df = 39.523`, summarise the 'degrees of freedom'. This is essentially a measure of how much power our statistical test has (see the box below). The third part, `p-value = 0.005487`, is the all-important p-value. This says that there is a statistically significant difference in the mean dry weight biomass of the two morphs, because *p*<0.05.

The fourth line of text (`alternative hypothesis: true difference in means is not equal to 0`) just reminds us what the alternative to the null hypothesis is. We will discuss the meaning of this at the end of this chapter.

The next two lines show us the 95% confidence interval for the difference between the means. We don't really need this information, but you can think of this interval as a summary of the likely values of the true difference (a confidence interval is more complicated than that in reality).

The last few lines just summarise the sample means of each group. This is only useful if you did not bother to summarise these already (which you should always do!). 

<div class="advanced-box">
#### A bit more about degrees of freedom (again)
<div class="box-text">
In the original version of the *t*-test (which assumes equal variances) the degrees of freedom of the test are give by (n~a~-1) + (n~b~-1)  where n~a~ is the number of measurements from sample a and n~b~ the number of measurements from sample b. The plant morph data has measurements for 25 male and 25 females, so if we had used the original version of the test we would have (25-1) + (25-1) = 48 df. However, the R default version of the *t*-test reduces the numbers of degrees of freedom using a formula which takes into account the difference in variance in the two samples--the greater the difference in variances the smaller the number of degrees of freedom. This usually results in degrees of freedom that are not whole numbers.

In either case, a test with high degrees of freedom is more powerful than one with low degrees of freedom. 
</div>
</div>

### Summarising the result of a *t*-test

Having obtained the result we need to write the conclusion. Remember you are testing an hypothesis so go back to the original question to write your conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (t=2.94, df=40, p<0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (t=2.94, df=40, p<0.01)

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A t statistic can come out negative or positive in a test, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that matters, when presenting the results just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The t statistics should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, $df$ and $p$ are always best as lower case.

3.  **Should I use categories for $p$?** In this analysis R displayed the probability of our result to six decimal places (p = `r round(tmod.diffv$p.value, 6)`). Often however you will see results from tests presented as one of the following four categories: p>0.05, for results which are not statistically significant (sometimes also written as ‘NS’), and then: p<0.05, p<0.01, and p<0.001 for results of increasing significance. This style of presentation stems from the fact that, in the days before everyone had access to a computer, a statistic (like t) was often calculated by hand. The significance of the result was difficult to calculate directly, and so it would have been looked up in a special table. These days, a computer package can calculate the exact probability for you, and so there is no reason not to present the results as the actual p value. It is not wrong to use the four categories above if you wish to do so, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p=0.014 rather than p=0.047, though if categories were used both would simply appear as p<0.05. Similarly it can be informative to know that a test had p=0.06 rather than simply quoting it just as p>0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p<0.001.

4.  **When should I use asterisks instead of $p$ values?** You will also sometimes see the ranges of probabilities coded with asterisks: * for p<0.05...0.01, ** for p=0.01...0.001, and *** for p<0.001. This is common in tables and on figures as it is a more compact and visually obvious representation than numbers, but you would never use it in the text of a report.

<!--chapter:end:100_t-tests_two_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Paired-sample *t*-tests

[[PREAMBLE]]

## The paired sample t-test {#paired-t}

### What is a paired-sample design?

In the last session you carried out t-tests to compare two means. The two samples you had in each case were entirely separate sets of measurements (purple and green plant morphs, rat skulls from pellets collected in winter and summer, fungal infection in beans on two different soil types). One measurement in one data set (e.g. the first green plant) has no link to any particular measurement in the second data set—-the measurements are said to be independent. The particular flavour of t-test you used in this situation is called a two-sample t-test (logically enough).

However, there are situations in which data may naturally form pairs, i.e., the first value in a sample may be linked in some way with the first value in the second sample. This is known, not surprisingly, as a *paired-sample* design.

The commonest example of a paired sample design is the situation where you have a set of organisms, and you record some measurement from each organism before and after a treatment. For example, if you were studying heart rate in relation to position (sitting vs. standing) you might measure the heart rate of a number of people in both positions. In this case the heart rate of a particular person when sitting is paired with the heart rate of the same person when standing.

### What’s the point of a paired-sample design?

```{r, echo = FALSE}
drug_data <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```

A paired sample design can be very useful in biology because we often have the problem that there is a great deal of variation between individual organisms. In some cases there may be so much variation between the organisms within each sample that the effect of any difference between the samples is obscured.

Consider the following example. A drug company wishes to test two drugs for their effectiveness in treating a rare illness in which glycolipids are poorly metabolised. The company is only able to find 8 patients willing to cooperate in the early trials of the two drugs. The 8 patients vary greatly in their age, body weight, severity of symptoms and other health problems. Five are female and three are male.

If the group of 8 patients was randomly assigned to one or other drug treatments and their performance monitored it would be very difficult to detect significant differences between the treatments. This is because we have an experiment which provides very little replication, yet we should expect considerable variability from one person to another both in the levels of glycolipid before any treatment. There may also be problem if male and female patients respond differently (even if in the same direction), since one treatment will have more males and fewer females assigned to it.

One solution to all these problems is to treat each patient with both drugs in turn and record the glycolipid concentrations in the blood, for each patient, after a period taking each drug. We'll see why shortly.

The best arrangement would be for four patients to start with drug A and four with drug B, and then after a suitable break from the treatments, they could be swapped over onto the other drug. This would give us eight replicate observations on the effectiveness of each drug and we can determine for each patient which drug is more effective. This kind of experimental design is called a cross-over study. It can be problematic if, for example, "carry-over" effects occur, e.g., the effect of one drug is altered when the other drug has previously been administered. We won't worry about these problems here though.

The experimental design, and one possible outcome, can be schematically represented as in the diagram below... where each patient is represented by a number (1-9). The order does not matter, i.e. it doesn’t mean that Drug A was tested before Drug B just because Drug A appears first.

```{r drug-linked, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Data from glycolipid study, showing paired design. Each patient is denoted by a unique number.'}
ggplot(drug_data, aes(x = Drug, y = Glycolipid, 
                      group = Patient, label = Patient)) + 
  geom_line() + 
  geom_label(alpha = 0.5, size = 3)
```

Notice that there is a lot of variability both in the glycolipid levels of each patient, and also in the amount by which the drugs differ in their effects (e.g. for patient C the drugs have equal effects, while for patient A drug B is more effective).

What is obvious from this pattern is that although the glycolipid level varies a good deal between patients, as far as each individual patient goes, Drug B does appear to reduce glycolipid levels more than Drug A in all but one case (patient C). This is important information about the performance of the drugs.

The advantage to using a paired sample design in this case is clear if we look at the results we might have obtained on the same patients but by dividing them into two groups of four and giving one group Drug A and one group Drug B:

```{r drug-not-linked, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Data from glycolipid study, ignoring paired design.'}
ggplot(drug_data, aes(x = Drug, y = Glycolipid, label = Patient)) + 
  geom_point(size = 4, alpha = 0.5)
```

The patients and their glycolipid levels are identical to those in the previous diagram, but only patients B, D, E and H (selected at random) were given Drug A, while only patients A, C, F, and G were given Drug B. Clearly, if you calculated the means of the two groups, they would be very similar and a t-test of the two samples, given the variation between patients, would almost certainly find no significant difference between them.

So, it would be quite possible to end up with two groups where there was no clear difference in the mean glycolipid levels between the two drug treatments even though (as we have seen in the previous diagram) Drug B seems to be rather more effective in almost all patients. What the pairing is doing is allowing us to factor out (i.e. remove) the variation among individuals, and concentrate on the differences between
the two treatments. The result is a much more sensitive test.

### How do you do a t-test on paired samples?

It should be clear why a paired-sample design might be useful, but how do you actually do the test?

The thing to note here is that we can forget about the original data values for the two samples, and just concentrate on the differences between each pair of values. So in the case of the glycolipid levels
illustrated in the first diagram, we noted that there was a greater decrease of glycolipids in all but one patient using Drug B compared with Drug A.

If we had calculated the actual differences (i.e. subtracted the value for Drug A from the value for Drug B) for each patient we might have got something like...

    -3, -4,  0, -2, -3, -1, -2, -1

Notice that there isn't a single positive value in this sample of differences. The mean difference is -2, i.e. on average, for individual patients, glycolipid levels are lower with Drug B. Another way of stating this observation is that *within subjects* (patients), the mean difference between drug B and drug A is -2.  

If, on the other hand, the two drugs had had similar effects then what would we expect to see? Well obviously we would expect no consistent difference in glycolipid levels between the Drug A and Drug B treatments. Glycolipid levels are unlikely to remain exactly the same over time, but there shouldn’t be any pattern to these changes with respect to the drug treatment, some patients will show slight increases, some slight decreases and some no change at all. The mean of the differences in this case should be somewhere around zero.

So, what we do to carry out a t-test on paired-sample data is to find the mean difference of all the pairs and test this to see if it is significantly different from zero. You already know how to do this! **This is just an application of the one-sample t-test**. The thing to remember here is that although you started out with two sets of values, the data you are actually evaluating is the mean differences between pairs. This is just one set of numbers that you can think of as a sample of the differences.

When used to analyse paired data in this way, the test is sometimes referred to as a paired-sample t-test. This is not wrong, but it important to remember that a paired-sample t-test is is really it is just a one-sample t-test applied to the differences between pairs of associated observations. Just don't forget that a one-sample t-test can be used for things other than testing differences---it is a useful test in its own right.

Many computer packages do offer the option of a ‘paired sample t-test’ to save you the effort of calculating differences. The computer finds the differences between pairs for you as part of the test, but it is still just doing a one-sample test on those differences. Just remember, you could just calculate the differences yourself, then do a one-sample test.

R offers a dedicated procedure for doing paired-sample t-tests. We are still going to show you how to do it the ‘old fashioned’ way first---calculating the differences and running a one-sample test. This is a good idea because it will help you understand how the test works.

### Carrying out a t-test on paired-sample data using R

<div class="exercise-box">
#### Work through the example
<div class="box-text">
You should work through the one sample t-test example in this section. You will need to download the GLYCOLIPID.CSV file from MOLE and place it in your working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name `glycolipid`.
```{r, echo=FALSE}
glycolipid <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```
</div>
</div>

Staying with the problem of trials of two drugs for controlling glycolipid levels, the serum glycolipid concentration data from such a trial (not those used in the schematic illustration above) are stored in the file, 'GLYCOLIPID.CSV'. We have read this into an R data frame, giving it the name `glycolipid`. As always, we should start by looking at the raw data. We'll use `glimpse` to do this:
```{r, echo=FALSE}
glimpse(glycolipid)
```
You may also wish to examine the data with the `View` function to ensure that you understand how it is organised. 

First we need to calculate the differences between each pair. We can do this with the `dplyr` functions `group_by` and `summarise`:
```{r}
glycolipid.diffs <- 
  glycolipid %>%
  group_by(Patient) %>%
  summarise(Difference = diff(Glycolipid))

glycolipid.diffs
```
Don't worry too much if that looks cryptic to you. What we did was group the data by the values of `Patient`, and then used a function called `diff` (you haven't seen this before) to calculate the difference between the two Glycolipid concentrations. We stored the result of this calculation in a new data frame called `glycolipid.diffs`. This is the data we'll use it to carry out the paired-sample t-test.

<div class="advanced-box">
#### The assumptions of a paired-sample t-test
<div class="box-text">
The assumptions of a  paired-sample t-test are straightforward. There is no requirement for the original data to be normal, or have equal variances. The only assumptions are that the data are on interval or ratio scales and that the differences are approximately normally distributed. This is very useful because even where the original data are not normally distributed, the differences between pairs can often be acceptably normal.
</div>
</div>

Check, as far as you can, that the differences are not seriously non-normal in their distribution. Normality is quite hard to assess with only 8 observations though.

If the data seem OK then carry out a one-sample t-test on the calculated differences, where the null hypothesis is one where the population mean is zero. This is very easy to in R:
```{r}
t.test(glycolipid.diffs$Difference)
```
Notice that do not have to set the `data` argument to carry out a one-sample t-test on the differences. We just pass along one argument, which is numeric vector of differences, extracted from `glycolipid.diffs` using the `$` operator. If you find this confusing, try breaking the calculation up into two steps:
```{r}
gdiffs <- glycolipid.diffs$Difference
t.test(gdiffs)
```

The output has much the same structure as with two-sample t-tests...

The first line reminds us what kind of test we did, and the second line reminds us what data we used to carry out the test. 

It is the third line that really matters: `t = -2.6209, df = 7, p-value = 0.03436`. This gives the t-statistic, the degrees of freedom, and the all-important p-value associated with the test. Make sure you understand what the p-value is telling you.

The next line (`alternative hypothesis: true mean is not equal to 0`) reminds us that R has tested whether the population mean is equal to a value of zero, versus the alternative possibility that it is not equal to (i.e. greater or less than) zero.

We now need to express these results in a clear sentence incorporating the relevant statistical information to indicate whether we accept or reject our test hypothesis:

> Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A  (t = 2.62, d.f. = 7, p = 0.034).

There are a few things to point out in interpreting the result of such a test.

1.  By convention t-values are quoted as positive values whether the value of from the calculation is positive or negative — it is only the absolute size of that matters.

2.  The degrees of freedom for a one-sample test are one less than the number of differences (or number of pairs); not one, or two, less than the total number of data values.

3.  Note that since we have used a paired-sample design our conclusion stresses the fact that the use of the Drug B results in a lower glycolipid level in individual patients; it doesn’t say that the use of Drug B resulted in lower glycolipid concentrations for everyone given Drug B than for anyone given Drug A.

### The power of pairing

The paired t-test is a very powerful technique. You can get an idea of the value of the paired-sample test by seeing what would have happened if we had ignored the pairing structure of data and analysed it with an unpaired, two-sample approach (N.B.---This analysis is wrong!):
```{r}
t.test(Glycolipid ~ Drug, data = glycolipid)
```

<div class="well">
**MOLE question**

What result do you get and how does this compare with the paired-sample test?
</div>

### Using the `paired = TRUE` argument

As mentioned earlier R does have a built in procedure for doing paired sample t-tests. Now you’ve done it the hard way, try running the data you’ve just analysed through the test using that procedure to confirm it really does so the same thing. All you have to do is set the `paired` argument of the `t.test` function to `TRUE`: 
```{r}
t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE)
```
Notice that you work with the original `glycolipid` data frame, not the `glycolipid.diffs` data frame that we constructed above. R takes care of the differencing for us.

R certainly makes it easy to do paired sample t-test. It is up to you which method you use for doing standard paired-sample t-tests, just don’t forget it is really only a one-sample test wearing fancy dress.




<!--chapter:end:110_t-tests_paired_sample.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# One-tailed vs. two-tailed tests

[[PREAMBLE]]

## One-tailed *t*-tests {#one-tailed}

In all the *t*-tests we have considered so far we been testing hypotheses of the type: ‘male and female locusts differ in length’, ‘eagle owls capture rats of different sizes during summer and winter’, ‘two drugs differ in their effectiveness at reducing glycolipid concentrations’. In such cases we simply want to know whether there is any difference at all between our two samples. Before doing the test we didn’t have specific ideas about which sample mean should be greater than the other. If our test revealed a difference between samples we were interested in it, whichever way round it was. We learned during the permutation test example that a test of this sort is called a *two-tailed test*.

However, there are occasions when we may wish to test more specific hypotheses. For example if we have two samples A and B we might only be interested in whether the mean of A is bigger than the mean of B. Testing this sort of directional hypothesis can be done using a *one-tailed *t*-test*.

is that a one-tailed test is not a different sort of *t*-test from those you have already seen, it simply refers to the use of any of the *t*-tests (two-sample, paired-sample ...) to examine an hypothesis where the direction of the effect is specified (if this doesn’t make sense just now it should start to as we go on!).

### An example of a one-tailed hypothesis

A farmer has been persuaded to try a new pesticide called Toxic Death on his broad bean crop. He sprays 20 fields of beans with Toxic Death and leaves 20 fields untreated. To test the effectiveness of Toxic Death he is only interested in detecting a positive response in his crop. It makes no difference to the farmer if the pesticide has no effect or proves to reduce the crop yield: in either case the product is a waste of money to the farmer and he will not use it again.

The farmers’ test hypothesis is quite specific, in terms of the direction of the effect that is being tested for: ’Toxic Death increases the mean yield of broad beans’.

This is what is meant by a one-tailed test. In a one-tailed test we may be testing for a positive response, or for a negative response - but not both.

### So how do we perform a one-tailed *t*-test?

We are interested in testing the hypothesis: ‘Toxic Death increases the mean yield of broad beans’ since this is the hypothesis of greatest relevance to the farmer. If the yield of beans in the two treatments was as illustrated below...

```{r, echo=FALSE}
set.seed(27081975)
n_rep <- 20
trts <-  rep(c("No Toxic Death", "With Toxic Death"), n_rep)
df <- data.frame(Pesticide = trts, Yield = 4 + rnorm(n_rep, mean = c(0, 1))) 
ggplot(df, aes(x = Yield)) + 
  geom_histogram(binwidth = 0.5) + 
  facet_wrap(~ Pesticide, ncol = 1) +
  xlab("Yield (t/ha)")
```

...we would not even need to perform a test. The mean yield in the Toxic Death treatment is actually lower than in the control — we can automatically reject the hypothesis that treated fields have higher yield. The Toxic Death salesman might be in for some of his own medicine!

However, if the results indicated that the mean yield was greater where Toxic Death was used we would then want to perform the test to determine how confident we can be that this is a real increase rather than a chance outcome.

To carry out a one-tailed *t*-test we do exactly the same as we did for the previous *t*-tests, except that when we come to find the probability (*p*) value to judge the significance of the test, the correct probability for a one-tailed test is half that found for the two-tailed test.

So, suppose we had performed a two-tailed test (i.e. a test of the hypothesis "Toxic Death changes the yield of broad beans"" – no direction specified) and found a positive effect of Toxic Death, but with a probability *p*=0.08. Performing a one-tailed test of the hypothesis "Toxic Death increases the yield of broad beans" would give a probability of exactly half this (*p*=0.04).

In this case using the two-tailed test we would have concluded that there was no significant effect of Toxic Death on the yield of broad beans (*p*=0.08), whereas with the one-tailed test we would conclude that Toxic Death significantly increased the yield of broad beans (*p*=0.04).

This is a rather striking change of conclusion, which may seem like a fiddle. It is not a fiddle (at least not if used properly) but because using a one- rather than two-tailed test can alter the conclusion you draw, such tests should be used with caution, and the rules about how and when to use them strictly adhered to. These rules are discussed below, but first we will see how to actually do a one-tailed test in R.

### Carrying out one-tailed *t*-tests in R

Remember that we said one-tailed tests were not a different sort of *t*-test to those you’ve seen so far, you can do one-tailed tests with any of the *t*-tests you’ve seen so far. We'll show you how to do it using one example, the paired sample *t*-test, applied to drug data.

#### Doing a one-tailed paired-sample *t*-test

```{r, echo = FALSE}
glycolipid <- read.csv(file = "./data_csv/GLYCOLIPID.CSV")
```

Let’s go back to the data on glycolipid concentrations in eight patients being treated with Drugs A and B. Imagine now that rather than A and B being two new drugs, Drug A is the existing treatment for the disease, while Drug B is a new type of drug being tested for effectiveness. In this case the drug company is obviously only interested in whether the new drug causes a greater reduction in the glycolipid levels of individual patients than the old one. If it has the same effect, or if it is less effective than the existing treatment it will not be worth spending time and money developing to the production stage.

So the company’s test hypothesis is: ‘The new drug (B) is more effective than the existing treatment (A) at reducing glycolipid levels’. Let’s test this. In order to do this we have to set one more argument in the `t.test` function. This one, called `alternative`, can take one of three values: "two.sided", "less", or "greater". We pick a one-side test with an associated direction of effect by choosing "less" or "greater". Here is how it works:

```{r}
t.test(Glycolipid ~ Drug, data = glycolipid, 
       paired = TRUE, alternative = "greater")
```

Are you confused--why did we set the alternative to "greater"? We wanted to assess whether drug B really leads to **lower** glycolipid concentrations. Look at the `mean of the differences` in the output. R has assumed that we wanted to examine at the 'Drug A - Drug B' differences because Drug A appears first in the data frame. It doesn't actually matter which way round we calculate the differences--the t-statistic and p-value will be the same. However, we do have to be careful to make sure that the direction of the alternative hypothesis that we choose is correct. It is easy to get this wrong if you are not paying attention. This is why R always prints the means. This is also another reason why it is important to know your data before you start analysing it.

Compare this with the output from the previous (two-tailed) test on the glycolipid data. You should find that it differs in two respects..

1.  The hypothesis now specifies that it is a one-tailed test: means that R has tested whether the mean of our sample is greater than zero, as opposed to equal to or greater than zero. Remember, the differences are 'the wrong way around' so this is the right test.

2.  The probability is half the value it was before, i.e. the result is ‘more significant’ when done as a one-tailed test.

Our conclusion from this test would be:

> Individual patients had significantly lower serum glycolipid concentrations when 
> treated with Drug B than when treated with Drug A (one-tailed test: t=2.62, d.f.=7, p=0.017).

Note two things about the conclusion. First, you should specify that a one-tailed test was used. If no information is given it is conventional to assume a two-tailed test has was used. Second, it is sensible to put the actual probability level, rather than just giving the category for *p*. This is because if anyone does then want to see what the significance of the two-tailed test would have been, they can easily double the probability, which cannot be done if we just say *p*<0.05.

Also note that in this case, the drug effect was significant in both one- and two-tailed tests, but it is not always so.

### When to use, and not to use, one-tailed *t*-tests

As we saw above, whether you use a one- or two-tailed tests can sometimes appear to change our conclusions rather dramatically. There is an obvious temptation here! It would be easy to collect your data and see which mean value is larger and then test for differences in that direction using a one-tailed test, since this would increase the apparent significance of the results. Why? Well, if you do this you are implicitly doing a two-tailed test (you are going to test the effect whichever direction it goes in) while using the extra power of a one-tailed test by pretending only one direction is being considered. It is very important to get clear in your mind what one-tailed tests do and when (if ever) you might legitimately use them.

The key principle is that the direction of the predicted effect must be specified before the data are collected. You are then effectively forfeiting the right to test for differences in the opposite direction to that predicted. You are putting all your statistical eggs in one basket---if the result is the other way round you are saying you are not interested in testing it.

What this means is that one-tailed tests are often not that useful in investigative research. Just because you have an idea about which direction you think an experimental result might go in is not a good reason to just test for that and effect in that direction. Instead you need to ask whether or not you would genuinely be prepared to ignore a result in the other direction. Usually the answer is no.

For example, if we dose the soil in which experimental plants are growing with a low concentration of a particular compound we suppose will be toxic to them, we might expect that their growth will be reduced. However, if in fact they show higher growth with the compound than without it, we would almost certainly want to test to see if this was a genuine effect (perhaps the compound also contains important trace nutrients, or affects the microbial community in the soil) or whether the compound is really having no effect and the difference we see is just chance variation between the two samples. We would, therefore, be better off using a two-tailed test.

Similarly in the case of testing a new drug against an existing treatment, on the face of it we may primarily be interested in whether the new treatment is better than the old one, and might consequently think of a one-tailed test. However, we may also be interested in whether the new treatment is actually worse that the old one, rather than simply the same - i.e. an effect in the opposite direction to that we predict. Why? Well perhaps the new treatment has fewer side effects, so even if it is only of the same efficacy as the old one, it may still be preferable - but we would most definitely want to ensure that it was no worse! So a two-tailed test might still be the most appropriate analysis.

**So if there are so many problems why use them?** There are many situations where you are interested in the direction of the effects, rather than understanding mechanisms. Here one-tailed tests are a useful tool. Testing medical or veterinary products for efficacy might be one (as discussed above). Another is in situations such as industrial processes: if you are in charge of managing the production of blood test kits, and you are considering a change to the production process you might want to sample the production line under the old and new systems and test whether the new system has a lower failure rate. You are only interested in an improvement – if the change has a higher failure rate, or simply makes no difference, you are not going to convert the entire production process to the new system. Here the extra power to detect an effect in a specified direction would certainly be worth considering.

One-tailed tests have their uses (and you will see them in statistics books and used in biological studies, so you need to know what they are) but they should be used with caution. The default procedure should be to use a two-tailed test unless there are very good reasons for doing otherwise.

<!--chapter:end:120_one_vs_two-tailed_tests.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# (APPENDIX) Supplementary Material {-} 

# Exercises 

## Data and variables

### What kind of variable is it?

The following table gives a number of measurements taken in the course of a study of a woodland ecosystem. What type of variable results from the measurements taken in each case?

```{r, echo = FALSE}
table_data <- read.csv(file = "./tables_csv/variable_types.csv")
knitr::kable(
  table_data, booktabs = TRUE,
  caption = 'Examples of different kinds of variable.'
)
```

## Week2: Statistical concepts

### Sampling distributions

Spend a few minutes looking at the distribution of purple morph counts we simulated. See if you can answer the following questions:

*   What is the most common purple morph count we should expect to find in a sample of 20 individuals, when the population frequency is thought to be 40%?

*   Imagine someone told you that they thought 40% of the plants were purple. If you sampled 20 individuals and found that 4 of them were purple, would you be convinced of their assertion? What if you found only 2 purple plants?

#### How does sample size influence the standard error?

Think back to the purple morph / green morph example. We can use a simulation in R to calculate the (approximate) standard error of purple morph frequencies. For example, if we want to know the standard error when we use a sample size of 20, and the purple morph frequency is 40% (i.e., the proportion of purple morph plants is 0.4), we can use this snippet of R code:
```{r}
purple.prob <- 0.4
sample.size <- 20
samples <- 100 * rbinom(n = 100000, size = sample.size, prob = purple.prob) / sample.size
sd(samples)
```
You **do not** have to understand exactly how this works. If you did A-level statistics you might be able to guess what the `rbinom` function is doing. Essentially, what we did was to simulate the percentage of purple morph individuals found in 100000 samples of 20 individuals (stored in `samples`), and then used the `sd` function to calculate the standard error--remember, the standard error is the standard deviation of the sampling distribution of a statistic (= morph frequency). Ask a demonstrator if you want to know more. We just want you to use this R code to do the next exercise.

Use the above code to vary the sample size from 20 to 320, doubling the sample size each time (i.e., use samples of 20, 40, 80, etc). You only need to vary the value of `sample.size` to do this. Make sure you do this in your script, not at the command line. If you are feeling ambitious, store the results of your investigation in a data frame and use `ggplot2` to help you visualise them. You don't have to do this though; it is fine just to write down the numbers.

See if you can work out how the standard error changes as the sample size increases. Does the standard error halve when you double the sample size, or is the relationship more complicated? If you think the relationship is more complicated, what form does it take?

ANSWER:

The more important insight relates to the form of this relationship. What you should have noticed is that doubling the sample size does not halve the standard error. In fact, doubling the sample size only changes the standard error by a factor of 1/√2, which is less than 1/2 (don't worry if you did not spot this).

The somewhat depressing conclusion from this investigation is that we have to increase the size of a sample by a factor of 4 to halve the uncertainty associated with an estimate of a population parameter. This result isn't a peculiarity of the morph frequency example; it is very general.

### How big does a bootstrap sample need to be?

Use the R code above to investigate how large a bootstrap sample needs to be to reliably estimate the standard error. Start with a bootstrap sample size of 10, and gradually increase it, calculating the standard error each time you do so. How many bootstrap samples are needed to reliably estimate the SE to one decimal place?

## Week 3: *t*-tests

You should work through the exercises step-by-step, following the instructions carefully. At various points we will interupt the flow of instructions with a question. Make a note of your answer so that you can complete the associated MOLE quiz, which is called 't-tests 1'.   

### Eagle owls and Norway rats

```{r, eval = FALSE, echo = FALSE}
ratskulls <- read.csv("./data_csv/RATSKULL.CSV")
ggplot(ratskulls, aes(x = Weight)) + 
  geom_dotplot() + facet_wrap(~Season, ncol = 1)
t.test(Weight ~ Season, data = ratskulls)
```

A dataset containing information about the sizes of Norway rat skulls in the pellets of Scandinavian eagle-owls is available in the RATSKULL.CSV file (you may have come across this before). The data comprise a column of rat skull sizes (measured in grams) and a column of codes indicating the season when a particular skull sample was taken. These data were collected in order to evaluate whether there is a difference between sizes of rats eaten in summer and winter. That is, we want to know if there is a statistically significant difference between the mean rat skull sizes in the winter and summer samples.

Download the RATSKULL.CSV file from MOLE and **place it in your working directory** (this is the location you set at the beginning of this practical). Read the data in RATSKULL.CSV into an R data frame, remembering to assign the data frame a name. Ask a demonstrator to remind you how to do this if you have forgotten, or look over the instructions for reading in the MORPH_WEIGHTS.CSV file from the last practical.

As always, we should always start by *looking at the data* — both visually and in terms of its descriptive statistics:

**Inspection.** Use the `View` function and `dplyr` function `glimpse` to visually inspect the raw data. What are the names given to rat skull size variable and the season indicator variable? What values does the season indicator variable take?

**Descriptive statistics.** Use the appropriate `dplyr` functions (`group_by` and `summarise`) to calculate the sample size, sample mean and standard deviation of each sample. HINT: you will need to use the `mean`, `sd` and `length` functions to help you do this.

**Graphs.** Use `ggplot2` to construct a pair of dot plots, one above the other, to summarise the winter and summer skull size distributions. HINT: you will need to use `geom_dotplot` and the `facet_wrap` functions to do this; look over the plant morph example from the beginning of this practical to see how to use these.

Using the dot plots, and the descriptive statistics, conduct an informal evaluation of the assumptions of the t-test. You should re-read the relevant section above if you can't remember what these are.

<div class="well">
**MOLE question**

Do you feel the data conform acceptably to the assumptions? If not, explain why.
</div>

Let's carry on, assuming that we are confident that it is OK to use a two sample t-test to compare the sample means. Use the R `t.test` function to carry out this evaluation now.

<div class="well">
**MOLE question**

Write a concise but complete conclusion summarizing the results of the test.

Is this what you expected from looking at the distributions of data in the two samples?

Suggest two possible biological reasons for the result you observe.
</div>

### Fungal infection in French beans

```{r, echo=FALSE, eval=FALSE}
sandy <- c(2.3, 2.4, 2.5, 2.6, 2.8, 2.7, 3.1, 2.3, 2.5)
clay  <- c(2.3, 2.5, 2.8, 3.2, 2.9, 3.1, 3.2)
t.test(sandy, clay)
beans <- data.frame(glucosamine = c(sandy, clay),
                    soil = rep(c("sandy","clay"), c(length(sandy), length(clay))))
write.csv(beans, file = "./course-data/FRENCH_BEANS.CSV", row.names = FALSE)
```

A plant pathologist noticed that fungal infection in roots of French beans (*Phaseolus vulgaris*) was rather variable among crops and hypothesized that infection might be affected by the soil type: in particular whether the beans were grown on clayey or sandy soils. Root samples were taken from beans growing in each soil type and fungal infection was measured indirectly by measuring the amount of glucosamine in the roots. Glucosamine is a fungal sugar which is polymerised into chitin which forms the cell walls of most fungi.

The glucosamine concentrations ($\mu$g g$^{-1}$ root dry weight) recorded from the samples were:

  ------------ ----- ----- ----- ----- ----- ----- ----- ----- -----
  Sandy soil    2.3   2.4   2.5   2.6   2.8   2.7   3.1   2.3   2.5
  Clay soil     2.3   2.5   2.8   3.2   2.9   3.1   3.2        
  ------------ ----- ----- ----- ----- ----- ----- ----- ----- -----

Download the FRENCH_BEANS.CSV file from MOLE and place it in your working directory. Read the data into an R data frame (remember to give this a name!), inspect the data, generate some summary statistics (means and SDs) and then plot the data, just as you did in the last exercise. This should be quick to achieve--just copy and paste the code you produced, and edit this where required.

Use a t-test to decide whether there is a significant difference between the amount of infection of bean roots in the two different soils.

<div class="well">
**MOLE question**

Make a note of the results:

Mean for plants on clay soil = ?

Mean for plants on sandy soil = ?

t = ?

d.f. = ? 

p = ? 
</div>

<div class="well">
**MOLE question**

Write a statement of the result of the test suitable for inclusion in the results section of the plant pathologist's report.
</div>

### Sheep, grass and nature reserves

The management committee of a nature reserve wants to manage some large grassland areas of the reserve using low density sheep grazing to prevent the grass becoming too long and making the habitat unsuitable for some of the low-growing herbaceous plants for which the reserve is important. Before implementing the plan they conduct a pilot experiment using some fenced plots on the reserve, to test whether low density sheep grazing affects various species of plants.

One problem is that the area is very variable - some parts are wetter than others, and the plants of interest are not particularly evenly distributed. There is also a limit to the number of plots (and sheep) they can use in the experiment. In order to make the maximum use of the resources and, take some account of the variability in the habitat the experiment is set up by randomly placing eight fenced plots around the reserve, with each plot being divided in half by a fence down the middle. Sheep are introduced to one half of each plot (the half being randomly selected in each case), and allowed to graze for the appropriate period of the year. The other half is left ungrazed.

<div class="well">
**MOLE question**

Why is this a better design than just having separate grazed and ungrazed plots positioned at random?
</div>

Over the next 2 years, the abundances of various plants in the in the plots are surveyed.

The data below give the numbers of gentians from each of the eight half-plots with sheep, and the corresponding ungrazed halves after one year of the experiment. 

  ----------- -------- -------- -------- -------- -------- -------- -------- --------
  Treatment                                                                  
               Plot 1   Plot 2   Plot 3   Plot 4   Plot 5   Plot 6   Plot 7   Plot 8
  Grazed         27       1        16       8        10       19       30       9
  Ungrazed       14       6        17       5        0        11       21       6
  ----------- -------- -------- -------- -------- -------- -------- -------- --------
  
These data are stored in GENTIANS_GRAZING.CSV. Test whether there is any evidence for an effect of sheep grazing on the numbers of gentians.

<div class="well">
**MOLE question**

What is your conclusion?
</div>

<div class="well">
**MOLE question**

What other comparison would it be useful to be able to make in order to reach a satisfactory conclusion about the effects of grazing?  What test would you do for this?
</div>



## Regression


<!--chapter:end:910_exercises.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Tips and FAQs {#tips-faqs}

## Starting a new session

## Staying organised

## Fixing errors

## FAQs



<!--chapter:end:920_tips_faq.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
# Templates {#intro}

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].


<!--chapter:end:980_templates.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# Set plotting to bw plot default, but with transparent background elements. Note
# transparency requires the panel.background, plot.background, and device background
# all be set!

theme_update(plot.background = element_rect(fill = "transparent", colour = NA))


```
`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:999_references.Rmd-->

