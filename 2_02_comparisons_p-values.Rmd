# Comparing populations

## Making comparisons

Scientific enquiry often requires that we evaluate predictions about 'natural' or experimentally induced differences between populations. In its simplest form this involves just two populations:

‘Do male and female locusts differ in length?’

‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’

‘Do eagle owls feed on rats of different sizes during winter and summer?’

'Do purple and green plants differ in their biomass?'

What we're really evaluating is whether or not underlying *population parameters* are different in some way. In the previous chapter we wanted to know whether the purple morph frequency was different from 25%. In one sense, we were comparing two populations in this study, because the 25% number arose from observations of a neighbouring population. However, that number was also an estimate (though we never said how it was arrived at), which must carry with it some uncertainty. We should have used a methodology that accounted for this uncertainty to have made a completely reliable comparison.

In order to do this we have to step through the same kind of process discussed in the last few chapters. This chapter is demonstrates how to compare two populations by employing ideas like null hypotheses and *p*-values. The goal is not really to learn how to compare populations. Instead, we want to get a better sense of how these ideas can be used to construct significance tests and evaluate predictions.

First, we need to introduce a new example...

## A new example {#morph-weights-eg}

```{r, echo = FALSE}
morph.data <- read.csv(file = "./data_csv/MORPH_DATA.CSV")

sum_stat <- 
  morph.data %>% 
  group_by(Colour) %>% 
  summarise(mean = mean(Weight), sd = sd(Weight))
```

Let's step through the process introduced in the [Learning from data] chapter. We want to tackle the following question: "Is there a fitness difference between the purple and green morphs in the new population?" Based on various observations (that we've already discussed), our hypothesis is that purple plants are generally fitter than green plants. Since fitness is often strongly correlated with size in plants, we predict that purple morphs will be larger. That's our question-hypothesis-prediction sorted out.

What is (are) the population(s)? Our definition of needs to be different than before. This time we are going to conceive of each morph as a seperate population, i.e. there are two populations in this new problem. This change of focus is perfectly valid. Remember, statistical populations are not really concrete things. That's what we were getting at when we said statistical populations are defined by the investigator.

What variable should we study? One way to address the prediction of size differences would be to measure the dry weight biomass of individuals of each morph. That's a pretty reliable measure of how big a plant is. Dry weight is a numeric variable, measured on a ratio scale (zero really means 'nothing').

Which population parameter should we work with? Our prediction is that purple morphs will be larger than green morphs. What do we really mean by that? We probably don't mean that every purple plant is bigger than every green plant. That's a really strong precition, and in any case, it's not something we could ever validate with a single sample. Instead, we want to know if purple plants are generally bigger than green plants. This can be thought of as a statement about population means, i.e. we want to evaluate whether purple plants are larger than green plants, on average, in their respective populations. The population parameters of interest are therefore the population mean dry weight of each morph.

The next step is to gather appropriate samples. Since this is a made-up example, we'll just cut to the chase. You've already seen the samples we're going to use. When we read in the 'MORPH_DATA.CSV' in the previous chapter and looked over the variables we found it contained a numeric variable called `Weight`. That contains our dry weight biomass information, and the categorical `Colour` variable tells us which kind of colour morph each observation corresponds to. These data are **tidy** by the way---each observation is in a seperate row and each variable is in only one column.

```{block, type='do-something'}
You should work through this example from here onwards. Read the data into an R data frame using `read.csv`, assigning it the name `morph.data`. Check it over with `str` or `glimpse` again. Just do it. Yes, you already know about these data, but it is a good habit to get into.
```

The next step is to calculate the point estimate of the mean dry weight of morph. These are our 'best guesses' of the population means. It may be useful to know something about the variability of the samples as well though. We can summarise this with the standard deviation of each sample (these are not the standard errors!). Here is a reminder of how to do this using `dplyr`:
```{r}
morph.data %>% 
  group_by(Colour) %>% 
  summarise(mean = mean(Weight), 
            sd = sd(Weight),
            samp_size = n())
```
This shows that the mean dry weights of purple morphs is greater than that of green morphs. The standard deviation estimates from the two samples suggest that dry weights of purple morphs are also more variable than the green morphs. Remember, these numbers are just point estimates derived from limited samples. If we sampled the populations again, sampling variation would ensure that we end up with different estimates. We are not yet in a position to conclude that purple morphs are bigger than green morphs.

We'll move onto the main event---evaluating statistical significance of the difference in means---in the next section. As always, we should visualise our data. We could do this in a variety of ways, but since we only have two groups (purple and green morphs), we may as well summarise the full sample distribution of each morph weight. Here is some `ggplot2` code to make the required plot:

```{r two-morph-dist}
ggplot(morph.data, aes(x = Weight)) + 
  geom_histogram(binwidth = 50) + 
  facet_wrap(~Colour, ncol = 1)
```

(Hopefully this plot will also remind you how to use the `facet_wrap` function to make a multipanel plot based on the values of categorical variable---`Colour` in this instance).

What does this figure tell us? We are interested in the degree of similarity of the two samples. It looks like purple morph individuals tend to have higher dry weights than green morphs. Didn't we already know this? This was indicated by our comparison of the two sample means. However, that difference could have resulted from the odd outlier (an unusually large or small value). The histograms indicate there really is a general difference in the size of the two morphs. There is also a lot of overlap between the two dry weight distributions though, so maybe this difference is purely a result of sampling variation. We're going to need to employ some kind of statistical test...

```{block, type='do-something'}
**What do people mean when they 'compare samples'?

By comparing the central tendency (e.g. the mean) of different samples, we can evaluate whether or not something we have measured changes, on average, among different populations. We do this using the information in the samples (not just the means) to learn about the populations (the population parameters, in fact). It's quite common to use the pharse 'comparing samples' when discussing the statistical tests that underlie these efforts. This phrase is a little misleading though. When someone uses a statistical test to 'compare samples', what they are really doing is 'using information in the sample to compare population parameters'. This distinction might seem unnecessarily pedantic (it is a bit, to be honest). However, it is good to keep the *right* description in mind, as this helps us understand what statistical tools are really doing.

That said, saying / writing 'using information in the sample to compare population parameters' all the time is no fun, so we often revert to the phrase 'comparing samples'. We'll occasionally do it in this book, but try to keep in mind what we really mean when that phrase is used.
```

## Evaluating differences between population means

We are going to examine one method (there are others) for applying frequentist concepts to evaluate whether two population means are different or not. Specifically, we're going to use something called a permuation test to evaluate the statistical significance of the difference between purple and green morph mean dry weights.

```{r, echo = FALSE}
set.seed(27081975)
nperm <- 2500
perm.out <- numeric(nperm)
perm.eg <- list()
data.i <- morph.data
ids <- morph.data$Colour
for (i in 1:nperm) {
  morph.labels <- sample(ids, replace = FALSE)
  perm.out[i] <- 
    mutate(data.i, Colour = morph.labels) %>% 
    group_by(Colour) %>% summarise(mean = mean(Weight)) %$% diff(mean)
  if (i <= 3) {
    perm.eg[[i]] <- morph.data$Weight
    names(perm.eg[[i]]) <- morph.labels
  }
}
names(perm.eg) <- paste("Sample", 1:3)
```

In order to assess the strength of evidence for a difference between two population means, we have to do something that seems quite strange (ses, frequentist statistics is odd). We can break this down into four steps:

1. First, we assume that there is actually no difference between the population means. That is, we hypothesise that all the data are sampled from a pair of populations that are characterised by a single, shared population mean. You might recognise this trick. It's that **null hypothesis** again.

2. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation of 'no difference between samples'. We summarise this by calculating a sampling distribution of some kind of **test statistic**.

(We worked directly with the point estimates and their bootstrapped versions in the previous chapter. We dealing with more compicated statistical tests, we tend to work with some other kind of numeric quantity derived from the samples involved. The generic name for this is a test statistic.)

3. We then ask, "if there is no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we actually observed in the true sample?" You may also recognise this probability. It's a *p*-value.

4. If the observed difference is sufficiently improbable, then we conclude that we have found a statistically significant result. A statistically significant result is therefore one that is inconsistent with the hypothesis of no difference. You might recognise this logic from the previous chapter.

There are many different ways to go about realising this process. Regardless of the details, they all work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (e.g. a difference between means) *is absent*.

Let's return to our example to see how this might work in practise.

## A permutation test

In our example, a hypothesis of 'no difference' between the mean dry weights of purple and green morphs implies the following. If both morphs are sampled from the same population, the labels 'purple' and 'green' are meaningless. These labels may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows:

1. Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels 'purple' and 'green' to this new copy of the data. Do this in such a way that the original sample sizes are preserved.

(We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used, i.e. we want to hold everything constant apart from the labelling of individuals. The process of assigning random labels is called permutation)

2. Repeat this permutation scheme until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient.

3. For each permuted sample, calculate whatever sample statistic is of interest. In this case, we want the *difference* between the mean dry weight of purple and green morphs in each sample.

(It doesn't matter which way round we do this. We're going to use mean(purple) - mean(green) when we do this below.)

4. Compare the observed sample statistic---the difference between the mean dry weights of purple and green plants in the true sample---to the distribution of sample statistics from the randomly permutated samples.

This scheme is called a permutation test, because it involves random permutation of the group labels. Why is it useful? *Each unique random permutation yields an observation from the sampling distribution of the difference among sample means, under the assumption that this difference is really zero in the population.* We can use this to assess whether an observed difference is consistent with the hypothesis of no difference, by looking at where it lies relative to this sampling distribution. 

We have implemented a permutation test in R using the purple/green morph dataset for you, using `r nperm` permutations. We won't show the code because it uses quite a few new R tricks, and they won't be needed again. But we can look at the first 50 values of the first two permuted samples to get a sense of how this works: 
```{r, echo=FALSE}
head(perm.eg$'Sample 1', 50)
head(perm.eg$'Sample 2', 50)
```
The data from each permutation are stored as numeric vectors, in which each element of the vector is named according to which morph type it corresponds to (these are the labels we referred to above). Notice that he set of numbers doesn't change among the two permuted samples. The only difference between them is the labelling. The difference between the mean dry weights in the first permutation is `r perm.out[1]`. This difference is `r perm.out[2]` in the second sample.

What we really care about here is distribution of these differences. This distribution is an approximation to the sampling distribution of the difference between means. You might want to read that last sentence a few times. Here is a histogram that summarises the `r nperm` mean differences from the permuted samples:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 6) + 
  geom_vline(xintercept = diff(sum_stat$mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

Notice that the distribution is centred at zero. This makes sense---if we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. What does the read line show? This is the value of the difference between the mean purple and green morph dry weights in the real sample. The relevant thing to pay attention to is the location of this observed difference within the distribution of differences.

```{r, echo=FALSE}
nhgher <- sum(perm.out >= diff(sum_stat$mean))
nlower <- sum(perm.out <= -diff(sum_stat$mean))
```

What does this tell us? It looks like the observed difference is very unlikely to have arisen through sampling variation when the population means of the two groups were identical. We can say this because the observed difference lies at the end of one 'tail' of the sampling distribution. Just as with the bootstrapping example in the previous chapter, we want to make a more precise statement than this though.

[HERE]

Only `r nhgher` out of the `r nperm` permutations ended up being equal to, or 'more extreme' (i.e., more positive) than, the observed difference. The probability of finding a difference in the means equal to or more positive than the observed difference (denoted *p*) is therefore, *p*=`r nhgher/nperm` (`r 100*nhgher/nperm`%). This probability has a special name. It is called the __*p*-value__. A *p*-value is defined as the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true.

We have to be careful at this point. The test we just did is called a 'one-tailed' test, because we only looked at one end (the tail) of the sampling distribution. However, we did not set out to test whether purple plants were smaller on average than green plants. We set out to assess whether they are different, but we never made a statement about the direction of the effect. This means we also have to consider the possibility of an effect in the opposite direction to what was observed. 

To do this, we have to count up the cases that fall into the upper and lower tails of the distribution, where each tail is defined by the region that lies beyond the absolute value of the observed difference, on the positive and negative halves of the x axis:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept =  diff(sum_stat$wght.mean), colour = "red") + 
  geom_vline(xintercept = -diff(sum_stat$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

When we do this, we find that `r nlower+nhgher` out of the `r nperm` permutations lie beyond the observed difference, and so the new *p*-value is *p*=`r (nlower+nhgher)/nperm` (`r 100*(nlower+nhgher)/nperm`%). This kind of test--where we look at both tails of the sampling distribution--is called a 'two-tailed' test. We discuss the reasoning for and against using a one- or two-tailed test in this week's self-directed practical.

## What have we learned?

Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to apply in a more complex setting. We are not expecting you be able to implement a permutation test yourself. We used it to demonstrate how frequentist statistics works for making comparisons. 


