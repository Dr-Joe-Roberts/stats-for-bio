# ANOVA for randomised block designs

> Block what you can; randomize what you cannot.
> 
> --George Box

## Two-way ANOVA without replication {#no-replication}

The two-way ANOVAs we have seen so far follow a similar design: two factors, each having two or more levels, with replicate measurements within each treatment combination:

|              | **Factor A** | Level 1 | Level 2 | Level 3 |
|-------------:|:-------------|:--------|:--------|:--------|
| **Factor B** | Level 1      |  1,2,3  | 1,5,9   | 2,6,8   |
|              | Level 2      |  3,4,2  | ...     | ...     |
|              | Level 3      |  4,7,9  | ...     | ...     |

It is possible, however, to have a two-way factorial design with only a single measurement within each treatment combination:

|              | **Factor A**  | Level 1 | Level 2 | Level 3 |
|-------------:|:--------------|:--------|:--------|:--------|
| **Factor B** | Level 1       |  1      | 1       | 2       |
|              | Level 2       |  3      | ...     | ...     |
|              | Level 3       |  4      | ...     | ...     |

What's this... no replication? Isn't that a problem? In fact there is replication of a sort for each level of the factors. For example there are 3 values for each level of Factor B, it’s just that each value is at a different level of Factor A. This means we can still compare the means for the different levels of each treatment using an ANOVA---we can analyse the main effects. What we can't do is analyse the interaction. It is easy to see why---the interaction is based on analysing the differences between the means and variances of the data *from each individual cell in the table* (= each combination of the two factor levels), but if there is only one value in a cell then clearly means and variances cannot be calculated.

It's important to realise that just because we can't test for the interaction, it does not mean there is no interaction effect between the two factors. We simply can’t detect it even if it is there. However, as long as we have a balanced, orthogonal design, the significance tests of main effects will still be meaningful. If the balanced and orthogonal criteria are not met, then we do have to be very careful about how we interpret a significant main effect in the absence of replication, because the presence of an interaction might generate a spuriously significant main effect.

OK, so it is possible to analyse a two-way ANOVA without replication, but is it useful?

## Randomised complete block designs {#randomised-block} 

Having replication at each combination of factor levels is something to try and achieve because it will maximise statistical power. It also makes for a more robust experiment---if we have only one replicate for a combination and we lose it then we have no information at all from that combination! If we lose just one of a number of replicates in a particular combination it may be a nuisance, but unlikely to ruin everything. Replication is always a good thing. However, there are certain situations where replication within factor combinations is not needed. These arise when we adopt a blocked design experimental arrangement. 

In experimental work blocking is used to control for unwanted sources of variation that cannot be dealt with directly, but which could influence the results. The principles of using blocking was considered discussed in the [Principles of experimental design] chapter, but let's remind ourselves what these were anyway.

One of these is a particular type of Block Design (RCBD). The defining feature of the Randomized Complete Block Design is that each block sees each treatment exactly once

In the most basic RCBD, say testing different fertilizer application rates, one factor is the set of treatment which is the subject of the experiment (three levels of fertilizer application rate) the other factor (the ‘block’) is the source of unwanted variation (say different positions in a field). The essence of the design is that a set of locations are chosen in the field, which may differ in various unknown conditions (soil water, aspect, disturbance, etc.) and at each location, three plots are set up and each receives one of the three fertilizer rate treatments. So at each location, each treatment level is represented, but only once. Why only once? Well one reason is that it can be difficult to get true replicates – each plot occupies a specific bit of ground so adding more plots will involve using different bits of the field anyway. Another reason is that if we are interested in the overall effect of fertilizer we would probably prefer to put our effort into including a range of possible environmental conditions, rather than just doing the experiment in one corner of the field which might turn out to be rather unusual. We are not interested in the environmental variation as such, we just want to know for a range of conditions, whatever they might be, whether there are consistent differences between fertilizer application rates.

### Damselfly larvae and midges: an example of a randomised block ANOVA

A freshwater ecologist wants to assess whether there is a difference in the impact that the predatory larvae of three damselfly species (*Enallagma*, *Lestes* and *Pyrrhosoma*) have on the abundance of midge larvae in a pond. He plans to conduct an experiment in which small (1 m^2^) nylon mesh cages are set up in the pond. All damselfly larvae will be removed from the cages and each cage will then be stocked with 20 individuals of one of the species. After 3 weeks he will sample the cages and count the density of midge larvae in each. He has 12 cages altogether, so four replicates of each of the three species can be established.

On the face of it this looks like a straightforward one-way design, with each species as a treatment. The only problem to resolve is how to distribute the enclosures in the pond. Obviously the pond is unlikely to be uniform in depth, substrate, temperature, shade, etc... . Some of the variation will be obvious, some will not. The variations almost certainly cause the density of midge larvae to vary around the pond.

Even if the cages are distributed at random it is still quite possible that we will end up with some, or all, of the four replicates for a species occurring close to each other in one area. For example, in the illustratory figure below (left) the *Enallagama* cages (labelled ‘E’) are largely towards the left hand bay of the pond, which may, for example, be shaded, deeper and perhaps have lower densities of midge larvae than other areas. If this is the case, then at then end of the experiment the mean midge larva density in the *Enallagma* treatment will be lower than the other treatments even if *Enallagma* eat midges at exactly the same rate as the other species. To get round this, it might be more sensible to design our experiment to try and take account of the possible variation in the midge density (or other factors) by setting out the cages in groups (these are the 'blocks'). This arrangement is shown in the illustratory figure below (right).

```{r, echo=FALSE}
embed_png("figures/damselfly-expt-layouts.png", dpi=450)
```

Each block contains one representative of each treatment. The blocks should be placed at random, and within each block the three treatments should be randomly allocated to cages. The advantage of this design is that within each block the three cages should be in reasonably similar environments. There may be considerable differences between blocks, but we can account for this in the analysis so that it doesn’t obscure any differences between the treatments.

Conceptually a randomised block approach is the equivalent of the paired sample *t*-test, but it is not restricted to just two levels. It effectively allows us to analyse the differences between treatments
ignoring the variation between blocks. Randomised block versions of two-way (or higher order) designs are also possible, but we won't discuss these.

Carrying out an analysis of a randomised block design in R is straightforward and only slightly different from the ANOVAs we have just done.

### Analysing a randomised block design using R

The data from the damselfly experiment are in a file called DAMSELS.CSV. The density of midge larvae in each enclosure, after running the experiment for 3 weeks, are in the `Midge` variable (number m$^{-2}$), codes
for species in the `Species` variable  (levels: *Enallagma*, *Lestes* and *Pyrrhosoma*), and the block identities (A, B, C, D) in the third column.

<div class="exercise-box">
#### The diets example
<div class="box-text">
we should work through the damselfly example from this point. we need to download the DAMSELS.CSV file from MOLE and place it in your working directory. Read the data in DAMSELS.CSV into an R data frame, giving it the name `damsels`. Make sure we use `View` to look over the data before we proceed.
```{r, echo = FALSE}
damsels <- read.csv("./course-data/DAMSELS.CSV")
```
</div>
</div>

Checking the assumptions of ANOVA with this design is not straightforward becasue there is no replication within treatment combinations (strictly speaking, a blocking factor is not a treatment). We can't plot summaries of distributions when there is only a single observation! There are ways to evaluate the ANOVA assumptions with this kind of design, but we won't learn about these until a later session. For the purposes of doing this particular problem, assume the data fit the assumptions.

The process of analysing a randomised block design experiment with ANOVA is essentially the same as any other type of ANOVA. First we fit the model using the `lm` function and then we use the `anova` function to calculate *F*-statistics, degrees of freedom, and *p*-values:
```{r, echo = FALSE}
damsels.model <- lm(Midge ~ Species + Block, data = damsels)
anova(damsels.model)
```

We have surpressed the output for now. Notice that we only specify the two main effects (damselfly species and blocks) since we cannot test for the interaction. If we put an interaction term in the model, `lm` will fit the model it but the results from `anova` will be useless:
```{r, echo = FALSE}
wrong.model <- lm(Midge ~ Species + Block + Species:Block, data = damsels)
anova(wrong.model)
```
Look at the residual degrees of freedom (these are the error degrees of freedom). The model we just fitted is called a saturated model---there are zero degrees of freedom left over after fitting the three terms, so we can't calculate an error sum of squares, which means we can't calculate mean squares or F-ratios. This demonstrates---in case we weren't convinced---that there really is no way estimate an interaction in ANOVA when there is no replication at the level of each treatment combination.

(N.B. In the case of a more complex design than this using blocks (with more than one treatment in addition to the blocks) it there is an additional complication we have to consider: the question of whether the
block effects are what is termed *fixed* or *random*. The issue of random and fixed effects is explained in a little more detail in this week's extra reading, but often with randomised block designs, it is
appropriate to treat the blocks as random factors. In the case of a simple design like this one (just one treatment and one set of blocks) we do not have to make that decision as the results are the same in either case.)

Let's look at the results of the correct ANOVA applied to our randomised block experiment:
```{r, echo = FALSE}
damsels.model <- lm(Midge ~ Species + Block, data = damsels)
anova(damsels.model)
```
There are only two terms in the ANOVA table, as we expect. By this point we should be able to interpret these.

<div class="well">
**MOLE question**

What do we conclude from the ANOVA results?
</div>

It is worth looking at what happens if we analyse the damselfly data as though they are from a one-way design. We do this by including only the experimental treatment term (Species) in the model:
```{r}
damsels.oneway <- lm(Midge ~ Species, data = damsels)
anova(damsels.oneway)
```
Look at the degrees of freedom and the sums of squares of the residual (error). How do these compare to the previous model that accounted for the block effect? The degrees of freedom is higher---in principle, this is a good thing, because it means we have more pawer to detect a significant difference among the treatment means. However, the error sums of squares is also much higher when we ignore the block effect. We have have accounted more much less variation by ignoring the block effect. As a result, the error mean square is much lower, and so the *F*-statistic associated with the treatment effect is also much lower. The take home message is that designing a blocked experiment, and properly accounting for the blocked structure, will (usually) result in a more powerful analysis. We discuss again below.

### Multiple comparisons anyone?

In a randomised block analysis we are not usually interested in investigating significant block effects - the primary role of the blocking is to remove unwanted variation that might obscure the differences between treatments. R automatically gives us the a test of the block effect, and if it is significant it tells us that using the block layout has removed a considerable amount of variation (though even if the result isn’t quite what would conventionally be regarded as significant, i.e. if is not as low as 0.05, then the
blocking may still have been helpful). For this and other, technical, reasons it is not usual to carry out multiple comparisons between the block means. If the treatment effect is significant, multiple comparisons can be done between the treatment means using the Tukey test, as before.

## Are there disadvantages to randomised block designs?

The short answer is no, not really. There are instances when a randomised block design might appear to be disadvantageous at first glance, but these don't really stand up to critiscism:

1.    What if we were interested in knowing whether there is an interaction between the levels of the block and the treatments? For example, in the damselfly experiment we might be interested to know whether, if the damselfly species have differing effects on the midge densities, these effects are consistent in all habitat areas (e.g. some species may forage more effectively in muddy areas, others where there are more leaves). If this is the question we are trying to answer, then we should really design a different experiment. For example, a two-way design (which might also include blocking) in which we consider treatment combinations of different midge species and habitat characteristics might be appropriate. Fundamentally, the goal of blocking is to account for *uncontrolled* variation. Designing a blocked experiment and then lamenting the fact that we can't fully evaluate differences among blocks is a good example of trying to "have your cake and eat it too". 

2.    If the blocking term is having no effect in accounting for some of the variation, then the analysis may be slightly less powerful than just using a one-way ANOVA. This is because there are fewer error degrees of freedom associated with the blocked analysis (we lose a few to the block effects). This argument only works if the block effect accounts for very little variation. We can never know before we start an experiment whether or not blocking is needed, but we do know that biological systems are inherently noisy, with many sources of uncontrolled variation coming into play. In the majority of experimental settings (in biology at least) we can be fairly sure that blocking will 'work'. If we choose not to block your experiment there is no way to account for uncontrolled variation, and we will almost certainly end up losing statistical power as a result.

The advice contained in the quote at the beginning of this chapter is probably the best experimental design advice you will ever receive: "Block what you can; randomize what you cannot."
