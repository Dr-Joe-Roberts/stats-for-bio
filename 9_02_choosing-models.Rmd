# Choosing models and tests

## Introduction {#intro}

One of the more difficult skills in applying statistical models and tests is knowing which approach is the most appropriate for any particular situation. This book has introduced you to a range of different models and tests, and has demonstrated a variety of biological questions which can be addressed using these tools. Here we draw together the statistical tools we've encountered and explain how to match them to particular kinds of question or hypothesis.

### Do we need to carry out a statistical analysis?

This may seem like an odd question to ask, having just spent a considerable amount of time learning statistics. But it is an important one. There are many situations in which we don't need use statistical tools. Here are two common ones:

1. *There are no statistical procedures that will allow us to analyse our data correctly*^[Alternatively, an appropriate technique exists, but we don't know about it!]. This happens sometimes. Even with careful planning, things don’t always work out as anticipated and we end up with data that cannot be analysed with the technique we know about. If the data can't be analysed in a sensible way there is no point doing any old analysis just because we feel we have to  produce a *p*-value. If we can't analyse a data set with the methods we know about then it's time to seek some advice.

2. *We could quite correctly apply a statistical test to your data, but it would be entirely superfluous.* We don't always need to use statistics to tell you what is going on. The effect may be exceptionally strong and clear, or it may be that the importance of the result is not something captured by applying a particular statistical model or test^[As the old joke goes: *What does a statistician call it when ten mice have their heads cut off and one survives? Not significant.*]. This caveat is particulary relevant to exploratory studies, where the goal is to use the data to generate new hypotheses, rather than test *a priori* hypotheses.

Also in the category of superfluous statistics are situations where we are using statistics in a technically correct way, but we're evaluating effects that simply are not interesting or relevant to the question in hand. This often arises from a misplaced worry that unless we have lots of statistics in our study it somehow isn’t ‘scientific’, so we apply them to everything in the hope that your reader will be impressed by how many things you have tested. Resist the temptation! This strategy will have the opposite effect---a competant reader of your work will assume you don't know what you're doing if they see a load of pointless analyses.

The only thing that matters is that the tests you use are informative about the questions you are examining, and appropriate for the data you have.

## Before you start

There are a number of things we should always do before leaping into a statistical analysis:

1.    Be sure to look over the data after importing the data into R (`View` and `glimpse` is good for this). There are a number of things to look out for...
    
    - Understand how the data set is organised and be sure it is 'tidy'---each variable is in only one column and each observation corresponds to a row. The vast majority of statistical modeling tools in R expect the data to be organised in this format (as do **dplyr** and **ggplot2**). If it isn't already tidy, a data set will need to be rerognaised first. The **tidyr** package is really good at this.

    - Make sure there are no miscoded values. In an ideal world we should leave the source data alone and fix these miscoded values in the R script. Why? Because changing the source data is slow and runs the risk of introducing new errors. It's easy to edit an R script and rerun it. Editing source data is time consuming and error prone. If this is too difficult then fix these in the source data. Either way, there's no point starting an analysis until the data are error free. 
    
    - Check whether or not there are any missing values present in the data set. These appear as `NA` in R. If they are present, were they expected? If not, check the original data source to determine what has happened. Again, unless we're absolutely certain the missing values represetn an error in the way the data were coded, it is better to leave the source data alone and fix the miscoded `NA`s in the R script.
    
    - Check to see how R has encoded the variables in the data set (via `glimpse`). Pay close attention to the type of each variable (numeric, character, factor, etc). If a variable types are not appropriate for the planned analysis, make any necessary changes. For example, if we plan to treat a variable as a factor but it has been read in as a number, we should convert it to a factor before preceding.
    
2.    Spend some time thinking about the variables in your data set. Which ones are relevant to the question you are asking? If appropriate, decide which variable is the dependent variable (the 'y' variable) and which variable(s) is (are) the independent variable(s)? What kind of variables are you dealing with---ratio or interval scale numeric variables, ordinal or nominal categorical variables? If we can get these details straightened out it will often be fairly obvious which analysis options are available.

3.    Make *at least* one figure to visualise the data. We have done this throughout this book. This wasn't to fill the time---it is a crucial step in any data analysis exercise. Informative figures help us spot potential problems with the data. More importantly, they give us a way to evaluate our question before diving into to the statistical analysis. If we can't see an appropriate way to visualise the data, we probably aren't ready to start doing the statistics!

Steps 1-3 are all important parts of 'real world' data analysis. It may be tempting to skip these steps and just get on with the statistics, especially when pressed for time. Don't do this! The 'skip to the stats' strategy almost always leads to a lot of time being wasted, either because we fail to spot problems with our data or because we end up carrying out an inappropriate analysis.

## A key to choosing statistical models and tests

The choice of statistical model/test is affected by two things:

1.  The kind of question we are asking.

2.  The nature of data we have:

    -   what type of variables: ratio, interval, ordinal or nominal?

    -   are the assumptions of the statistical model likely satisfied by the data?.

The schematic key (below) provides a overview of all the statistical models and tests we've covered in this book, structured in the form of a key. The different choices in the key are determined by a combination of the type of question being asked, and the nature of the data you are dealing with.

<img src="figures/stats_key_solarized.svg" class="img-responsive">

The notes that follow the key expand on some of the issues summarized in the key and explain some of the tricker elements of deciding what to do. The key is quite large, so you may not be able to read it very easily a web browser (It also doesn't render very well in Firefox for some reason). If you do have trouble reading it [download a PDF copy of the key]({{ site.baseurl }}/figures/stats_key.pdf) or right click on the figure to open it on its own in a new tab and then zoom in. 

## Four main types of question {#four-questions}

**Question 1**: Are there significant differences between the means or medians ('central tendency') of measurements made on objects from one, two, or more groups, or between the mean of a sample and a single theoretical value?

This first question is relevant when we have measurements of one variable on each object (e.g. plant height) and objects are separated into distinct groups, where group membership is described by one or more categorical variables (i.e. a factor). This category of question includes anything where a comparison is being made between a sample which consists of measurements on a set of objects (e.g. weights of rat skulls, densities of bacteria in milk samples, concentrations of amylase in beans, numbers of flowers on plants), and...

-   a single theoretical value

-   another sample which has values that form logical pairs with those in the first sample

-   another sample which whose values are independent of those in the first sample (unpaired)

-   more than one other sample

The data for these situations may take a variety of forms: ratio, interval, or ordinal. The only type of data for which the statistical tools described here would not be suitable are categorical data.

**Question 2**: Is there a relationship between two variables? What is the equation that describes the dependence of one variable on the other?

Where Question 1 is concerned with comparing distinct samples, where we have measurements of one thing on each object (e.g., plant size) and objects are separated into distinct groups, Question 2 occurs where we have a set of objects on which we have two different measurements for each object (e.g., plant size and number of seeds produced). Here we are interested in asking whether the two variables are related to each other, i.e. is there a relationship between them, or are changes in one, unrelated to the other.

Here, again, the data that we need for this are either ratio, interval, or ordinal.

**Question 3** Is there a relationship between two variables AND does this relationship differ among two or more groups? Are the independent variables a mixture of categorical and numeric variables?

Question 3 is in some ways a mixture of questions 1 and 2. Sometimes we are concerned with comparing distinct samples, where objects are separated into distinct groups, but we want to understand: 1) how the relationship between one variable (the dependent variable) depends on another variable; **and** 2) how this relationship differs among groups. 

The data that we need for this are more restricted than questions 1 and 2. The dependent variable is either ratio or interval, the numeric independent variable must be ratio or interval, and the groups are defined by a categorical (nominal or ordinal) independent variable.

**Question 4**: Are there significant differences between the observed and expected number of objects or events classified by one or more categorical variables?

Question 4 is the only question which is focused on the analysis of categorical data. Here we have situations where the ‘measurements’ on objects can be things like colour, species, sex, etc. In these
situations we analyse the data by counting up how many of the objects fall into each category. The frequency of counts across the categories can then be tested against the pattern of frequencies in another data set, or against some predicted pattern.

## Question 1 –- Comparison of sample means or medians {#qu1}

### Question 1 How many samples?

Within the set of situations covered by Question 1, there are some further subdivisions: we need to decide whether: we have one sample only (and a theoretical or expected value to compare it with), we have two samples, or we have more than two samples. Usually this should be a fairly straightforward decision.

Confusion can sometimes arise is if you have a situation such as the one with *Festuca* experiment, where the data are classified by two factors (pH and and *Calluna*), and the factors each have two levels. What exactly do we mean by two samples? The answer is that if you have more than one factor, then you can think of the each ‘sample’ as being the set of data defined by each particular combination of the levels of each factor. So in the case of the *Festuca* experiment, there were four different combinations of the factors (with and without *Calluna*, at either pH level). The simplest thing to remember is that if you have more than one factor (regardless of how many levels the factors have) then you will have more than two samples.

### [Question 1] Single samples

If you have a **single sample** (**1a**) then the only thing that remains to be done is check the form of the data. If the data are suitable then the obvious test is a __one-sample *t*-test__. If the data are not suitable for a *t*-test, even after transformation, then we could also use a **Wilcoxon test** (we studied this in terms of a paired design, but remember, a paired experimental design is ultimately reduced to a one sample test).

### [Question 1] Two samples

If you have two samples then there is a further choice to be made: whether there is a logical pairing between data in the two samples, or whether the data can be regarded as independent. This sometimes causes problems, particularly where the pairing is not of the obvious sort. One useful rule-of-thumb is to ask yourself whether there is more than one way the data could be ‘paired’. If you find there is any uncertainty about how the pairing should be done (i.e. you can think of more than one way to (apparently) pair up the data), that is probably an indication that you don’t have a proper paired sample situation. The most common problem, however, is failing to recognize pairing when it exists. There is no real substitute for just getting practice at recognizing paired and non-paired situations.

If you have pairing then the test actually involves subtracting one sample from the other and testing the resulting sample to see if it is different from zero using a one-sample test, and this can be a __one-sample *t*-test__, or **Wilcoxon paired-sample test** depending on whether the data (the sample differences) are normally distributed or not.

If the data are independent then a __two-sample *t*-test__ or **Mann-Whitney -test** will be the best approach.

<div class="warning-box">
#### One-tailed or two-tailed tests?
<div class="box-text">
The tests discussed above can all be carried out as either one-tailed or two-tailed tests, depending on whether we are interested in effects in one direction or in both directions. This is not in the key because it is not relevant to the decision about which statistical model to use. Whether or not to use a one- or two-tailed test is a decision to take by thinking clearly about what we are trying to learn from the test. It is something to decide prior to carrying out an analysis and does not affect which of the above tests we choose.
</div>
</div>

### [Question 1] More than two samples

The first decision here is about the structure of the data. This sometimes causes problems. There are a variety of different situations in which we may be interested in testing for differences between several means. Most of the time these will involve either a one-way comparison in which each data value can be classified as coming from one type of treatment (factor), or a two-way comparison in which each value comes from a combination of two different types of treatments (factors). However, it is possible to make a mistake in recognizing which situation we have if we're not paying attention.

One way to try and establish the structure of the data is to draw up a table.

If the data fit neatly into the sort of table below, where there is one factor which has (for example) three treatments, with data replicated within each treatment. Then we have a *one-way design*. The treatments could be four different kinds of treatment (e.g. diets, types of fertilizer), or different levels of the same kind of treatment (e.g. concentrations of pesticide, temperatures). The only question it makes sense to address with these data is whether there are differences among the means of the three treatments.

| **FACTOR A**    |           | 
|:----------------|:---------:|
| **Treatment 1** | 1,4,6,2,9 |
| **Treatment 2** | 7,3,8,9,4 |
| **Treatment 3** | 5,3,7,6,4 |

If there are two different types of treatment (Factors A and B, e.g. light and moisture) and within each factor there are, say, three different treatments, which could be levels of the factor (e.g. light intensity) or different kinds of treatment of that factor (e.g. light of different wavelengths) then we have a *two-way design*. We could not draw up a table like the first one (above) and fit these data into it, because each observation occurs simultaneously in one treatment level from each of the two factors (below).

|                  | **Treatment B1** | **Treatment B2** | **Treatment B3** | 
|:-----------------|:-----:|:-----:|:-----:|
| **Treatment A1** | 1,4,6 | 3,9,1 | 2,2,7 |
| **Treatment A2** | 7,3,8 | 2,3,6 | 9,3,4 |
| **Treatment A3** | 5,3,7 | 1,8,6 | 2,2,6 |

From the data in this design, it clearly makes sense to ask questions about the difference among means of treatments of Factor A (rows), the differences among means of treatments of Factor B (columns), and the interaction – the differences among means for each different combination of treatments of Factors A and B (individual cells of the table).

If our data fit the second table, but there is only one value in each cell of the table, then you still have a two-way design, but now without replication. It should be possible to analyse this, but you will not be able to assess the interaction between treatments. We can only test the main effects with this kind of design. This design is most often associated with a Randomized Complete Block Design with one treatment factor (the thing we care about) and one bocking factor (a nuisance factor that is not of primary interest).

Having established whether we have a one-way or two-way design we need to determine whether the data are likley to satisfy the assumptions of the model we're presented with. We can start to make this evaluation by plotting the raw data (e.g. using a series dot plots). Sometimes it will be obvious at this stage that the data are not suitable for ANOVA. However, sometimes things are not soo clear. In this case we should fit the model and use regression 


In the case of a one-way design, you have the option of a non-parametric, **Kruskal-Wallis test** if the data are not suitable for ANOVA, otherwise a **one-way ANOVA** may prove the best solution. Remember, before turning to a Kruskal-Wallis test it is a good idea to see if the data can be made suitable for ANOVA by transformation.

In the case of the two-way design, you don’t have the option of a non-parametric test. If the data are suitable (or can be made suitable by transformation), then use a **two-way ANOVA**, otherwise you may have to break the analysis down into its component parts which can be analysed as one-way designs, giving you access to non-parametric methods. This isn't ideal though, as you will lose all information about the interactions if you do this (which somewhat defeats to purpose of designing a two-way experiment). 

(N.B. In the special case of a two-way design without replication, then there is a non-parametric test---**Friedman’s test**---that can be used instead of normal two-way ANOVA.)

Finally, you have the option of multiple comparison tests. You will only need to consider these if the ANOVA is significant. Additionally, in the two-way ANOVA, there is a further decision to be made, which is whether just the main effects (one or both of them) are significant, or whether the interaction is significant. If the latter then the multiple comparison should be done for the interaction means (i.e. the means in each treatment combination), if the interaction is not significant, then the significance of the differences between the main effect means (whichever are significant) should be evaluated.

## Question 2 -- Relationships between two variables? {#qu2}

Although it seems straightforward to choose between Question 1 and Question 2, it does sometimes cause a problem in the situation you have two samples, and the data are paired between the samples. Because two
sets of measurements on the same objects (say individual organisms) *could* apparently fit the structure a paired-sample *t*-test or a regression (or correlation), it is very important to identify clearly
*what effect it is you want to test for*. A concrete example will help make this clearer...

The place where confusion most easily arises is when the same variable has been measured in both samples. Imagine you have data on bone strength from males and females in twenty families, where the males and females are siblings – a brother and sister from each family. The pairing clearly makes sense – the siblings are obviously genetically related and likely to have grown up in similar environments. If you wanted to test whether males and females tend to have different bone strengths, then a paired-sample *t*-test makes sense: it compares males with females controlling (at least to some extent) for differences due to relatedness and environment. However, if you wanted to know whether bone strength seems to run in families (for genetic or environmental reasons) then the *t*-test wouldn’t help you. In this case you might want to see if there is a correlation in the bone strength of sibling pairs (i.e. if one sibling has high bone strength then does the other as well?). So while the data can be analysed in either way, it is the question you are asking that is the critical thing to consider. Just because data *can* be analysed in a particular way, doesn’t necessarily mean that analysis will tell you what you want to know.

One way to do tackle this sort of situation is to imagine what the result of each test using those data might look like and think about how you would interpret that result. Does the interpretation you get, answer the question you are trying to address?

But, assuming you have decided that it is the relationship you are after, not the difference between the samples, then the decision you need to make is whether it is a correlation or regression that you need.

### [Question 2] Testing $y$ as a function of $x$, or for an association between $x$ and $y$? 

The choice between regression and correlation techniques, for analysing the relationship between two variables, depends on the nature of the data *and* the purpose of the analysis. If the purpose of the
analysis is to find (and describe mathematically) the straight line relationship which describes the dependence of one variable ($y$) on another ($x$), then this points to a **regression** being the most appropriate technique. If you just want to know whether there is a linear relationship between two variables, then this would suggest **correlation**.

However, it is not just what you want that matters, unfortunately! The two techniques also make different assumptions about the data. Regression makes the assumption that the $x$-variable is measured with
little error relative to the $y$-variable, but doesn’t require the $x$-variable to be normally distributed. Correlation (parametric correlation) assumes that both $x$ and $y$ are normally distributed. So the final decision about which method to use may depend on trying to match up both the question and the nature of the data. It sometimes happens that you want to ask a question that requires a regression approach, but have data that are better suited to correlation. In this situation you might decide that it is worth proceeding with the regression, but you need to bear in mind that the answer you get may be less accurate than it should be (though careful use of transformations may improve things).

If you decide a correlation is appropriate, then the choice of parametric or non-parametric test should be based on the extent to which the data match the assumptions of the parametric test.

A final point here that can cause difficulties is the issue of dependence of one variable on another. In biological terms we are often interested in the relationship between two variables, one of which we
know is biologically dependent on the other. For example, tooth wear in a mammal is dependent on age, but age is not dependent on tooth wear. However, imagine that you have collected a number of samples of mammalian teeth from individuals of the species you are interested in which have died for various reasons, and for which you have an estimate of age at death. You may actually want to find the an equation for the
relationship between age ($y$) and tooth wear ($x$). Why? Well for example, in a population study you might often recover remains of individuals that have died, from which you can obtain the teeth (even if
not much else remains). It would be really useful to be able to use the measurement of tooth wear to estimate the age of the individuals, and to do this you want the equation that describes the ‘dependence’ of age on tooth wear. So here the direction of dependence in the analysis is not the same as the biological dependence of the two variables. The point is that the analysis knows nothing of the direction of biological dependence; it is up to you to do the analysis in a way that makes sense for the purpose of the study (and is compatible with the characteristics of data).

## Question 3 -– Relationships and groups {#qu3}

Although in some ways, two-way ANCOVA is the the most complicated statistical tool considered in this course, it will often be easy to spot a situation where it is needed. ANCOVA often crops up when we want to evaluate whether the relationship between two variables differs among groups. Just as with regression, one of these two variables has to act as the dependent variable ($y$) and another numeric variable ($x$) serves as the first of two independent variables. Both these numeric variables should be measured on interval or ratio scales. The second independent variable will be a categorical variable whose levels divide the data into groups (we will call it $T$). An ANCOVA really is like a cross between a regression and an ANOVA. It simultaneously evaluates whether: 1) on average, there is a relationship between $y$ and $x$ (the main effect of $x$), 2) whether the mean of $y$ is different among the groups (the main effect of $T$), and whether the relationship between $y$ and $x$ is different among groups (the interaction).

There are no non-parametric equivalents of ANCOVA (or at least, not simple ones), so if you find yourself needing to compare relationships between variables in different groups, your only option is to use ANCOVA. 

## Question 4 -– Frequencies of categorical data {#qu4}

This kind of question relates to categorical or qualitative data, (e.g. the number of male versus female offspring; the number of black versus red ladybirds, the number of plants of different species). The data are frequencies (counts) of the number of objects or events belonging to each category. The data are counts, never means.

The principle of testing such data is that the observed frequencies in each category are compared with expected (predicted) frequencies in each category.

Deciding between goodness of fit tests and contingency tables is generally fairly straightforward once you get used to recognizing whether data are classified by a single factor, or more than one factor.

If there is more than one factor, then you should be able to classify each observation (organism, event, habitat, location, etc.) into one category of each of the factors, and that allocation should be unique,
each observation should fit in only one combination of categories.

If it helps to think about it this way you can imagine your table as a series of boxes and you have to allocate each object to a box. No object can be in more than one box. If you just have one factor then you just need one row of boxes (below left), if you have two factors then you need two rows (below right) – so that each combination of factors can be represented by a box.

There is a further difference which may also affect your choice. In the case of a single factor, the question you can ask is whether the numbers of objects in each category differ from some expected value which you specify (it might be that there are equal numbers in each box, but could be something more complicated – it is entirely up to you). In the case of the two factor classification, the test addresses one specific question: is there an association between the two factors? The expected numbers are generated automatically based on what would be expected if the frequencies of objects in the different categories of one factor were unaffected by the other factor.

## Variables or categories? {#var-cat}

One final issue that sometimes causes questions is when a varaible is treated as categorical when it is really a continuous measure or when a continuous variable is made into categories. There is indeed some blurring of the boundaries here. Two situations are discussed below.

### ANOVA and regression

There are many situations in which data may be suitable for analysis by regression or one-way ANOVA, although they do not test exactly the same thing (similarly, we sometimes have to choose between two-way ANCOVA or two-way ANOVA---e.g. the limpets example). For example, if a farmer wishes to determine the optimal amount of fertiliser to add to fields to achieve maximum crop yield, he might set up a trial with 5 control plots and 5 replicate plots for each of 4 levels of fertiliser treatment: 10, 20, 40 and 80 kg ha NPK (nitrogen, phosphorus and potassium fertiliser) and measure the crop yield in each replicate plot at the end of the growing season (kg ha year).

If we are simply interested in determining whether there is a significant difference in yields from different fertiliser treatments, and if so which dose from the levels we have used is best, then ANOVA
(and multiple comparisons) is probably the best technique. On the other hand we might be interested in working out the general relationship between fertiliser dose and yield, perhaps in order to be able to make predictions about the yield at other doses than those we have tested. If the relationship between fertiliser and yield was linear, or could be made so by transformation, then we could use a regression to determine whether there was a significant relationship between the two, and describe it mathematically. 

(One additional potential advantage of regression in this kind of situation is that it *might* result in more a powerful statistical test of fertiliser effects than ANOVA. This is because a regression model only 'uses up' two degrees of freedom---one for each of the intercept and slope---while ANOVA uses four (number of treatments - 1). A regression makes stronger assumptions about the data though, because it assumes a linear relationship between crop yield and fertiliser)

### Making categories out of continuous measures

Sometimes you will have data which are, at least in principle, continuous measurements (such as the abundance of an organism in different samples), but have been grouped into categories (e.g. abundance categories such as 1-100, 101-200, 201-300, etc. ). One question is whether these count as categories – and hence whether for example you could look at the association between abundance categories and habitat by taking the frequencies of samples in each abundance category across two different habitats and examining the association using a contingency table test. The answer is that this would indeed be a perfectly legitimate procedure, and there are some situations in which it might actually be a more informative procedure than just comparing the mean abundances of the organism in the two habitats.

Sometimes you may start with actual measurements, and create categories out of these. In general, it is not a good idea to group numeric data into categories because this throws away information, but sometimes you may encounter situations where it may not be possible to make a proper measurement, and but you can make estimates, or divide the samples into categories. For example observing flocks of birds in the field, you might well find that (with a bit of practice) you can estimate the numbers of birds in a flock in abundance categories (1-100, 101-200, etc. ), even though it would be impossible to count them properly. And in fact when you start to think about it, many things we actually treat as categories could in principle be measured in a more continuous form: ‘unpigmented’, ‘lightly pigmented’, ‘heavily pigmented’; ‘yellow’, ‘orange’, ‘red’; ‘large’, ‘small’; ‘fertile soil’, ‘infertile soil’; all these and many others are convenient categories, but in one sense they are (or can be) fairly arbitrary. This doesn’t mean that you can’t analyse the data using these categories. However one thing to bear in mind is that if you are
dividing a continuous variable into categories then the decisions you make about how to do that, where to draw the boundaries, can affect the pattern of the results, and this may have implications for things such
as what the expected values are in a goodness of fit test.

And, of course, there are many ‘true’ categorical data: on the whole things like male/female, species, dead/alive, can be regarded as fairly unambiguous categories.

## ...And finally: getting advice — two golden rules {#advice}

So, you’ve worked through all the relevant information here, and you are still not clear about the best course of action with your data, you’ve hit a problem, or you’ve got to a point in the text where it helpfully says: “...seek some advice.”-—-how do you best do that? Once you have identified an appropriate source of advice, there are a couple of things to keep in mind to get the best advice, and to get it in a way that is efficient, both for you and the person you are asking.

The first golden rule is that you should have made a sensible attempt to do the analysis yourself and you should be clear about what the problem is, or why you think the techniques you know about are not appropriate. We all need to seek advice from someone else from time to time, and most people are happy to give advice, but you won’t get the best out of someone if you simply treat them as a substitute for straightforward work you could have done yourself. So, please don’t just sit down and say “I’ve generated all these data and I want you to tell me how to analyse them”!

The second thing to bear in mind is that you may have been have been thinking about, and working on, this problem for quite a while. You know (or should know!) exactly what you were trying to do in the study, and
exactly how the data were generated. However, the person you are going to talk to may not. So the second golden rule is that you need to have a clear explanation prepared which starts with what you are trying to do (what question you are trying to answer with the analysis) and how the data were generated (the experimental, or study, design). Don’t just start by putting down a spreadsheet of numbers on the desk and saying “I’m having problems analysing these data”. Actually looking at the details of the numbers is often the last thing necessary, if it is necessary at all. It is tempting to try and save time by just starting with the detail of the problem, but almost inevitably you will have to backtrack and explain the context and background, so have a clear explanation ready and start with that.

(Even if you are not sure how to analyse it, at the very least you should be able to visualise your data. You can often save everyone a great deal of time by preparing one or more figures that capture the key relationships you are concerned with)

As you will be aware from the examples we have considered through the course, the correct analysis for a data set is determined both by the form of the data *and* the particular question you are asking of the
data. The answer you get to your problem will depend on the person you are talking to understanding what that question is. The issue should not be “what can I do with these data?”, but “what is the appropriate thing to do with these data to address this particular question?”.

Do talk to people about what you are doing, both at the design and analysis stage of your work. It can help identify potential problems, and sometimes it will reveal interesting new possibilities that you might not otherwise have thought of. And if you get stuck, don’t be reluctant to seek advice – it is all part of the cooperative process that makes science work. The suggestions above should help make that
process as effective as possible.


