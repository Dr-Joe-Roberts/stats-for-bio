# One sample *t*-tests

[[PREAMBLE]]

## When do we use one-sample t-test?

The one-sample t-test allows us to compare the mean from a sample of a numeric variable with an expected value. More precisely, it allows us to use the sample evaluate whether the (unknown) population mean is likely to be different from an expected value. This value might be something predicted from theory or some other prespecified value you are interested in. Here are some examples:

-   We have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If we have data on the actual time spent by 15 animals observed foraging in the patch, then we could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test.

-   We are monitoring sea pollution and have a series of water samples from along a beach. You wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach as a whole can be regarded as greater, or less than the legislated limit. A one-sample t-test will enable us to test whether the mean value for the beach as a whole exceeds this limit^[The question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue].

-   We are in charge of packaging seed samples from a horticultural firm, and our sales literature says that the packs will contain an average of 40 seeds. We select 20 packets off the production line at random and count the seeds in each. We could use a one-sample t-test to test whether this claim was true (or perhaps more importantly our competitors could!).

## How does the one-sample *t*-test work?

[[SCHEMATIC]]

### Assumptions of the one-sample *t*-test

There are a number of assumptions that need to be met in order to use a one-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because they can't do much about it once the data have been collected---it's a consequence of how the data were collected. We're going to discuss the idea of independence later in the book. For now, we'll just point out that it matters and tell you why. If the data are not independent, then the *p*-values generated by the one-sample *t*-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well.

2. **Measurement scale.** The variable that you are working with should be measured on an interval or ratio scale, i.e. it should be a numeric variable. It generally doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales. 

3. **Normality.** The one-sample *t*-test will produce exact p-values if the variable is normally distributed in the population. However, this assumptions is less important than many people think. The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large the normality assumption matters even less^[It's hard to define what constitutes a 'large' sample, but 100s of observations would usually be safe. We don't have time to properly explain why the normality assumption doesn't matter too much for large samples, but we can at least state the reason: it is a consequence of the 'central limit theorem'.].

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the distribution of the sample. If the sample size is small, and each sample looks approximately normal when you graphically summarise its distribution, then it is probaly fine to use a *t*-test. If you have large samples, you don't even need to worry about moderate departures from normality--ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

[[WRITE THIS]]

## Carrying out a one-sample *t*-test in R

```{block, type='advanced-box'}
**A bit more about degrees of freedom**

[[UPDATE THIS]]
```

### Checking the assumptions

### Summarising the result

[[REWRITE THIS SECTION]]

When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not a conclusion.**

There are a number of common questions that arise when presenting *t*-test results:

1.  **Help - what do I do if is negative?** Don’t worry! A *t* statistic can come out negative or positive, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of *t* that determines the *p*-value, when presenting the results, just ignore the minus sign and always give as a positive number.

2.  **Upper or lower case 't'?** The *t* statistic should always be written as lower case when writing them in a report (as in the conclusions above). There are some statistics you will encounter later which are written in upper case but, even with these, d.f. and *p* are always best as lower case.

3.  **How should I present _p_?** 

[[FINISH]]

```{block, type='warning-box'}
**p = 0.0000? It’s impossible! p = 1e-16? What's that?** 

Some computer packages (e.g. Minitab) will sometimes give a probability of p=0.000. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, you should never say this. When a computer package says p=0.000 it just means that the probability was 'very small'.

R uses a different convention for presenting small probabilities. `p-value < 2.2e-16`
```
