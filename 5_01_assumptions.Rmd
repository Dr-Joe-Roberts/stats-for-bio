# Assumptions

We usually have an analysis in mind when we design an experiment or observational data collection protocol. It may be tempting to jump straight into this analysis without carefully examining the data first. This is never a good idea. In the past few chapters we have repeatedly emphasised that careful data analysis always begins with inspection of the data. Visualising a dataset helps us to understand the data and evaluate whether or not the assumptions of a statistical tool are likely to be violated.

As we learnt in the last section regression and one-way ANOVA are both types of linear model. They are parametric techniques and as such they make a number of assumptions that we need to be aware of. If our data do not meet the assumptions of the tests then we cannot rely on the results (i.e. the *p*-values) given by those tests. 

### Understanding data

We've been using 'well-behaved' data sets in this book so far, which tends to give the impression that visual inspections of the data are not all that necessary. Here's an example of why it matters. Imagine we are interested in quantifying the relationship between two variables, called $x$ and $y$. We might be tempted to carry out a linear regression analysis without first inspecting these data to get straight to 'the answer': the coefficients of the linear regression model. This could be very misleading. Take a look at these four scatter plots:

```{r, fig.align='center', fig.asp=1, fig.width=7, echo=FALSE}
rbind(select(anscombe, x = x1, y = y1) %>% mutate(case = "Case 1"), 
      select(anscombe, x = x2, y = y2) %>% mutate(case = "Case 2"),
      select(anscombe, x = x3, y = y3) %>% mutate(case = "Case 3"),
      select(anscombe, x = x4, y = y4) %>% mutate(case = "Case 4")) %>%
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~ case, ncol = 2)
```

These four artificial data sets were constructed by the statistician Francis Anscombe. The means and variances of $x$ and $y$ are nearly identical in all four data sets, and what's more, the intercepts and slopes of the best fit regression lines are almost identical (the intercept and slope are 3.00 and 0.500, respectively). The nature of the relationship between $x$ and $y$ is quite obviously different among the four cases:

1. "Case 1" shows two linearly related, normally distributed variables. This is the kind of data we often hope for in a statistical analysis. 
2. "Case 2" shows two variables that are not normally distributed, but there is a perfect non-linear relationship between the two. 

3. "Case 3" shows an example the variables are perfectly linearly associated for all but one observation which ruins the perfect relationship.

4. "Case 4" shows an example where a single outlier generates an apparent relationship where the two variables are otherwise unrelated.

Each of these plots tells a different story about the relationship between $x$ and $y$, yet the linear regression model says the same thing is happening in each case. These are obviously somewhat pathological examples, but they clearly illustrate the kinds of issues that can, and do, arise with real data. There is a real risk we will apply an inappropriate analysis if we fail to detect these kinds of problems. 

Every statistical model makes certain assumptions about the data^[Even so-called 'non-parametric' models have underpinning assumptions; these are just not as restrictive as their parametric counterparts.]. Even if a dataset doesn't exhibit the very obvious problems seen in the Anscombe examples, we still need to assess whether the assumptions of the statistical model we want to use are likely to be valid. For example, when working with a linear regression model, we started with a scatter plot of the response variable vs. the predictor variable. This allowed us to assess whether the two variables are linearly related. However, as we noted at the time linearity is not the only assumption we need to think about when carrying out a linear regression. In the rest of this chapter we'll go through the remaining assumptions for regression and also consider the assumptions for a one-way ANOVA. In the [Diagnostics] chapter we'll move on to how to check whether these assumptions are valid with your data.

## The assumptions of regression

Let's consider each of the assumptions of a regression, in their approximate order of importance:

1.    **Independence.** The residuals must be independent. Another way of stating this assumption is that the value of each residual does not depend on the value of any others. This can be difficult to check. If the data are from a carefully designed experiment, everything should be OK. If the data are observational, then we need to be a lot more careful. This assumption matters because when the residuals are not independent any *p*-values we generate will be unreliable.

2.    **Measurement scale.** The response ($y$) and predictor ($x$) variables are measured on an interval or ratio scale. It doesn't really make sense to use categorical data in a regression^[It sometimes makes sense to use a regression analysis when the predictor variable is an ordinal categorical variable. It depends what you want to do with the resulting model. However, some people disagree with this approach, so it's best to avoid doing it unless you're confident you can justify it.]. This one is easy to assess.

3.    **Linearity.** The relationship between the predictor $x$ variable and the response $y$ variable is linear. Obviously there is little point in fitting a straight line to data which clearly don’t form a straight line relationship. There may also be circumstances in which it is theoretically unlikely for a relationship to be linear, e.g. the length and weight of an animal will not be well described by a linear relationship because weight is roughly a cubic function of length. If the data fail this assumption then applying a mathematical transformation of $x$ or $y$ can help. We will come back to this idea in the [Data transformations] chapter.

4.    **Constant variance.** The variance of the residuals is constant. This assumption essentially means the variability of the residuals is not related to the value of the predictor $x$ variable. It is violated if the magnitude of the residuals increase or decrease markedly as $x$ gets larger. If the data fail this assumption then again, sometimes applying a mathematical transformation of $y$ will help. 

5.    **Normality.** The residuals are drawn from a normal distribution. This essentially means that for a particular value of $x$ we would expect there to be a range of responses in $y$ which follow a normal distribution. It is the distribution of the deviations of $y$ from the fitted line (the residuals) that are assumed to be normal, not the raw $y$ values. This means that we can generally only test this assumption *after* the line has been fitted. It does not make sense to evaluate this assumption by looking at the raw data.

6.    **Measurement error.** The values of the predictor $x$ variable are determined with negligible measurement error^[It is the measurement error, not the sampling error, that matters. This means it is fine to use regression when the $x$ variable represent a sample from a population.]. It is often hard to obtain the $x$ values with absolutely no measurement error, but the error $x$ in should at least be smaller than that in the $y$ values. So for example, in the thermal tolerance experiment the temperature values (set by the experimenter) almost certainly have little error, so it is appropriate to use regression.

Assumptions 1 (independence), 2 (measurement scale) and 6 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data. This leaves assumptions 3 (linearity), 4 (constant variance) and 5 (normality). We can check these assumptions using regression diagnostics, which we'll cover in the next chapter. 

## The assumptions of one-way ANOVA

As regression and ANOVA are both types of linear model it is unsurprising that the assumptions for these two types of model are similar. There are some differences however, for example the linearity assumption does not make any sense with an ANOVA as the response variable is not numerical. We'll now step through each of the assumptions of the ANOVA. 

1. **Independence.** If the experimental units of the data are not independent, then the *p*-values generated by an *F*-test in an ANOVA will not be reliable. This one is important. Even mild non-independence can be a serious problem.

2. **Measurement scale.** The response variable ($y$) should be measured on an interval or ratio scale.

3. **Equal variance.** The validity of *F*-tests associated with ANOVA depends on an assumption of equal variance in the treatment groups. If this assumption is not supported by the data, then it needs to be addressed. If you ignore it, the *p*-values that you generate cannot be trusted. There is a version of one-way ANOVA that can work with unequal variances, but we won't study it in this course.

4. **Normality.** The validity of *F*-tests associated with ANOVA also depends on the assumption that the residuals are drawn from a normal distribution. ANOVA is reasonably robust to small departures from normality, but larger departures can start to matter. Unlike the *t*-test, having a large number of samples doesn't make this assumption less important.

Strictly speaking, assumptions 3 and 4 really apply to the (unobserved) population from which the experimental samples are derived, i.e., the equal variance and normality assumptions are with respect to the variable of interest *in the population*. However, we often just informally refer to 'the data' when discussing the assumptions of ANOVA.
