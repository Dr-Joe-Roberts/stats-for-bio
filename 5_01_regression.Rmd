# Simple regression

# Introduction

The simple *t*-tests we encountered earlier have been concerned with how to compare mean(s) of numeric variables in some way. We now know how to...

- compare one mean to any particular value via the one-sample *t*-test

- compare means among two groups or conditions via the two-sample *t*-test. 

- compare two conditions *within* items / objects via the paired-sample *t*-test.

One way to think about the two-sample and paired-sample *t*-tests is that they evaluate whether or not the variable changes among two groups or experimental conditions. Membership of the different groups/conditions can be encoded by a categorical variable. In R, we use a formula involving the numeric (`num_var`) and categorical (`cat_var`) variables to set up the test (e.g. `num_var ~ cat_var`). There is a good reason for doing things this way. It reflects the fact that we can conceptualise the tests as considering a *relationship* between between a numeric and categorical variable^[It is perfectly possible to evaluate differences among means in more than two categories, but we don't use *t*-tests to do this. Instead, we us a more sophisticated tool called Analysis of Variance (ANOVA). We'll learn about ANOVA in later chapters.].

This chapter is about a different kind of relationship. The question we want to address is---what should we do when two numeric variables have been measured we want to understand how they are related? We can use a statistical tool called regression to tackle this new kind of question.

## Relationships between numeric variables

Much of biology is concerned with relationships between numeric variables. For example...

-   We sample fish and measure their length and weight because we want to understand how weight scales with respect to length.

-   We survey grassland plots and measure soil pH and species diversity because we want to understand how species diversity depends on soil pH.

-   We may manipulate temperature and measure fitness in insects because we want to characterise their thermal tolerance.

```{r, echo = FALSE}
x <- data.frame(x = seq(-2, 2, length.out = 50))
set.seed(27081975)
```

In each of these settings the goal is to understand how one numeric variable depends on the values of another. Graphically, we evaluate such dependencies using a scatter plot. We may be interested in knowing:

1.    Are the variables related or not? There's not much point studying a relationship that isn't there:

```{r reg-eg-related, echo = FALSE, out.width='75%', fig.align='center'}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Related"),
          mutate(x, y = 3 +     rnorm(n(), sd = 1.0), labs = "Unrelated")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

2.    Is the relationship positive or negative? Sometimes we can answer a scientific question just by knowing the direction of a relationship:

```{r reg-eg-posneg, echo = FALSE, out.width='75%', fig.align='center'}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Positive"),
          mutate(x, y = 3 - x + rnorm(n(), sd = 0.5), labs = "Negative")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

3.    Is the relationship a straight line or a curve? It is important to know the form of a relationship if we want to make predictions:

```{r reg-eg-linornot, echo = FALSE, out.width='75%', fig.align='center'}
bind_rows(mutate(x, y = 3 + x + rnorm(n(), sd = 0.5), labs = "Straight"),
          mutate(x, y = 4.5 + .8*x - .6*x^2 + rnorm(n(), sd = 0.45), labs = "Curved")) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

Although sometimes it may be obvious that there is a relationship between two variables from a plot of one against the other, at other times it may not. Take a look at the following

```{r reg-eg-related, echo = FALSE, out.width='75%', fig.align='center'}
x <- data.frame(x = rnorm(40))
bind_rows(mutate(x, y = 3 + 0.3 * x + rnorm(n(), sd = 0.5), labs = "Sample A"),
          mutate(x, y = 3 + 0.1 * x + rnorm(n(), sd = 0.5), labs = "Sample B")) %>%
  ggplot(aes(x = x, y = y)) + geom_point() + facet_wrap(~labs, nrow = 1)
```

We might not be very confident in judging which, if either, of these plots provides evidence of a positive relationship between the two variables. Maybe the pattern the we perceive can just be explained by sampling variation, or maybe it can't. Clearly it would be useful to have a measure of how likely it is that the relationship could have arisen as a result of sampling variation. In addition to judging the statistical significance of a relationship, we may also be interested in describing the relationship mathematically – i.e. finding the equation of the best fitting line through the data. 

A linear regression analysis allows us to do all this. 

## What does linear regression do?

**Simple linear regression** finds the straight-line relationship which best describes the dependence of one variable (the **dependent variable**) on the other (the **independent variable**). 

-   What does the word 'simple' mean here? A simple linear regression is a regression model which only accounts for one independent variable. If more than one independent variable is considered, the correct term to describe the resulting model is 'multiple regression'. Multiple regression is a very useful tool but we're only going to study simple regression in this book.

-   What does the word 'linear' mean here? In statistics, the word linear is used in two slightly different (but closely related) ways. When discussing simple linear regression the term linear is often understood to mean that the relationship follows a straight line. That's all. The more technical definition concerns the relationship between the parameters of a statistical model. We don't need to worry about that one here.

Writing 'simple linear regression' all the time soon becomes tedious. We'll often write 'linear regression' or just 'regression' instead. Just keep in mind that we're always referring to simple linear regression in this chapter.

A linear regression describes how the dependent variable changes in response to the values of the independent variable. It is conventional to label the dependent variable as '$y$' and the independent variable as '$x$'. When we present such data graphically, the independent variable always goes on the $x$-axis and the dependent variable on the $y$-axis. Try not to forget this convention!

How to select which is to be used as the dependent and which as the independent variable is fairly straightforward in an experimental setting. Consider the thermal tolerance example from earlier. Temperature was manipulated in this experiment, so it absolutely must be designated the independent variable. Moreover, *a priori* (before conducting the experiment), we can reasonably suppose that changes in temperature may cause changes in enzyme activity, but the reverse seems pretty unlikely.

However, things may not be so clear cut when we are working with data from an observational study. These problems will be discussed in more detail later. There is one important point to be aware of now however: in regression it matters which way round we designate the dependent and independent variables. If you have two variables A and B, the relationship you find from a regression will not be the same for A against B as for B against A.

```{block, type='advanced-block'}
**'dependent vs. independent'  or 'response vs. predictor'?**

Another way to describe linear regression is that it allows us to *predict* how one variable (the **response variable**) *responds* to another (the **predictor variable**). The dependent vs. independent  and response vs. predictor conventions for variables in a regression are essentialy equivalent. The only differ in the nomenclature they use to describe the variables involved. To avoid confusion, we will stick with dependent vs. independent naming convention in this course.
```

## How does simple linear regression work?

### Finding the best fit line

```{r, echo=FALSE}
set.seed(27081976)

exp.data <- 
  data.frame(x = seq(-2, +2, length = 12)) %>% 
  mutate(y = x + rnorm(n(), sd = 1), y = y - mean(y))

lm.mod <- lm(y ~ x, data = exp.data)

mod.data <- data.frame(x = seq(-2.2, +2.2, length.out = 25))

all.mod.data <- list()
all.exp.data <- list()

all.mod.data[[1]] <- 
  mod.data %>% 
  mutate(y = mean(exp.data$y), labs = "A")
all.mod.data[[2]] <- 
  mod.data %>% 
  mutate(y = 0.5*x*coef(lm.mod)[2], labs = "B")
all.mod.data[[3]] <- 
  mod.data %>% 
  mutate(y = 1.6*x*coef(lm.mod)[2], labs = "D")
all.mod.data[[4]] <- 
  mod.data %>%  
  mutate(y = predict.lm(lm.mod, newdata = .), labs = "C")

all.exp.data[[1]] <- 
  exp.data %>% 
  mutate(yend = mean(exp.data$y), labs = "A")
all.exp.data[[2]] <- 
  exp.data %>% 
  mutate(yend = 0.5*x*coef(lm.mod)[2], labs = "B")
all.exp.data[[3]] <- 
  exp.data %>% 
  mutate(yend = 1.6*x*coef(lm.mod)[2], labs = "D")
all.exp.data[[4]] <- 
  exp.data %>%  
  mutate(yend = predict.lm(lm.mod, newdata = .), labs = "C")

all.mod.data <- bind_rows(all.mod.data)
all.exp.data <- bind_rows(all.exp.data)
```

If we draw a straight line through a set of points on a graph then, unless they form a perfect straight line, some points will lie close to the line and others further away. The vertical distances between the line and each point (i.e. measured parallel to the $y$-axis) have a special name. They are called the *residuals*. Here's an visual example:

```{r}
ggplot(filter(all.exp.data, labs == "C"), aes(x = x, y = y)) + 
  geom_segment(colour = "darkgrey",
               aes(xend = x, y = y, yend = yend)) + 
  geom_line(data = filter(all.mod.data, labs == "C")) + 
  geom_point(data = exp.data, colour = "blue")
```

In this plot, the blue points are the data and the vertical lines represent the residuals. The residuals represent the variation ‘left over’ after the line has been fitted through the data. They give an indication of how well the line fits the data. If all the points lay close to the line the variability of of the residuals would be low relative to the variation in the dependent variable, $y$. When the observations are more scattered around the line the the variability of the residuals would be large relative to the variation in the dependent variable, $y$.

Regression works by finding the line which minimises the size of the residuals in some sense. We'll explain exactly how in a moment. The following illustration indicates the principle of this process:

```{r, fig.width=4, echo = FALSE}
ggplot(all.mod.data, aes(x = x, y = y)) + 
  geom_point(data = exp.data, colour = "blue") + 
  geom_line() + facet_wrap(~labs, nrow = 2)
```

The data are identical in all four graphs, but in the top left left hand graph a horizontal line (i.e. no effect of $x$ on $y$) has been fitted, while on the remaining three graphs sloping lines of different magnitude have been fitted. To keep the example simple, we assume we know the true intercept of the line, which is at $y=0$, so all four lines pass through $x=0$, $y=0$ (i.e. the 'origin').

```{block, type='do-something'}
**Which line is best?**

One of the four lines is the 'line of best' fit from a regression analysis. Spend a few moments looking at the four figures. Which line seems to fit the data best? Why do you think this line is 'best'?
```

Let's visualise the data, the candidate lines and the residuals:
```{r, fig.width=4, echo = FALSE}
ggplot(all.exp.data, aes(x = x, y = y)) + 
  geom_segment(colour = "darkgrey",
               aes(xend = x, y = y, yend = yend)) + 
  geom_line(data = all.mod.data) + 
  geom_point(data = exp.data, colour = "blue") + 
  facet_wrap(~labs, nrow = 2)
```

We said that regression works by finding the intercept and slope that minimises the vertical distances between the line and each observation in some way^[Notice we said that it is the vertical distance that matters, not the perpendicular distance from the line.]. In fact, it minimises something called the 'sum of squares' of these distances. We calculate a sum of squares for a particular line and data set by squaring the residual distances and adding all of these up. This quanity is called the **residual sum of squares**. 

You should be able to see that, for the horizontal line ('A'), the residual sum of squares is larger than any of the other three plots with the sloping lines. This suggests that the sloping lines fit the data better. The line with the *lowest* residual sum of squares is the best line, because it ‘explains’ the most variation in the dependent variable. Which one is best among the three we've plotted? To get at this we need to calculate the residual sum of squares for each line. These are...
```{r, echo = FALSE}
all.exp.data %>% 
  rename(Line = labs) %>% group_by(Line) %>% 
  summarise('   Residual Sum of Squares' = sum((y-yend)^2)) %>% as.data.frame
```
So it looks like the line in panel C is the best fitting line among the candidates. In fact, it is the best fit line among all possible candidates. Did you manage to guess this by looking at the lines and the raw data? If not, think about why you got the answer wrong.

```{block, type='do-something'}
It is very important that you understand what a residual from a regression represents. Residuals pop up all the time when evaluating statistical models. If you're still confused about what they represent ask a TA to explain them to you some time.
```

### What do you get out of a regression?

A regression analysis will tell us (among other things):

-   The equation of the straight line that best describes how the $y$ (dependent) variable *depends on* the $x$ (independent) variable. To put it in slightly more technical terms, it describes the $y$ variable as a function of the $x$ variable.

-   Whether there is a statistically significant relationship between the $x$ and $y$ variables. That is, the analysis will tell us whether an apparent association is likely to be real, or just a chance outcome resulting from sampling variation. 

Let's consider each of these...

### Interpreting a regression

What is the form of the relationship? The equation for a straight line relationship is $y = a + b \times x$, where $y$ is the dependent variable, $x$ is the independent variable, $a$ is the intercept (i.e. the value at which the line crosses the $y$ axis), $b$ is the slope of the line. The $a$ and the $b$ are referred to *parameters* or *coefficients* of the line.

The slope of the line is usually the thing we care about most. It tells us the amount by which $y$ changes for a change of one unit in $x$. If the value of $b$ is positive (i.e. a plus sign in the above equation) this means the line slopes upwards to the right. A negative slope ($y = a - bx$) means the line slopes downwards to the right. The diagram below shows the derivation of an equation for a straight line.

```{r, fig.width=4, echo = FALSE}
I <- 1
S <- 2/3
data.frame(x    = c(0, 3), 
           y    = c(I, I), 
           xend = c(3, 3), 
           yend = c(I, I + 3 * S)) %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + 
  geom_segment(colour = "darkgrey") + 
  geom_abline(intercept = I, slope = S, linetype = 2) + 
  scale_x_continuous(limits = c(-0.25, 3.5)) + 
  scale_y_continuous(limits = c(-0.25, 4.0)) + 
  xlab("Independent variable (x)") + ylab("Dependent variable (y)") + 
  annotate("text", x = 0.2, y = 3.9, hjust=0, parse = TRUE, label = "b==Delta*y/Delta*x") + 
  annotate("text", x = 1.6, y = 3.9, hjust=0, parse = TRUE, label = "b==2/3") +
  annotate("text", x = 0.2, y = 3.5, hjust=0, parse = TRUE, label = "a==1") + 
  annotate("text", x = 0.2, y = 2.9, hjust=0, parse = TRUE, label = "y==1+0.667*x") + 
  annotate("text", x = 1.5, y = 0.7, hjust=0, parse = TRUE, label = "Delta*x==3") + 
  annotate("text", x = 2.1, y = 1.8, hjust=0, parse = TRUE, label = "Delta*y==2") +
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0)
rm(I, S)
```

Having the equation for a relationship allows you to predict the value of the $y$ variable for any value of $x$. For example, in the thermal tolerance example, we might want an equation which would allow us to work out the threshold temperature at whihc fitness drops below 1. Such predictions can be made by hand (see below) or using R (details later).

In the above diagram, the regression equation is: $y = 1 + 0.66 x$. So to find the value of $y$ at $x = 2$ you get: $y = 1 + (0.667 \times 2) = 2.32$. Obviously, by finding $y$ values for 2 (or preferably 3) different $x$ values from the equation, the actual line can easily be plotted on a graph by hand if required---plot the values and join the dots!

```{block, type='advanced-block'}
**Regression involves a statistical model**

If you skim back through the [Parametric statistics] chapter, you may recall that the equation $y = a + b \times x$ represents the 'systematic component' of the regression. This bit describes the relationship between the $y$ and $x$ variable. The residuals correspond to the 'random component' of the model. These represent the copmponent of variation in the dependent $y$ variable our regression model fails to describe.
```

### Evaluating hypotheses

[[FINISH ME]]

## The assumptions of regression

Simple linear regression is a type of parametric statistical technique, i.e. it makes a number of assumptions. Let's consider each one in detail, in their approximate order of importance:

1.    **Independence.** The residuals must be independent. Another way of stating this assumption is that the value of each residual does not depend on the value of any others. This can be difficult to check. If the data are from an experiment and we made sure to properly randomise the treatments, it should be OK. If the data are observational, then we need to be more careful. The reason this assumption matters is because, if the residuals are not independent, any *p*-values we generate will unreliable.

2.    **Measurement scale.** The dependent ($y$) and independent ($x$) variables are measured on an interval or ratio scale. This one is easy to assess.

3.    **Linearity** The relationship between the independent $x$ variable and the dependent $y$ variable is linear. Obviously there is little point in fitting a straight line to data which clearly don’t form a straight line relationship. There may also be circumstances in which it is theoretically unlikely for a relationship to be linear, e.g. the length and weight of an animal will not be well described by a linear relationship because weight is generally a cubic function of length. If the data fail this assumption then applying a mathematical transformation of $x$ or $y$ can help. We will come back to this idea later in the course.

5.    **Constant variance.** The variance of the residuals is constant. This assumption essentially means the variability of the residuals is not related to the value of the independent $x$ variable. It is violated if the magnitude of the residuals increase or decrease markedly as $x$ gets larger. If the data fail this assumption then again, sometimes applying a mathematical transformation of $y$ will help. We will also discuss this idea later in the course.

4.    **Normality.** The residuals are drawn from a normal distribution. This is a bit less obvious. It essentially means that for each value of $x$ we would expect there to be a range of responses in $y$ which follow a normal distribution. Notice that it is the distribution of the deviations of $y$ from the fitted line (the residuals) that are assumed to be normal. This means that we can generally only test this assumption *after* the line has been fitted. It does not make sense to evaluate this assumption by looking at the raw $y$ values.

6.    **Measurement error.** The values of the independent $x$ variable are determined with negligible measurement error. It is often hard to obtain the $x$ values with absolutely no measurement error, but hopefully the error will at least be smaller than that in the $y$ values. So for example, in the thermal tolerance experiment, the temperature values (set by the experimenter) almost certainly have little error.

### Checking the assumptions of regression

Assumption 2 (measurement scale) is easy to evaluate. Assumptions 1 (independence) and 6 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data; you have to think about the data and see if there are any obvious reasons why they might not be valid. There are a special set of tools, called 'regression diagnostics', that allow us to evaluate the remaining assumptions. We are going to study these in the next chapter, so for now we will rely on simple, but less effective means: 

-   Assumptions 3 (linearity) and 5 (equal variance) can be informally evaluated by looking at a scatter plot of the data

-   Assumption 4 (normality) can be checked by looking at the distribution of the residuals taken from the fitted the regression model.

## An example {#regression-example}

A plant physiologist studying the process of germination in the broad bean (*Vicia faba*) is interested in the relationship between the activity of the enzyme amylase, and the temperature at which the germinating beans are kept. As part of this work she carries out an experiment to find the relationship between glucose release (from the breakdown of starch by amylase) and temperature (over the range 2 - 20C). The data obtained from such an experiment are given below.

||||||||||||
|:---------------------------------------|:--|:----|:----|:----|:----|:----|:----|:----|:----|:----|
| Temperature ($C$)                      | 2 | 4   | 6   | 8   | 10  | 12  | 14  | 16  | 18  | 20  |
| Glucose ($\mu g$ $mg^{-1}$ dry weight) | 1 | 0.5 | 2.5 | 1.5 | 3.2 | 4.3 | 2.5 | 3.5 | 2.8 | 5.6 |

The data are in a CSV file called GLUCOSE.CSV. The first column (`Temperature`) contains the information about the experimental temperature treatments, and the second column (`Glucose`) contain the glucose measurements.

(Notice that we're referring to the different temperatures as 'treatments'. This is perfectly reasonable here, as these data are from an experiment where temperature was controlled by the investigator.)

```{block, type='do-something'}
**Work through the germination example**

You should work through the *Vicia* germination example from this point. Start by downloading the GLUCOSE.CSV file from MOLE and placing it in your working directory.
```

```{r, echo=FALSE}
vicia.germ <- read.csv(file = "./data_csv/GLUCOSE.CSV")
```

Once you have downloaded the data read it into an R data frame, giving it the name `vicia.germ`. Make sure you use `View`, `glimpse`, etc to examine the data before you proceed. Run through all the usual questions... How many variables (columns) are in the data? How many observations (rows) are there? What kind of variables are we working with?

We should visuaise the data next so that we understand it more. We just need to produce a simple scatter plot, which is easy to do with `ggplot2`:
```{r, fig.width=4}
ggplot(vicia.germ, aes(x = Temperature, y = Glucose)) + 
  geom_point()
```

The plot clearly suggests that there is some sort of positive relationship which might reasonably be described by a straight line.

What we want to do is work out whether there a statistically significant relationship between temperature and glucose release (and hence, presumably, amylase activity). If there is, we might also want to find the equation that describes the relationship. We will return to the mechanics of actually doing this later. But first, keeping the example in mind, we will consider the basic ideas of what regression does, and for what sort of data it is suitable.

```{block, type='warning-box'}
**Variables and axes**

Be careful when you produce a scatter plot to summarise data in a regression analysis. You need to make sure the two variables are plotted the right way around with respect to the $x$ and $y$ axes: place the dependent variable on the $y$ axis and the independent variable on the $x$ axis. Nothing says, "I don't know what I'm doing" more than mixing up the axes.
```

## Carrying out regression with R {#regression-R}

OK, let's carrry on with our example and see how to carry out a simple regression in R.

### Checking the assumptions

The data are on ratio (glucose release, $\mu g$ $mg^{-1}$ dry weight) and interval (temperature, °C) scales, the assumption of negligible measurement error seems perfectly reasonable, and without more knowledge of the experimental design, we have to assume that the independence assumption is met. As noted above, it is better to evaluate the remaining assumptions using regression diagnostics, but let's proceed using rough checks that rely on inspection of a scatter plot of the data:
```{r, fig.width=4}
ggplot(vicia.germ, aes(x = Temperature, y = Glucose)) + 
  geom_point()
```

The scatter plot suggests that the relationship between $x$ and $y$ is linear, and the scatter in $y$ neither increases nor decreases substantially with increasing values of $x$. It is quite had to judge whether there is anything obviously wrong with the normality assumption using this plot---we will check this assumption more carefully in a moment by looking at the residuals. 

### Model fitting and significance tests

Now that we understand roughly how simple linear regression works, and since the data appear to meet the requirements of this model, we can finally move on to some results. Carrying out a regression analysis in R is really no different from ANOVA. It is a two step process. The first step is a model fitting step. This is where R calculates the best fit intercept and slope, along with additional information needed to generate the results in step two.

Once again, we carry out the model fitting step using the `lm` function (remember, the data are in `vicia.germ`, `Temperature` is the independent variable, and `Glucose` is the dependent variable):
```{r} 
vicia.model <- lm(Glucose ~ Temperature, data = vicia.germ)
```
This should appear very familiar by now. We have to assign two arguments:

1. The first argument is a **formula** (i.e. it includes a 'tilde' symbol: `~`). The variable name on the left of the `~` should be the dependent variable and the variable name on the right should be the independent variable (`Glucose` and `Temperature`, respectively).

2. The second argument is the name of the data frame that contains the two variables listed in the formula (`vicia.germ`).

```{block, type='advanced-box'}
**How does R knows we want to carry out a regression?**

How does R know we want to use regression? After all, we didn't specify this anywhere. The answer is that R looks at what type of variable `Temperature` is. It is numeric, and so R automatically carries out a regression. If it had been a factor (i.e. a categorical variable) R would have carried out a different kind of analysis, called a one-way ANOVA. 

Most of the models that we examine in this course are very similar, and can all be fitted using the `lm` function. The only thing that really distinguishes them is type of variables that appear to the right of the `~` in a formula: if they are factors (categorical variables) we end up carrying out ANOVA, while numeric variables lead to a regression. The key message is that you have to keep a close eye on the type of variables you are modelling to understand what kind of model R will fit.
```

As usual, we assigned the result a name (`vicia.model`) so that `vicia.model` now refers to a fitted model object. When looking at ANOVA, we said that printing this object to console wasn't very useful. What happens if we print a regression model object to the console?
```{r}
vicia.model
```
Just as with ANOVA, this prints a summary of the model we fitted and some information about the 'coefficients' of the model. The coefficients, when working with a simple regression, are the intercept and slope of the fitted line: the intercept is always labelled `(Intercept)` and the slope is labelled with the name of the independent variable (`Temperature` in this case). We'll come back to these coefficients once we have looked at how to compute *p*-values.

```{r, echo=FALSE}
anova.out <- capture.output(anova(vicia.model))
```

We usually fit a regression model so that we can determine whether the slope (and perhaps, the intercept) is significantly different from zero---i.e. we want to know if the relationship between the $x$ and $y$ variables are really associated. Surprisingly, we use the `anova` function to do this. Why do we use the `anova` function? It turns out that the basic idea of ANOVA---the assessment of statistical significance of model terms via comparisons of variance---can be applied to any kind of model object created by `lm`, including a simple regression model.

As usual, all we have to do is pass the `anova` function one argument: the name of the fitted regression model object:
```{r}
anova(vicia.model)
```
The first line just informs us that we are looking at an ANOVA table, i.e. a table of statistical results from an analysis of variance. Just remember, this doesn't necessarily mean we are dealing with an ANOVA model---i.e., an ANOVA model is one in which the independent variable(s) is (are) categorical.

The second line reminds us what variable we analysed (the dependent variable). The important part of the output is the table at the end:
```{r, echo = FALSE}
invisible(sapply(anova.out[4:6], function(line) cat(line, "\n")))
```
This summarises the parts of the analysis of variance calculations, as it applies to a regression model. These should be familiar to you: `Df` -- degrees of freedom, `Sum Sq` -- the sum of squares, `Mean Sq` -- the mean square, `F value` -- the *F*-statistic (i.e. variance ratio), `Pr(>F)` -- the p-value).

The *F*-statistic (variance ratio) is the most important term. When working with a regression model, this is related to how much variability in the data is explained when we include the best fit slope term in the model. Larger values indicate a stronger relationship between $x$ and $y$. The p-value gives the probability that the relationship could have arisen through sampling variation, if in fact there were no real association: a p-value of less than 0.05 indicates a less than 1 in 20 chance of the result being due to chance, and we take this as evidence that the relationship is real.

#### Extracting a little more information

```{r, echo = FALSE}
summary.out <- capture.output(summary(vicia.model))
```

There is a second function, called `summary`, that can be used to extract a little more information from the fitted regression model:
```{r}
summary(vicia.model)
```
This is easiest to understand if we step through the constituent parts of the output. The first couple of lines just remind us about the model we fitted
```{r, echo = FALSE}
invisible(sapply(summary.out[2:3], function(line) cat(line, "\n")))
```
The next couple of lines aren't really all that useful---they summarise some properties of the residuals--so we'll ignore these.

The next few lines comprise a table that summarises some useful information about the coefficients of the model (the intercept and slope):
```{r, echo = FALSE}
invisible(sapply(summary.out[9:12], function(line) cat(line, "\n")))
```
The `Estimate` column shows us the estimated the intercept and slope of the regression. We saw these earlier when we printed the fitted model object to the console.

Staying with this table, the next three columns (`Std. Error`, `t value` and `Pr(>|t|)`) show us the standard error associated with each coefficient, the corresponding *t*-statistics, and the *p*-values. Remember standard errors? These are a measure of the variability of the sampling distribution associated with something we estimate from a sample. We discussed these in the context of sample means. It turns out that one can calculate a standard error for many different kinds of quantities, including the intercept and slope of a regression model. And just as with a mean, we can use the standard errors to evaluate the significance of the coefficients via *t*-statistics.

In this case, the *p*-values associated with these *t*-statistics indicate that the intercept is not significantly different from zero (*p*>0.05), but that the slope is significantly different from zero (*p*<0.01). Notice that the *p*-value associated with the slope coefficient is the same as the one we found when we used the `anova` function. This is not a coincidence---`anova` and `summary` test the same thing when working with regression models.

The only other part of the output from summary that is of interest now is the line containing the `Multiple R-squared` value:
```{r, echo=FALSE}
invisible(sapply(summary.out[17], function(line) cat(line, "\n")))
```
This shows the $R$-squared ($R^{2}$) of our model. It tells you what proportion (sometimes expressed as a percentage) of the variation in the data is explained, or accounted for, by the fitted line. If $R^{2}=1$ the line passes through all the points on the graph (all the variation is accounted for) and if $R^{2}\approx 0\%$ the line explains little or none of the variation in the data.

(The `Adjusted R-squared:` value can be ignored in this analysis---it is used when doing a form of regression called *multiple regression*, in which there is more than one $x$ variable. We will not be dealing with multiple regression here.)

The $R^{2}$ value here is 0.64. This is very respectable, but still indicates that there are other sources of variation (differences between beans, inaccuracies in the assay technique, etc.) which remain unexplained by the line.

#### Residual analysis

There are various situations in which we need to be able to extract the residuals from a fitted model. We'll look at one example now. Remember the 5th assumption: the residuals are drawn from a normal distribution. How might we examine the distributional assumptions of the regression? We can do this with a dot plot or histogram of the residuals, if know how to extract these from a model. This is easy in R---we use the `resid` function:
```{r}
resid(vicia.model)
```
This just extracts a numeric vector containing the residuals and prints them to the console. In order to plot these we need to put them inside a data frame (`ggplot2` only works with data frames), and store the result:
```{r}
resid.data <- data.frame(Residuals = resid(vicia.model))
```

Once we have extracted the residuals into a data frame we just use `ggplot2` in the usual way to plot them. We'll use a dot plot, as there aren't many residuals:
```{r, fig.width=4, }
ggplot(resid.data, aes(x = Residuals)) + geom_dotplot(binwidth = 0.3)
```
It is hard to know if these are normally distributed when we only have 10 observations, but there is nothing that screams 'non-normal' here.

## Presenting results {#present-results}

From the preceding analysis we can conclude...

> There is a significant positive relationship between the incubation temperature (°C) and glucose released ($\mu g mg^{-1}$ dry weight) in germinating bean seeds ($y=0.52+0.20x$,  F=14, d.f.=1,8, *p*<0.01).

Don't forget to quote both degrees of freedom in the result. These are obtained from the ANOVA table produced by `anova` and should be given as the slope degrees of freedom first (which is always 1), followed by the error degrees of freedom.

If the results are being presented only in the text it is usually appropriate to specify the regression equation as well as the significance of the relationship as this allows the reader to see in which direction and how steep the relationship is, and to use the equation in further calculations. It may also be useful to give the units of measurement---though these should already be stated in the Methods.

###

Often, however, you will want to present the results as a figure, showing the original data and the fitted regression line. In this case, most of the statistical detail can go in the figure legend instead.

We already know how to make a scatter plot. The only new trick we need to learn is how to add the fitted line. Remember the output from the summary table---this gave us the intercept and slope of the best fit line. We could extract these (the is a function called `coef` that does this), and using our knowledge of the equation of a straight line, use them to then calculate a series of points on the fitted line. However, there is an easier way to do this using the `predict` function. 
```{block, type='do-something'}
Don't worry too much if this next segment on predictions is confusing. It looks more complicated than it is, but you may have to come back to it a few times before it all sinks in. At first reading, try to focus on the logic of the calculations without worrying too much about the details.
```

In order to use `predict` we have to let R know the values of the independent variable for which we want predictions. In the bean example the temperature was varied from 2-20 °C, so it makes sense to predict glucose concentrations over this range. Therefore the first step in making predictions is to generate a sequence of values from 2 to 20, placing these inside a data frame:
```{r}
pred.data <- data.frame(Temperature = seq(2, 20, length.out = 25))
```
We learned about the `seq` function last year. Here, we used it to make a sequence of 25 evenly spaced numbers from 2 to 20. If you can't remember what it does, ask a demonstrator to explain it to you (and use `View` to look at `pred.data`). Notice that we gave the sequence the exact same name as the independent variable in the regression (`Temperature`). **This is important**: the name of the numeric sequence we plan to make predictions from has to match the name of the independent variable in the fitted model object.

Once we have set up a data frame to predict from (`pred.data`) we are ready to use the `predict` function:
```{r}
predict(vicia.model, pred.data)
```
This take two arguments: the first is the name of the model object (`vicia.model`); the second is the data frame (`pred.data`) containing the values of the independent variable at which we want to make predictions. The predict function generated the predicted values in a numeric vector and printed these to the console.

To be useful, we need to capture these somehow, and because we want to use `ggplot2`, these need to be kept inside a data frame. We can use mutate to do this:
```{r}
pred.data <- mutate(pred.data, Glucose = predict(vicia.model, pred.data))
```

Look at the first 10 rows of the resulting data frame:
```{r, echo = FALSE}
head(pred.data, 10)
```
The `pred.data` is set out much like the data frame containing the experimental data. It has two columns, called `Glucose` and `Temperature`, but instead of data, it contains predictions from the model. Plotting these predictions along with the data is now easy:
```{r, fig.width=4}
ggplot(pred.data, aes(x = Temperature, y = Glucose)) + 
  geom_line() + geom_point(data = vicia.germ) + 
  xlab("Temperature (°C)") + ylab("Glucose concentration")
```
Notice that we have to make `ggplot2` use the `vicia.germ` data (i.e. the raw data) when adding the points. 

Let's summarise what we did: 1) using `seq` and `data.frame`, we made a data frame with one column containing the values of the independent variable we want predictions at; 2) we then used the `predict` function to generate these predictions, adding them to the prediction data with `mutate`; 3) finally, we used `ggplot2` to plot the predicted values of the dependent variable against the independent variable, remembering to include the data.

```{r, eval = FALSE, echo = FALSE}
data.frame(Temperature = seq(2, 20, length.out = 25)) %>% 
  mutate(Glucose = predict(vicia.model, .)) %>% 
  ggplot(aes(x = Temperature, y = Glucose)) + 
    geom_line() + geom_point(data = vicia.germ) + 
    xlab("Temperature (°C)") + ylab("Glucose concentration")
```

## What about causation? {#causation}

No discussion of regression would be complete without a little homily on the fact that just because you observe a (significant) relationship between two variables this does not necessarily mean that the two variables are causally linked. If we find a negative relationship between the density of oligochaete worms (the dependent variable) and the density of trout (the independent variable) in a sample of different streams, this need not indicate that the trout reduce the numbers of oligochaetes by predation - in fact oligochaete numbers are often very high in slow-flowing, silty streams where they live in the sediments, trout prefer faster flowing, well oxygenated, stony streams - so a negative correlation could occur simply for that reason. There are many situations in biology where a relationship between two variables can occur not because there is a causal link between them but because each is related to a third variable (e.g. habitat).

This difficulty must always be borne in mind when interpreting relationships between variables in data collected from non-experimental situations. However, it is often assumed that because of this problem regression analysis can never be used to infer a causal link. This is incorrect. What is important is how the data are generated, not the statistical model used to analyse them. If a set of ten plants were randomly assigned to be grown under ten different light intensities, with all other conditions held constant, then it would be entirely proper to analyse the resulting data (for, let us say, plant height) by a regression of plant height ($y$) against light level ($x$) and, if a significant positive straight-line relationship was found, to conclude that increased light level caused increased plant height. 

Of course this conclusion still depends on the fact that another factor (e.g. temperature) isn’t varying along with light and causing the effect. But the fact that you are experimentally producing an effect, in plants randomly allocated to each light level (i.e. plants in which it is highly improbable that the heights happened to be positively related to light levels at the start) which gives you the confidence to draw a conclusion about causality.


