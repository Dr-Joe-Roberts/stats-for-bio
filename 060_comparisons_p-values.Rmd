# Comparisons, null hypotheses, and tests

Scientific enquiry involves asking questions. One of the commonest kinds of question that we ask as biologists is: ‘Is there a difference between the measurements from two samples?’ For example:

‘Do male and female locusts differ in length?’

‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’

‘Do eagle owls feed on rats of different sizes during winter and summer?’

‘Do streams running through conifer forests differ in pH from those running through deciduous forests?’

'Do purple and green plant morphs differ in their biomass?'

In order to answer these kinds of questions we need to step through the same kind of process discussed in the last few chapters. We have to: 1. decide which variables we need to measure, 2. decide which population parameters are relevant, 3. design a suitable experiment or gather representative samples, and 4. use the samples to learn about the populations.

Let's assume that we already know how to go about steps 1-3 and have already collected our samples. Our focus in this chapter is on step 4: using these samples to learn about the populations. Specifically, the problem we want to understand is how to evaluate whether or not two populations are different in some way.

## A new example {#morph-weights-eg}

As always, we'll investigate the problem by working through a simple example. We will stay with the purple morph / green morph example, but this time we'll examine a different variable: the dry weight biomass of our imaginary plants. Perhaps we suspect that the two morphs have different growth habits. One way to address this hypothesis would be to measure the biomass of individuals of each morph. Ultimately, we want to compare the mean biomass of each morph.

In order to study the biomass of individuals in our hypothetical population, we would collect a sample of dry weights. Dry weight is a numeric variable, measured on a ratio scale. The population parameters of interest now are the population mean dry weights of each morph. 

Notice that our definition of 'the population' is slightly different than before. Now we are imagining that each morph is a seperate population. Let's assume that we have sampled the dry weight (in grams) of 25 representative individuals of each morph.

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

```{block, type='do-something'}
You should work through this exampe from now on...
```

[[TRANSITION]]

The next step is to calculate some simple descriptive statistics for each morph. We need to know the sample means as these are our 'best guesses' of the population means. It may be useful to know something about the variability of the samples, which can be summarised by the sample standard deviation is a good measure of the latter. Here is a reminder of how to do this using `dplyr`:
```{r}
temporary <- group_by(morph.weights, pmorph)
summarise(temporary, mean = mean(weight), stan.dev = sd(weight))
```
This shows that the mean dry weights of purple and green morphs are 658 grams and 787 grams, respectively. The standard deviation estimates from the two samples suggest that dry weights of green morphs are a little more variable than the purple morphs. 

By the way, if you like using the `%>%` operator to chain together `dplyr` functions (the 'verbs'), then here is how to use this to achive exactly the same result as above:
```{r}
morph.weights %>% 
  group_by(pmorph) %>% 
  summarise(mean = mean(weight), stan.dev = sd(weight))
```

Using means and standard deviations to summarise samples can be tricky to understand until you are used to them. A plot of some kind is much more useful. At the moment, we are just trying to understand the distribution of our two samples. One way to do this is to construct a histogram or a dotplot of each sample distribution. We don't have much data, so a dotplot is probably the best option. Here is one way to make a dot plot for just the purple morph data:
```{r purple-dist}
purp.weights <- filter(morph.weights, pmorph == "purple") 
ggplot(purp.weights, aes(x = weight)) + geom_dotplot(binwidth = 30)
```

First, we filtered the complete dataset so that only the observations corresponding to the purple morph were retained, then we used `ggplot` with the dotplot geom to construct the figure we wanted. Once again (just as a reminder), here is how to achieve the same thing using the `%>%` operator:
```{r, fig.keep = FALSE}
morph.weights %>% 
  filter(pmorph == "purple") %>% 
  ggplot(aes(x = weight)) + geom_dotplot(binwidth = 30)
```

You should always explore your data visually before carrying out any kind of statistical analysis of it. You might learn something new about it, and at the very least, this allows you to assess whether there are any problems with it. A quick inspection of this figure suggests that there is nothing odd about the dry weight data. We only have 25 observations, so we can't say too much about its shape, but there don't seem to be any outliers. 



Last week, we looked at how to quantify uncertainty in a single estimate of the population mean biomass of a morph. This week, we want to compare the biomass samples of the two morphs to address the question: 'Do purple and green plant morphs differ in their dry weight biomass?'. However, as it is currently framed, this question is a little too vague. The first thing we have to do is define what we mean by 'different'.

```{r, echo = FALSE}
morph.weights <- read.csv(file = "./data_csv/MORPH_WEIGHTS.CSV")
```

As always, it is a very good idea to plot the data. We could do this in a variety of ways, but since we only have two groups (purple and green morphs), we may as well summarise the full sample distribution of each morph. The data are in the MORPH_WEIGHTS.CSV file, which we have read into R using the `read.csv` function. We stored the data in a data frame called `morph.weights`. Here is some `ggplot2` code to make the required plot:  
```{r two-morph-dist}
ggplot(morph.weights, aes(x = weight)) + 
  geom_dotplot(binwidth = 30) + 
  facet_wrap(~pmorph, ncol = 1)
```

(Hopefully this plot will also remind you how to use the `facet_wrap` function to make a multipanel plot, based on the values of particular variable--`pmorph` in this case).

What does this plot suggest? We are interested in the degree of similarity (or not!) of the two sample distributions. There is a lot of overlap in the dry weights of each morph, but in general we can say that: (1) green morph individuals tend to have higher dry weights than purple morphs, and (2) the green morphs seem to be more variable than purple morphs.

We can get a better handle of these patterns by calculating the sample means and standard deviations of the two samples to evaluate their central tendency and spread, respectively. We know how to use `dplyr` functions `group_by` and `summarise` to do this:
```{r}
descrip.stats <- 
  morph.weights %>% 
  group_by(pmorph) %>%
  summarise(wght.mean = mean(weight), wght.sd = sd(weight))
descrip.stats
```

These descriptive statistics back up our visual impressions of the data: green morph individuals are larger than purple morphs (sample means: 787 vs. 658 grams), and green morphs are more variable than purple morphs (sample SDs: 118 vs. 114 grams). Remember, these numbers are just point estimates derived from limited samples. If we sampled the populations again, sampling variation would ensure that we end up with different estimates. This means we are not yet in a position to conclude that green morphs are bigger than purple morphs.

Let's return to our question: 'Do purple and green morphs differ in their dry weight biomass?'. We said this question was a little too vague, and that we needed to define what we meant by 'different'. One way in which the sample distributions seem to be different is with respect to their spread, and so one possible refinement of this question would be to ask whether this difference is 'real', or simply arises from sampling variation. There are circumstances where it is scientifically interesting to compare variability. However, it is much more common to focus on differences in the central tendency of samples.

By looking at the central tendency of different samples, we can evaluate whether or not something we have measured increases or decreases, on average, among different populations. Many scientifically relevant questions are addressed by making this assessment. When someone uses a statistical test or model to 'compare samples', what they are usually doing is evaluating whether or not the central tendency of the populations are different. We typically make this assessment by evaluating the strength of evidence for the presence of **a difference in the population means** of the focal populations. 

The question we want to address is therefore: 'What is the strength of evidence for a difference in the population mean biomass of purple and green plant morphs?' In practise, this boils down to a another question: 'Is there a statistically significant difference between their means.'

We will now turn to the idea of **statistical significance**. We will also see how to evaluate statistical significance by calculating a ***p*-value** under a suitable **null hypothesis**.

```{block, type='do-something'}
**Don't panic...**

The ideas in these next couple of sections are really quite abstract and can be difficult to understand. We are not expecting you to understand them straight away, and you certainly won't be asked to explain them in an assessment. 
```

### How do we evaluate statistical significance?

```{r, echo = FALSE}
set.seed(27081975)
nperm <- 2000
perm.out <- numeric(nperm)
perm.eg <- list()
data.i <- morph.weights
ids <- morph.weights$pmorph
for (i in 1:nperm) {
  morph.labels <- sample(ids, replace = FALSE)
  perm.out[i] <- 
    mutate(data.i, pmorph = morph.labels) %>% 
    group_by(pmorph) %>% summarise(mean = mean(weight)) %$% diff(mean)
  if (i <= 3) {
    perm.eg[[i]] <- morph.weights$weight
    names(perm.eg[[i]]) <- morph.labels
  }
}
names(perm.eg) <- paste("Sample", 1:3)
```

In order to assess the strength of evidence for the presence of a difference between the population means of two groups, we have to do something that, at first glance, looks very strange. We can break this down into four steps:

1. We assume that there is actually no difference between the population means. That is, we hypothesise that all the data are sampled from a pair of populations that are characterised by a single, shared population mean.

2. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation of 'no difference between samples'. This usually involves calculating a sampling distribution of some kind of test statistic.

3. We then ask, 'if there is no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we actually observed in the true sample?'

4. If the observed difference is sufficiently improbable, then we conclude that we have found a 'statistically significant' result. A statistically significant result is therefore one that is inconsistent with the hypothesis of no difference.

(We will discuss how to define 'sufficiently improbable' once we've worked through an example.)

There are many different ways to go about realising this process. We'll look at one of these today. Regardless of the details, they all work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (e.g. a difference between means) *is absent*. That sounds odd, but if you can understand this fundamental idea you are well on your way to understanding how frequentist statistics works. 

Let's return to our example to see how this might work in practise.

### A permutation test

In our example, a hypothesis of 'no difference' between the mean dry weights of purple and green morphs has the following implication. It means both morphs are really sampled from the same population, and as such, the labels 'purple' and 'green' are meaningless. These labels may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows:

1. Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels 'purple' and 'green' to this new copy of the data. Do this in such a way that the original sample sizes are preserved.

(We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used. The process of assigning random lablels is called permutation)

2. Repeat this permutation scheme until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient.

3. For each permuted sample, calculate whatever sample statistic is of interest. In this case, we want the *difference* between the mean dry weight of purple and green morphs in each sample.

4. Compare the observed sample statistic (i.e. the difference between the mean dry weights) to the distribution of sample statistic from the randomly permutated samples.

This scheme is called a permutation test, because it involves random permutation of the group labels. Why is it useful? *Each unique random permutation yields an observation from the sampling distribution of the difference among sample means, under the assumption that this difference is really zero in the population.* This means we can assess whether an observed difference is consistent with the hypothesis of no difference by looking at where it lies relative to this sampling distribution. 

We have implemented a permutation test in R using the purple/green morph dataset for you, using `r nperm` permutations. We won't show you the R code because it uses a few tricks you haven't been taught, but we can look at a couple of permuted samples to get a sense of how this works: 
```{r, echo=FALSE}
perm.eg[1:2]
```
The data from each permutation are stored as numeric vectors, in  which each element of the vector is named (these are the labels). Notice that the set of numbers does not change among the permuted samples. The only difference between them is the labelling of the numbers. The difference between the mean dry weights in the first permutation is `r perm.out[1]`. This difference is `r perm.out[2]` in the second sample.

What we really care about here is distribution of these differences. This is an approximation to the the sampling distribution of the difference between means. Here is a histogram that summarises the `r nperm` mean differences from the permuted samples:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept = diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

Notice that this distribution is centred at zero. This makes sense--if we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. The red line shows the location of the observed difference between purple and green morph mean dry weights. The relevant feature here is the location of this observed difference within the sampling distribution.

```{r, echo=FALSE}
nlower <- sum(perm.out <=  diff(descrip.stats$wght.mean))
nhgher <- sum(perm.out >= -diff(descrip.stats$wght.mean))
```

What does this figure tell us? It looks like the observed difference is very unlikely to have arisen through sampling variation, under the assumption that the population means of the two groups are identical. We can say this because the observed difference lies at the end of one 'tail' of the sampling distribution. We need to be able to make a more precise statement than this though.

Only `r nlower` out of the `r nperm` permutations ended up being equal to, or 'more extreme' (i.e., more negative), than the observed difference. The probability of finding a difference in the means equal to or more negative than the observed difference (denoted *p*) is therefore, *p*=`r nlower/nperm` (`r 100*nlower/nperm`%). This probability has a special name. It is called the ***p*-value**. A *p*-value is defined as the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true.

We have to be careful at this point. The test we just did is called a 'one-tailed' test, because we only looked at one end (the tail) of the sampling distribution. However, we did not set out to test whether purple plants were smaller on average than green plants. We set out to assess whether they are different, but we never made a statement about the direction of the effect. This means we also have to consider the possibility of an effect in the opposite direction to what was observed. 

To do this, we have to count up the cases that fall into the upper and lower tails of the distribution, where each tail is defined by the region that lies beyond the absolute value of the observed difference, on the positive and negative halves of the x axis:
```{r, echo=FALSE, fig.height=3}
ggplot(data.frame(perm.out), aes(x = perm.out)) + 
  geom_histogram(fill = grey(0.4), binwidth = 12) +   
  geom_vline(xintercept =  diff(descrip.stats$wght.mean), colour = "red") + 
  geom_vline(xintercept = -diff(descrip.stats$wght.mean), colour = "red") + 
  xlab("Difference between means of permuted samples")
```

When we do this, we find that `r nlower+nhgher` out of the `r nperm` permutations lie beyond the observed difference, and so the new *p*-value is *p*=`r (nlower+nhgher)/nperm` (`r 100*(nlower+nhgher)/nperm`%). This kind of test--where we look at both tails of the sampling distribution--is called a 'two-tailed' test. We discuss the reasoning for and against using a one- or two-tailed test in this week's self-directed practical.

#### The significance of *p*-values

What are we supposed to do with the finding *p*=`r (nlower+nhgher)/nperm`? This is the probability of obtaining a result equal to or 'more extreme' than what was actually observed, assuming that the hypothesis under consideration is true. The hypothesis under consideration is one of no effect, and so a low *p*-value can be interpreted as evidence for an effect being present. In our example, the low *p*-value is evidence for a difference among the population mean dry weights of purple and green morphs.

One question remains: How small does a *p*-value have to be before we are happy to conclude that the effect is probably present? In practise, we do this by applying a threshold, called a **significance level**. If the *p*-value is less than the chosen significance level we say the result is said to be **statistically significant**. Most often (in biology at least), we use a significance level of *p*<0.05 (5%). Why? The short answer is that this is just a convention. We'll come back to this below.

#### The null hypothesis

Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to apply in a more complex setting. We are not expecting you be able to implement a permutation test yourself. We used it to demonstrate how frequentist statistics works. The details vary from one problem to the next, but ultimately, if we are using frequentist ideas we always have to find a way to do the following: 1) assume that there is actually no 'effect', where an effect is expressed in terms of one or more population parameters, 2) work out what would happen if we were to repeatedly take samples from a population in this hypothetical situation, 3) evaluate how likely our observation would be under the hypothesis of no effect.

When using frequentist statistics we are always asking what would happen if we continually sample from a population *in the absence of the effect we are interested in*. This idea of a hypthetical 'no effect' situation is so important that it has a special name; it is called **the null hypothesis**. Every kind of statistical test (in this course at least) has a very specific null hypothesis associated with it. You can only fully understand the resuts of a statistical test if you understand the null hypothesis it relies on. We will remind you about the null hypothesis and introduce the related concept of the alternative hypothesis at the end of this chapter.

## Hypotheses and null hypotheses--why such a negative approach?

Previously we have said that an hypothesis is a statement of a proposed process or mechanism which might be responsible for an observed pattern or effect. We have also seen that in statistics, you will encounter 'hypothesis' used in a different, and quite specific way. In particular you will frequently see the term: *null hypothesis* (often written in statistics books as H~0~).

The null hypothesis is simply statement of what we would expect to see if there is actually no effect of the factor we are looking at (e.g., plant morphology) on the variable that we measure (e.g., dry weight biomass). So in the above example our null hypothesis was *There is no difference in mean biomass of purple and green plants*. When comparing the sizes of rats eaten by eagle owls, the appropriate null hypothesis was *There is no difference between the mean size of rats eaten by eagle owls in summer and winter*.

All statistical tests you are likely to encounter in biology work by specifying a null hypothesis and then testing the observed data to see if they deviate from the null hypothesis. This may seem like a rather odd approach, but there are good theoretical and practical reasons for doing things this way.

Although you need to be aware of what a null hypothesis is, and what it is used for, in general discussion of tests we will normally refer to the effect which is the opposite of the null hypothesis - i.e. it is the effect you are actually interested in - known as the *test hypothesis*, or the *alternative hypothesis* (often denoted H~1~ in statistics books).

The alternative hypothesis is essentially a statement of the effect you are interested in evaluating, e.g., purple and green plants differ in their mean size. It is a statement of whatever is implied if the null hypothesis is not true.

Having got all the types of hypothesis sorted out, we can then use a particular statistical technique (such as a permutation test) to evaluate the observed result against that expected if the null hypothesis was true. The test gives us a probability (*p*-value) telling us how likely it is that we would have got the result we observe if the null hypothesis was, indeed, true.

If the value is sufficiently small (conventionally if *p*<0.05) we judge it unlikely that we would have seen this result if the null hypothesis was true and consequently we *reject the null hypothesis* (i.e. reject the notion that there is no difference) and instead *accept the alternative hypothesis* (that there is a difference). Note that this is not the same as 'proving' the alternative hypothesis is true. You can't prove anything by collecting data or carrying out an experiment.

If the probability is large, then it is quite likely that we could have got the observed result if the null hypothesis was true, and in this case we cannot reject the null hypothesis. Note that in this situation we *'do not reject the null hypothesis'*, but this is not quite the same as accepting that the null hypothesis is true, paradoxical though this may seem. One obvious reason for this is that if we only have a small sample then there may be an effect of the factor we are looking at, but we simply can’t detect it because we don’t have enough data.



