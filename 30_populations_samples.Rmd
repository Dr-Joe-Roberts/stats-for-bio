# Populations, samples and sampling error

> Statistics is the science of learning from data, and of measuring, controlling, 
> and communicating uncertainty; and it thereby provides the navigation essential 
> for controlling the course of scientific and societal advances
> 
> [Davidian and Louis (2012)](https://doi.org/10.1126/science.1218685)

## Revision: distributions {#revision-distributions}

We touched on the idea of a distribution last year: in statistics, a distribution is a statement about the frequency with which different values of a variable are observed. The mean and variance of a population are both examples of population parameters. These describe the central tendancy ('which values are most common?') and the dispersion ('how variable are the values') of a distribution.

FINISH ME

## Populations and samples {#pops-and-samps}

The word '**population**' has much a wider meaning in statistics than it does in biology. When a biologist talks about a population they are referring to a group of individuals of a particular species who interbreed. In statistics, a population is a much more abstract concept. It refers to any group of items that share certain attributes or properties. This idea is best understood by example:

-   The readers of this book are an example of a population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, and they tend to have similar educational backgrounds. 

-   Peatland regions in the UK is another example. There are many peatland sites in the UK. Although their ecology varies from one location to the next, they are all similar in certain respects, e.g, they are generally characterised by low-growing vegetation and acidic soils.

-   A population of organisms -- as understood by biologists -- can of course also be thought of as a statistical population. The individuals that comprise a biological population share cpommon behaviours, physiology and life histories. 

The primary goal of a statistical analysis is to learn something about a population of interest. The 'something' can be anything we know how to measure. For example, a social scientist might be interested in understanding the political attitudes of university students, a climate change scientist might want to know how much carbon is stored in UK peatland areas, and a behavioural ecologist might want to understand how much time individuals spend foraging for food and searching for mates.

These are very different populations and questions. Nonetheless, but there are fundamental commonalities in how they can be addressed using statistical ideas. We can break the process down into a number of steps:

### Step 1: Decide which variables are you interested in

The first step is to decide which features of the focal population you are interested in learning about. In essence this comes down to refining your question, i.e. which variable (or variables) do I need to measure to address my research question? In the examples above, these would be things like, a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in a biological population. Sometimes we are interested in more than the properties of individual variables. Instead, we might want to understand the relationship between two or more variables.

### Step 2: Decide which population parameters are relevant

Next, we have to decide which **population parameter** associated with our variable(s) is relevant. A population parameter is a numeric quantity that describes something about the distribution(s) of one or more variables in the population. A simple, but important, population parameter is the population mean. We often want to know about the population mean because it allows us to answer questions such as "how many are there?" or "how much of something is present?". Much of this course is centred around asking questions about population means.

Many different population parameters can be relevant though. For example, the goal of statistical genetics is to partition out variablity among individuals---we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case, it is the population variance that we want to learn about. ALternativley, if we are trying to learn about how two variables are associated, then some kind of correlation coefficient (we'll learn more about this later) might be the right population parameter to focus on. 

### Step 3: Gather a representative sample

In the the type of statistics that we use in this course (called, 'frequentist statistics'), the population parameters are conceptualised as fixed but unknown quantities. Our goal is to learn about them by collecting data. If we were able to measure every object in the focal population we wouldn't need statistics. We could just calculate whatever quantity we needed to know using an exhaustive sample and we'd be done. Of course, in the real world we face all kinds of resource constraints. We have limited time and money to invest on any given problem, no matter how important it is. 

Instead of trying to measure every item in a population we only study a small **sample** of the population. A sample is just a subset of the wider population, which has been chosen so that it is representative of that population. The word "representative" is very important here. If we can't collect a representative sample of the population we can't really learn anything useful about it.

For example, if we want to understand the reproductive characteristics of our favourite study organism, but we have only sampled young or old individuals, it will be very difficult to generalise our findings to the wider population. The reproductive performance of most organisms changes as they age, so if we only measure young individuals, we haven't learnt anything about older individuals, and vice versa.

Sampling theory is a huge subdiscipline of statistics. We will touch a few of the important ideas about sampling as we go through this course. For now, we will just assume that we know how to collect samples that are representative of the population.

### Step 4: Estimate the population parameter.

Once we have a representative sample we can calculate something called a **point estimate** of the population parameter we're interested in. Remember, the population parameter is unknown. A point estimate is just a number that represents our "best guess" its true value. If we are interested in a population mean, then the obvious estimate to use is the sample mean. This is just "the mean" you learnt how to calculate at school.

(By the way, keep in mind that: 1) many people just say "estimate" instead of "point estimate" because writing "point estimate" all the time is tedious; 2) others refer to point estimates as "statistics". The exact terminology isn't really all that important. We'll mostly use the word "estimate" from now on)

### Step 5: Quantify the uncertainty of estimate(s)

A point estimate of a population parameter is virtually useless on its own, because an estimate is derived from a limited sample of the wider population. Even if we are very careful about how we sample the population, there is no way to guarantee that the composition of our sample exactly matches that of the population. And this means that any estimate we derive from a sample will be imperfect, in the sense that it won't exactly match the true (unknown) population value. 

In a nutshell, there is always uncertainty associated with an estimate of a population parameter. This means that we have to be careful if we want to answer a seemingly simple question such as, "Is the more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?" 

## Those first few steps {#first-steps}

Point estimates are generally easy to calculate. 

## Sampling error {#sampling-error}

Luckily, we can use the machinery of statistics to help us quantify this uncertainty. Once we know something about the Before we use these tools we first need to understand something called sampling error. The rest of this chapter will give you a flavour of this important idea.

```{r plant-sim-par, echo=FALSE}
set.seed(27081975)
nsamp <- 200
sampsize1 <- 20
sampsize2 <- 40
sampsize3 <- 80
index <- c(1,1,2,2,2)
prop.purp <- sum(index==1)/length(index)
```

The easiest way to get a sense of how population parameters and their estimates are related to one another is to use computer simulation. A simulation is just an imitation of a real-world process. It is very easy to simulate the process of sampling from a population in R.

Here is a concrete example. Let's assume that we are working with a plant species that exists as two different morphs: a purple morph and a green morph. To study this aspect of the population we would need to collect information about the colour of different individuals, which means we are working with a nominal variable (taking values 'purple' or 'green'). We are interested in a very simple question: What is the frequency of the purple morph? That is, we want to know what percentage (or equivalently, what proportion) of the population is purple. The population parameter of interest is therefore, morph frequency. 

We can depict this situation with a map showing where purple and green individuals are located on the hypothetical landscape:
```{r plants-all, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Landscape with purple and green plants on it'}
plantdata <- 
  data.frame(xloc  = runif(nsamp), 
             yloc  = runif(nsamp), 
             morph = sample(c("purple","green")[index], 100, replace = TRUE))
plttheme <- theme_get()
plttheme$axis.text <- plttheme$axis.ticks <- plttheme$axis.title <- element_blank()
baseplt <- ggplot(plantdata, aes(x = xloc, y = yloc, colour = morph, )) + 
           geom_point() + scale_color_identity() + coord_fixed() + plttheme
baseplt
```

These artificial, idealised data were generated using R. We placed 'individuals' onto the landscape at random locations---every location is equally likely---and then assigned them purple morph status with a probability of 0.4 (we made them are green otherwise).

### Sampling error and sampling distributions

In contrast to a real-world setting, we already know the true value of the population parameter in this example; the purple morph frequency is 40%. However, if we didn't know 'truth', and we weren't in a position to sample every individual, we would have to construct some kind of point estimate of the purple morph frequency. To do this, we could take a representative sample of plants from across the landscape and then calculate the percentage of purple plants in our sample. A representive sample in this case is one in which every individual has an equal probability of being sampled. We call this a **random sample**. Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality. Luckily, it is at least easy to simulate a random sample.

Let's seen what happens if we sample `r sampsize1` plants in this way. This plot shows the original population of plants, but now we have circled the selected individuals in red.
```{r plants-samp1, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Plants sampled the first time'}
sample1 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample1, colour = "red", shape = 1, size = 5)
freqs1 <- table(sample1$morph)
```
We found `r freqs1["green"]` green plants and `r freqs1["purple"]` purple plants in this first hypothetical sample, which means our estimate of the purple morph frequency is `r round(100*freqs1["purple"]/sampsize1)`%. This is not far off the true value of 40%. What happens if we repeat this process, resulting in a new, completely independent sample? Here is the sampled population:
```{r plants-samp2, echo = FALSE, out.width='50%', fig.asp=1, fig.align='center', fig.cap='Plants sampled the second time'}
sample2 <- sample_n(plantdata, size = sampsize1)
baseplt + geom_point(data = sample2, colour = "red", shape = 1, size = 5)
freqs2 <- table(sample2$morph)
```
This time we ended up sampling `r freqs2["green"]` green plants and `r freqs2["purple"]` purple plants, so our new estimate of the purple morph frequency is `r round(100*freqs2["purple"]/sampsize1)`%, which is quite some way off the true value.

Nothing about the study population changed between the first and second sample. What's more, we used a completely reliable sampling scheme to generate these samples; there is nothing biased or 'incorrect' about the way individuals were sampled. The different estimates of the purple morph frequency arise from nothing more than chance variation.

This chance variation--which arises whenever we observe a sample instead of the whole population--has a special name. It is called the **sampling error** (or sampling variation). Sampling error is the main reason we have to use statistics to learn from data. It is always present, and so any estimate you derive from a sample is affected by it. Sampling error is not a property of any particular sample. It is really a property of the population distribution of the focal variable, and the sampling method used to investigate this. That statement may seem a little cryptic now, but we will start to get a sense of what it means in this, and the next, practical.

We can develop our simple simulation example to explore the consequences of sampling error. Rather than taking one sample at a time, we will use R to simulate 1000s of different samples, and for each sample, calculate the number of purple morph individuals found. Each sample is drawn from the same population, i.e., the population parameter (purple morph frequency) is the same for every sample. Here is a summary of one such repeated simulation exercise:
```{r samp-dist-1, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 20)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize1, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(0:sampsize1)) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

This bar plot summarises the result from 100000 samples. In each sample, we took `r sampsize1` individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, ... purple individuals, all the way up to the maximum possible (`r sampsize1`). It summarises the distribution of purple morph counts that we can expect when we repeat the same sampling process over and over again.

This distribution has a special name. It is called the **sampling distribution**. The sampling distribution is just the distribution we expect a particular statistic to follow. In order to to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. We just used simulation to approximate the sampling distribution of purple morph counts that arises when we sample `r sampsize1` individuals from a population that is `r 100*prop.purp`% purple. 

The sampling distribution is the key to 'doing statistics'.  

Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error (to quantify uncertainty), and we can begin to make meaningful comparisons that enable us to address scientific questions. Fortunately, we don't have to work any of this out for ourselves. Statisticians have already done this for many different situations.

### The effect of sample size

Perhaps the most important property of any sampling scheme is the **sample size**: the number of observations (objects or items) in a sample. To see how sample size influences the sampling distribution, and to understabnd why it matters, let's carry on with our simulation example. We will repeat the resampling exercise, but this time we will do it twice, first taking a sample of `r sampsize2` individuals each time, and then taking a sample of `r sampsize3` individuals each time. In both cases, we'll examine the results of taking 100000 samples overall:

```{r samp-dist-2, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 40)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize2, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize2, 1)), 
                   breaks = as.character(seq(0, sampsize2, 2))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

```{r samp-dist-3, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 80)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsize3, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsize3, 1)), 
                   breaks = as.character(seq(0, sampsize3, 4))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

What do these plots tell us about the effect of increasing sample size? Notice that we plotted each of them over the full range of possible outcomes (the x axis runs from 0-`r sampsize2` and 0-`r sampsize3`, respectively, in the first and second plot). We did this so that we can meaningfully compare the spread of each sampling distribution, relative to the full range of possible outcomes.

What do these figures show? The range of outcomes in the first plot is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65% (we sampled 40 individuals each time). The range of outcomes in the second plot is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. Clearly, this suggests that when we increase the sample size we expect to encounter less sampling error. This makes intuitive sense: the composition of large sample should more closely approximate that of the true population than a small sample. 

How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample `r (sampsizebig <- 500)` individuals each time we take a sample: 
```{r samp-dist-big, echo = FALSE, out.width='80%', fig.asp=0.6, fig.align='center', fig.cap='Distribution of number of purple morphs sampled (n = 500)'}
out <- data.frame(n.purple = factor(rbinom(n = 100000, size = sampsizebig, prob = prop.purp)))
ggplot(out, aes(x = n.purple)) + geom_bar() + 
  scale_x_discrete(limits = as.character(seq(0, sampsizebig, 1)), 
                   breaks = as.character(seq(0, sampsizebig, 50))) + 
  xlab("No. of purple morph individuals") + ylab("Count")
```

Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples that we just considered, but even with 500 individuals in a sample, we should still expect quite a lot of uncertainty in our estimate. The take home message is that you need a lot of data to reduce sampling error.

