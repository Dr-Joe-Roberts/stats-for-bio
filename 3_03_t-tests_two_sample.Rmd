# Two-sample *t*-test

Think back to the permutation test we carried out in [Comparing populations] chapter. In order to construct the test, we pretended that the two samples were both drawn from one population. We used the combined purple morph and green morph samples to mimic the process of drawing new samples from this hypothetical population, each time assigning arbitary labels to generate artificial observations. [[MORE]]. 

The two-sample *t*-test can be viewed as a parametric version of this procedure.

## When do we use a two-sample *t*-test?

[[FINISH]]

## How does the two-sample *t*-test work?

[[NULL HYPOTHESIS ETC]]

The two-sample *t*-test starts by making an assumption about the mathematical form of the population distribution. The key assumption is that the variable is **normally distributed** in the populations.

We are not going to delve any deeper into the *t*-distribution. However, these observations suggests that a parametric version of our permutation test of whether two population means are different can be constructed as follows:

1. Calculate the sample mean of each sample.

2. Calculate the difference between the two sample means

3. Divide this difference by an estimate of the *standard error of the difference*.

(This generates the *t*-test statistic)

4. Compare the test statistic to the theoretical predictions of the *t*-distribution to assess the statistical significance of the observed difference.

This procedure is called a two-sample *t*-test, or sometimes, just a *t*-test. It is very easy to apply, as we will soon see.

### Assumptions of the two-sample *t*-test

There are a number of assumptions that need to be met in order to use a two-sample *t*-test. Some of these are more important than others. We'll start with the most important and work down the list of importance:

1. **Independence.** People tend to forget about this one, probably because we can't do much about it once the data have been collected. We discussed the idea of independence in the [[LINK ME]] chapter. If the data are not independent, then the p-values generated by a *t*-test will not be reliable. Even mild non-independence can be a serious problem. This is why it is so important to design our data collection / experiment well.

2. **Measurement scale.** The variable that we are working with should be measured on an interval or ratio scale. It really doesn't make much sense to apply a *t*-test to data that aren't measured on one of these scales.

The *t*-test will produce exact p-values if the two samples being compared are from populations that are normally distributed with equal variance. However, these assumptions are less important than many people think.

3. **Normality.** The *t*-test is fairly robust to mild departures from normality when the sample sizes are small. When the sample sizes are large (100s of observations per sample), the normality assumption matters even less. We don't have time to explain why this is true in this course, but it has something to do with the 'central limit theorem'.

How should we evaluate these assumptions? The first two are really aspects of experimental design, so they can't be addressed once the data have been collected. The 4^th^ assumption only matters if we plan to use the equal variance version of the two-sample *t*-test (the original Student's *t*-test). This version of the *t*-test is potentially a little more powerful than Welch's version, but not by much, and it is only correct if the population variances really are identical. Since we can never verify this, it is safer to just use the unequal variance version. 

That leaves the 3^rd^ assumption. This is best evaluated by plotting the sample distribution of each group. If the sample size is small, and each sample looks approximately normal when we graphically summarise its distribution, then it is probably fine to use a *t*-test. If we have large samples, we don't even need to worry about moderate departures from normality. Ask someone with experience of data analysis if you run into this situation and are not sure how to interpret the word 'moderate' in this statement.

```{block, type='advanced-box'}
***What about the _equal variance_ assumption?***

If you learned about the two-sample *t*-test at some point in the past you may have been told that variances of the two samples have to be the same. This isn't really correct, for two reasons:

1.    The original version of Student's two-sample *t*-test was derived by assuming that the *variance* (i.e. the variability) of each population was identical, so it is the population variances, not the sample variances, that matter. There *are* methods available to use samples to compare population variances. However, there's really no need to do this...

2.    What matters from a practical perspective is that R uses the "Welch" version of the *t*-test by default^[Welch was another statistician, in case you're wondering]. The Welch two-sample *t*-test does not rely on the equal variance assumption, so as long as we stick with this version of the *t*-test, the equal variance assumption isn't one we need to worry about.
```

## Carrying out a two-sample *t*-test in R

```{r, echo = FALSE}
morph.data <- read.csv(file = "./data_csv/MORPH_DATA.CSV")
tmod.equlv <- t.test(Weight ~ Colour,  data = morph.data, var.equal = TRUE )
tmod.diffv <- t.test(Weight ~ Colour,  data = morph.data, var.equal = FALSE)
nP <- table(morph.data$Colour)["Purple"]
nG <- table(morph.data$Colour)["Green"]
```

You should work through the example in this section. You should already have downloaded the MORPH_DATA.CSV file from MOLE and place it in your working directory. Next, read the data in MORPH_DATA.CSV into an R data frame, giving it the name `morph.data`:

```{r, eval = FALSE}
morph.data <- read.csv(file = "MORPH_DATA.CSV")
```

### Visualising the data and checking the assumptions

We looked at the sample distributions of the green and purple morphs in the [Comparing populations] chapter. Here they are again, plotted in a slightly different way:

[[PLOT]]

The sample size of each sample is [[FIXME]]. This is fairly small (though not bad), so we should keep an eye on the normality assumption. The two dot plots we produced earlier suggest that there is nothing too 'non-normal' about their distributions, so it should be fine to go ahead and use a *t*-test.

### Carrying out the test

It is very straightforward to carry out a two-sample *t*-test in R. We'll work with our plant morph example to demonstrate how to do it. The function we need to use is called `t.test` (no suprises there). Remember, we read the data into a data frame called `morph.data`. This has two columns: `weight` contains the dry weight biomass of each plant, and `Colour` is an index variable that indicates which group (plant morph) an observation belongs to. Here is the R code to carry out a two-sample *t*-test 
```{r, eval = FALSE}
t.test(weight ~ Colour,  morph.data)
```
We have surpressed the output for now as we want to focus on how to use `t.test` function. We have to assign two arguments (remember function arguments?--these control what a function does):

1. The first argument is a **formula**. We know this because it includes a 'tilde' symbol: `~`. The variable name on the left of the `~` should be the variable that contains the actual data (i.e. the numbers we want to compare). The variable on the right should be the indicator variable that says which group each observation belongs to. These are `weight` and `Colour`, respectively.

2. The second argument is the name of the data frame that contains the two variables listed in the formula.

That's it. Let's take a look at the output:
```{r}
t.test(Weight ~ Colour,  morph.data)
```

The part of the output reminds us what we did. The first line tells us what kind of *t*-test we used. This says: `Welch two-sample t-test`, so we know that we have used the version of the two-sample *t*-test that accounts for unequal variance in the samples. The next line reminds us about the data. This says: `data: weight by Colour`, which is R-speak for 'we compared the means of the `Weight` variable, where the groups are defined by the values of the `Colour` variable'.

The third line of text is the most important. This says: `t = -2.7808, d.f. = 140.69, p-value = 0.006165`. The first part of this, `t = -2.7808`, is the test statistic (i.e. the value of the t statistic). The second part, `df = 140.69`, summarise the 'degrees of freedom'. This is essentially a measure of how much power our statistical test has (see the box below). The third part, `p-value = 0.006165`, is the all-important p-value. This says there is a statistically significant difference in the mean dry weight biomass of the two colour morphs, because *p*<0.05.

The fourth line of text (`alternative hypothesis: true difference in means is not equal to 0`) just reminds us what the alternative to the null hypothesis is (H~1~).

The next two lines show us the '95% confidence interval' for the difference between the means. We don't really need this information, but we can think of this interval as a summary of the likely values of the true difference (a confidence interval is more complicated than that in reality).

The last few lines just summarise the sample means of each group. This is only useful if we did not bother to calculate these already (which we should already have done!). 

```{block, type='advanced-box'}
**A bit more about degrees of freedom**

In the original version of the *t*-test (the one that assumes equal variances) the degrees of freedom of the test are give by (n~A~-1) + (n~B~-1), where n~A~ is the number of observations in sample A, and n~B~ the number of observations in sample B. The plant morph data included `r nP` purple plants and `r nG` green plants, so if we had used the original version of the test we would have (`r nP`-1) + (`r nG`-1) = `r nP+nG-2` d.f.. 

However, the Welch version of the *t*-test reduces the numbers of degrees of freedom using a formula which takes into account the difference in variance in the two samples. In a nutshell, the greater the difference in variances the smaller the number of degrees of freedom (all else equal). Here's the important point to remember: the 'unequal variance accounting' results in degrees of freedom that are not whole numbers.

In either case, a test with high degrees of freedom is more powerful than one with low degrees of freedom. That is, the higher the degrees of freedom, the more likley we are to detect an effect if it is present. This is why degrees of freedom matter.
```

### Summarising the result

Having obtained the result we need to write the conclusion. Remember we are testing an hypothesis so go back to the original question to write our conclusion. In this case the appropriate conclusion is:

> Mean dry weight biomass of purple and green plants differed significantly (Welch's t = 2.94, d.f. = 40, *p* < 0.01), with green plants being the larger.

This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion:

> The mean dry weight biomass of green plants (787 grams) is significantly greater than that of purple plants (656 grams) (Welch's *t* = 2.94, d.f. = 40, *p* < 0.01)

When we are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. **Simply writing t = 2.94 or p < 0.01 is not an adequate conclusion.** Never write this kind of thing in a report. 
